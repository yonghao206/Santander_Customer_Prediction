{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Santander Customer Transaction Prediction\n",
    "Hi, this kernel produces a top 5 submission with public/private LB of 0.92569/0.92446 running in 1.5 hours on kaggle servers. It was written in a very short time thus containing some mistakes which aren't edited. This achievement wouldn't have been possible without the huge insights and inspiration I gained from many great kernels / discussions, thank you! I also want to thank my teammates interneuron and Chua Cheng Hong for helping me getting the most out of this kernel and collectively achieving 3rd place in this competition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from copy import copy\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "At this point I filter out the fakes (shoutout to YaG320) and concatenate train and test for future FE. Setting `use_experimental = True` splits the Train data into train / test which was useful for later NN training indicating whether a model is overfitting. I wasn't sure if the fakes are going to be used for final score evaluation, so I also applied all the transformations to them and kept them in a separate dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 202)\n"
     ]
    }
   ],
   "source": [
    "use_experimental = False\n",
    "\n",
    "train_df = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv')\n",
    "test_df = pd.read_csv('../input/santander-customer-transaction-prediction/test.csv')\n",
    "\n",
    "indices_fake = np.load('../input/list-of-fake-samples-and-public-private-lb-split/synthetic_samples_indexes.npy')\n",
    "indices_pub = np.load('../input/list-of-fake-samples-and-public-private-lb-split/public_LB.npy')\n",
    "indices_pri = np.load('../input/list-of-fake-samples-and-public-private-lb-split/private_LB.npy')\n",
    "indices_real = np.concatenate([indices_pub, indices_pri])\n",
    "\n",
    "features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\n",
    "target_train = train_df['target']\n",
    "X_train = train_df\n",
    "X_test = test_df.loc[indices_real,:]\n",
    "X_test['target'] = np.zeros(X_test.shape[0])\n",
    "X_fake = test_df.loc[indices_fake,:]\n",
    "X_fake['target'] = np.zeros(X_test.shape[0])\n",
    "train_length = X_train.shape[0]\n",
    "target_test = X_test['target']\n",
    "target_fake = X_fake['target']\n",
    "\n",
    "if use_experimental:\n",
    "    np.random.seed(42)    \n",
    "    indices = np.arange(train_length)\n",
    "    train_length = 150000\n",
    "    np.random.shuffle(indices)\n",
    "    indices_train = indices[:train_length]\n",
    "    indices_test = indices[train_length:]\n",
    "    # Swapped order to not overwrite X_train to soon\n",
    "    X_test = X_train.iloc[indices_test,:]\n",
    "    X_fake = X_train.iloc[indices_test,:]\n",
    "    target_fake = X_fake['target']\n",
    "    X_train = X_train.iloc[indices_train,:]\n",
    "    target_train = X_train['target']\n",
    "    target_test = X_test['target']\n",
    "\n",
    "X_all = pd.concat([X_train, X_test])\n",
    "print(X_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counts, Density, Deviation\n",
    "Here I calculate the unique counts of each faeture seaparately. Based on that I also calculate the density by smoothing the counts and also the deviation as counts/density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6287a009804658baa4636dbbae5e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(300000, 802)\n"
     ]
    }
   ],
   "source": [
    "import scipy.ndimage\n",
    "\n",
    "sigma_fac = 0.001\n",
    "sigma_base = 4\n",
    "\n",
    "eps = 0.00000001\n",
    "\n",
    "def get_count(X_all, X_fake):\n",
    "    features_count = np.zeros((X_all.shape[0], len(features)))\n",
    "    features_density = np.zeros((X_all.shape[0], len(features)))\n",
    "    features_deviation = np.zeros((X_all.shape[0], len(features)))\n",
    "\n",
    "    features_count_fake = np.zeros((X_fake.shape[0], len(features)))\n",
    "    features_density_fake = np.zeros((X_fake.shape[0], len(features)))\n",
    "    features_deviation_fake = np.zeros((X_fake.shape[0], len(features)))\n",
    "    \n",
    "    sigmas = []\n",
    "\n",
    "    for i,var in enumerate(tqdm(features)):\n",
    "        X_all_var_int = (X_all[var].values * 10000).round().astype(int)\n",
    "        X_fake_var_int = (X_fake[var].values * 10000).round().astype(int)\n",
    "        lo = X_all_var_int.min()\n",
    "        X_all_var_int -= lo\n",
    "        X_fake_var_int -= lo\n",
    "        hi = X_all_var_int.max()+1\n",
    "        counts_all = np.bincount(X_all_var_int, minlength=hi).astype(float)\n",
    "        zeros = (counts_all == 0).astype(int)\n",
    "        before_zeros = np.concatenate([zeros[1:],[0]])\n",
    "        indices_all = np.arange(counts_all.shape[0])\n",
    "        # Geometric mean of twice sigma_base and a sigma_scaled which is scaled to the length of array \n",
    "        sigma_scaled = counts_all.shape[0]*sigma_fac\n",
    "        sigma = np.power(sigma_base * sigma_base * sigma_scaled, 1/3)\n",
    "        sigmas.append(sigma)\n",
    "        counts_all_smooth = scipy.ndimage.filters.gaussian_filter1d(counts_all, sigma)\n",
    "        deviation = counts_all / (counts_all_smooth+eps)\n",
    "        indices = X_all_var_int\n",
    "        features_count[:,i] = counts_all[indices]\n",
    "        features_density[:,i] = counts_all_smooth[indices]\n",
    "        features_deviation[:,i] = deviation[indices]\n",
    "        indices_fake = X_fake_var_int\n",
    "        features_count_fake[:,i] = counts_all[indices_fake]\n",
    "        features_density_fake[:,i] = counts_all_smooth[indices_fake]\n",
    "        features_deviation_fake[:,i] = deviation[indices_fake]\n",
    "        \n",
    "    features_count_names = [var+'_count' for var in features]\n",
    "    features_density_names = [var+'_density' for var in features]\n",
    "    features_deviation_names = [var+'_deviation' for var in features]\n",
    "\n",
    "    X_all_count = pd.DataFrame(columns=features_count_names, data = features_count)\n",
    "    X_all_count.index = X_all.index\n",
    "    X_all_density = pd.DataFrame(columns=features_density_names, data = features_density)\n",
    "    X_all_density.index = X_all.index\n",
    "    X_all_deviation = pd.DataFrame(columns=features_deviation_names, data = features_deviation)\n",
    "    X_all_deviation.index = X_all.index\n",
    "    X_all = pd.concat([X_all,X_all_count, X_all_density, X_all_deviation], axis=1)\n",
    "    \n",
    "    X_fake_count = pd.DataFrame(columns=features_count_names, data = features_count_fake)\n",
    "    X_fake_count.index = X_fake.index\n",
    "    X_fake_density = pd.DataFrame(columns=features_density_names, data = features_density_fake)\n",
    "    X_fake_density.index = X_fake.index\n",
    "    X_fake_deviation = pd.DataFrame(columns=features_deviation_names, data = features_deviation_fake)\n",
    "    X_fake_deviation.index = X_fake.index\n",
    "    X_fake = pd.concat([X_fake,X_fake_count, X_fake_density, X_fake_deviation], axis=1)    \n",
    "\n",
    "    features_count = features_count_names\n",
    "    features_density = features_density_names\n",
    "    features_deviation = features_deviation_names\n",
    "    return X_all, features_count, features_density, features_deviation, X_fake\n",
    "\n",
    "X_all, features_count, features_density, features_deviation, X_fake = get_count(X_all, X_fake)\n",
    "print(X_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target encoding (unused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# # Also try to encode counts themselves\n",
    "\n",
    "# weighting = 500\n",
    "# n_splits = 2\n",
    "\n",
    "# features_to_encode = features_count\n",
    "\n",
    "# def get_encoding(X_all):\n",
    "#     arr_all_int = (X_all[features_to_encode].values * 10000).round().astype(int)\n",
    "#     arr_target_train = target_train.values \n",
    "#     arr_target_test = target_test.values\n",
    "    \n",
    "#     preds_oof = np.zeros((train_length, len(features)))\n",
    "#     preds_test = np.zeros((arr_all_int.shape[0] - train_length, len(features)))\n",
    "#     preds_train = np.zeros((train_length, len(features)))\n",
    "    \n",
    "#     for v ,var in enumerate(tqdm(features_to_encode)):\n",
    "#         lo = arr_all_int[:,v].min()\n",
    "#         arr_all_var = arr_all_int[:,v] - lo\n",
    "#         hi = arr_all_var.max() + 1\n",
    "#         arr_train_var = arr_all_var[:train_length]\n",
    "#         arr_test_var = arr_all_var[train_length:]\n",
    "#         folds = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "#         for train_idx, val_idx in folds.split(arr_train_var, arr_target_train):\n",
    "#             X_tr = arr_train_var[train_idx]\n",
    "#             y_tr = arr_target_train[train_idx]\n",
    "#             X_val = arr_train_var[val_idx]\n",
    "#             y_val = arr_target_train[val_idx]\n",
    "#             X_ts = arr_test_var\n",
    "#             arr1 = X_tr[y_tr == 1]        \n",
    "#             mean = arr1.shape[0] / X_tr.shape[0]\n",
    "#             hits = np.bincount(arr1, minlength=hi).astype(float)\n",
    "#             base = np.bincount(X_tr, minlength=hi).astype(float)\n",
    "#             lamb = base / (base + weighting)\n",
    "#             expected_target = (hits / (base+0.00000001)) * lamb + (1-lamb) * mean              \n",
    "\n",
    "#             prediction_oof = expected_target[X_val]\n",
    "#             prediction_test = expected_target[X_ts]\n",
    "#             prediction_train = expected_target[X_tr]\n",
    "#             preds_oof[val_idx,v] += prediction_oof \n",
    "#             preds_test[:,v] += prediction_test \n",
    "#             preds_train[train_idx,v] += prediction_train \n",
    "#         score_running_oof = roc_auc_score(arr_target_train, preds_oof.mean(axis=1))\n",
    "#         score_running_test = roc_auc_score(arr_target_test, preds_test.mean(axis=1))\n",
    "#         score_running_train = roc_auc_score(arr_target_train, preds_train.mean(axis=1))\n",
    "        \n",
    "#     preds_all = np.concatenate([preds_oof, preds_test], axis=0)\n",
    "#     feature_names = [var+'_encoding' for var in features_to_encode]\n",
    "#     X_all_encoding = pd.DataFrame(columns=feature_names, data = preds_all)\n",
    "#     X_all_encoding.index = X_all.index\n",
    "#     X_all = pd.concat([X_all,X_all_encoding], axis=1)\n",
    "#     return X_all, feature_names\n",
    "\n",
    "# X_all, features_encoding = get_encoding(X_all)\n",
    "\n",
    "# print(X_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB predictor (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# def compress(arr, indices, thresh = 100000):\n",
    "#     old_length = arr.shape[0] \n",
    "#     fac = old_length // thresh\n",
    "#     if fac < 1:\n",
    "#         return arr, indices\n",
    "#     new_length = int(np.ceil(old_length / fac))+2\n",
    "#     new_arr = np.zeros(new_length * fac)\n",
    "#     new_arr[fac:old_length+fac] = arr\n",
    "#     new_arr = new_arr.reshape(new_length, fac).sum(axis=1)\n",
    "#     index_shift = indices[0]-0.5*(fac-1)-1\n",
    "#     new_indices = np.arange(new_length) * fac + index_shift\n",
    "#     return new_arr, new_indices    \n",
    "\n",
    "# def transform_grouper(counts_orig, indices_orig, min_elems=10):\n",
    "#     first_ind, first_count, last_ind, last_count = indices_orig[0], counts_orig[0], indices_orig[-1], counts_orig[-1]\n",
    "#     indices = indices_orig[1:-1]\n",
    "#     counts = counts_orig[1:-1]\n",
    "#     new_indices = [np.array([first_ind])]\n",
    "#     cur_indices = []\n",
    "#     cur_counts = 0\n",
    "#     for i, count in enumerate(counts):\n",
    "#         cur_indices.append(indices[i])\n",
    "#         cur_counts += count\n",
    "#         if cur_counts >= min_elems:\n",
    "#             new_indices.append(np.array(cur_indices))\n",
    "#             cur_counts = 0\n",
    "#             cur_indices = []\n",
    "#     if cur_indices:\n",
    "#         new_indices.append(np.array(cur_indices))\n",
    "#     new_indices.append(np.array([last_ind]))\n",
    "#     return new_indices\n",
    "\n",
    "# def transform_space(counts, indices_grouped):\n",
    "#     new_counts = []\n",
    "#     new_indices = []\n",
    "#     for arr_index in indices_grouped:\n",
    "#         count = counts[arr_index]\n",
    "#         count_sum = np.sum(count)\n",
    "#         if count_sum > 0:\n",
    "#             new_indices.append(np.sum(arr_index*count)/count_sum)\n",
    "#         else:\n",
    "#             new_indices.append(np.mean(arr_index))\n",
    "#         new_counts.append(count_sum)\n",
    "#     return np.array(new_counts), np.array(new_indices)\n",
    "\n",
    "# def get_hist(arr, indices_grouped):\n",
    "#     hi = indices_grouped[-1][0] + 1\n",
    "#     counts = np.bincount(arr, minlength=hi)\n",
    "#     indices = np.arange(counts.shape[0])\n",
    "#     counts, indices = transform_space(counts, indices_grouped)\n",
    "#     return counts / counts.sum(), indices\n",
    "\n",
    "# def get_density_func(kde, indices, sigma=0.0001, resolution=2000):\n",
    "#     kde_smooth = scipy.ndimage.filters.gaussian_filter1d(kde,sigma*len(indices))\n",
    "#     return scipy.interpolate.interp1d(indices, kde_smooth, kind='linear')\n",
    "\n",
    "# def get_p_x_t(x, t, density_funcs):\n",
    "#     return density_funcs[t](x) \n",
    "\n",
    "# def get_p_1_x(x, density_funcs, prior=0.1, eps=0.00000000000000001):\n",
    "#     p_x_0 = get_p_x_t(x,0,density_funcs)\n",
    "#     p_x_1 = get_p_x_t(x,1,density_funcs)\n",
    "#     p_x = (p_x_1*prior + p_x_0*(1-prior)+eps)\n",
    "#     return p_x_1*prior / p_x, p_x\n",
    "\n",
    "# import scipy.ndimage\n",
    "# import scipy\n",
    "\n",
    "# n_splits = 2\n",
    "\n",
    "# def get_NB_predictor(X_all):   \n",
    "#     arr_all_int = (X_all[features].values * 10000).round().astype(int)\n",
    "#     arr_target_train = target_train.values \n",
    "#     arr_target_test = target_test.values\n",
    "#     preds_oof = np.zeros((train_length, len(features)))\n",
    "#     preds_test = np.zeros((arr_all_int.shape[0] - train_length, len(features)))\n",
    "#     preds_train = np.zeros((train_length, len(features)))\n",
    "#     for v, var in enumerate(features):\n",
    "#         lo = arr_all_int[:,v].min()\n",
    "#         arr_all_var = arr_all_int[:,v] - lo\n",
    "#         hi = arr_all_var.max() + 1\n",
    "#         arr_train_var = arr_all_var[:train_length]\n",
    "#         arr_test_var = arr_all_var[train_length:]\n",
    "#         counts_all = np.bincount(arr_all_var, minlength=hi)\n",
    "#         indices_all = np.arange(counts_all.shape[0])\n",
    "#         min_elems = [20]\n",
    "#         sigmas = [0.05,0.02,0.005,0.002]\n",
    "#         preds_oof_temp = np.zeros((preds_oof.shape[0], len(sigmas), len(min_elems)))\n",
    "#         preds_test_temp = np.zeros((preds_test.shape[0], len(sigmas), len(min_elems)))\n",
    "#         preds_train_temp = np.zeros((preds_train.shape[0], len(sigmas), len(min_elems)))\n",
    "#         for j,min_elem in enumerate(min_elems):\n",
    "#             indices_grouped = transform_grouper(counts_all, indices_all, min_elems=min_elem)            \n",
    "#             kfold = StratifiedKFold(n_splits=n_splits, shuffle=False, random_state = np.random.randint(10000))\n",
    "#             for train_idx, val_idx in kfold.split(arr_train_var, arr_target_train):\n",
    "#                 X_tr = arr_train_var[train_idx]\n",
    "#                 y_tr = arr_target_train[train_idx]\n",
    "#                 X_val = arr_train_var[val_idx]\n",
    "#                 y_val = arr_target_train[val_idx]\n",
    "#                 X_ts = arr_test_var\n",
    "#                 arr0 = X_tr[y_tr == 0]\n",
    "#                 arr1 = X_tr[y_tr == 1]\n",
    "#                 prior = arr1.shape[0] / (arr1.shape[0] + arr0.shape[0]) \n",
    "#                 kde0, indices0 = get_hist(arr0, indices_grouped)\n",
    "#                 kde1, indices1 = get_hist(arr1, indices_grouped)\n",
    "#                 for i,sigma in enumerate(sigmas): \n",
    "#                     density_func0 = get_density_func(kde0, indices0, sigma=sigma)\n",
    "#                     density_func1 = get_density_func(kde1, indices1, sigma=sigma)\n",
    "#                     prediction_oof, confidence = get_p_1_x(X_val, [density_func0, density_func1], prior=prior)\n",
    "#                     prediction_test, confidence = get_p_1_x(X_ts, [density_func0, density_func1], prior=prior)\n",
    "#                     prediction_train, confidence = get_p_1_x(X_tr, [density_func0, density_func1], prior=prior)\n",
    "#                     preds_oof_temp[val_idx,i,j] += prediction_oof \n",
    "#                     preds_test_temp[:,i,j] += prediction_test \n",
    "#                     preds_train_temp[train_idx,i,j] += prediction_train \n",
    "#         scores = np.zeros((len(sigmas), len(min_elems)))\n",
    "#         for i in range(len(sigmas)):\n",
    "#             for j in range(len(min_elems)):\n",
    "#                 scores[i,j] = log_loss(arr_target_train, preds_oof_temp[:,i,j])\n",
    "#         sorted_indices = np.dstack(np.unravel_index(np.argsort(scores.ravel()), (len(sigmas), len(min_elems))))[0]\n",
    "#         preds_oof[:,v] = preds_oof_temp[:,sorted_indices[0,0], sorted_indices[0,1]]\n",
    "#         preds_test[:,v] = preds_test_temp[:,sorted_indices[0,0], sorted_indices[0,1]]\n",
    "#         preds_train[:,v] = preds_train_temp[:,sorted_indices[0,0], sorted_indices[0,1]]\n",
    "#         score_running_oof = roc_auc_score(arr_target_train, preds_oof.mean(axis=1))\n",
    "#         score_running_test = roc_auc_score(arr_target_test, preds_test.mean(axis=1))\n",
    "#         score_running_train = roc_auc_score(arr_target_train, preds_train.mean(axis=1))\n",
    "#         print(var, sorted_indices[0], score_running_oof, score_running_test, score_running_train)\n",
    "#     preds_all = np.concatenate([preds_oof, preds_test], axis=0)\n",
    "#     feature_names = [var+'_pred' for var in features]\n",
    "#     X_all_pred = pd.DataFrame(columns=feature_names, data = preds_all)\n",
    "#     X_all_pred.index = X_all.index\n",
    "#     X_all = pd.concat([X_all,X_all_pred], axis=1)\n",
    "#     return X_all, feature_names\n",
    "        \n",
    "# X_all, features_pred = get_NB_predictor(X_all)\n",
    "\n",
    "# print(X_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize\n",
    "I standardize all the features (or supposedly so, apparently I forgot density and deviation being in time trouble). Which is important for later NN usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300000, 802)\n"
     ]
    }
   ],
   "source": [
    "features_to_scale = [features, features_count]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_standardized(X_all, X_fake):\n",
    "    scaler = StandardScaler()\n",
    "    features_to_scale_flatten = [var for sublist in features_to_scale for var in sublist]\n",
    "    scaler.fit(X_all[features_to_scale_flatten])\n",
    "    features_scaled = scaler.transform(X_all[features_to_scale_flatten])\n",
    "    features_scaled_fake = scaler.transform(X_fake[features_to_scale_flatten])\n",
    "    X_all[features_to_scale_flatten] = features_scaled\n",
    "    X_fake[features_to_scale_flatten] = features_scaled_fake\n",
    "    return X_all, X_fake\n",
    "\n",
    "X_all, X_fake = get_standardized(X_all, X_fake)\n",
    "\n",
    "print(X_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": false
   },
   "source": [
    "## Rotated features (unused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# features_to_rot = [features, features_count]\n",
    "# angles = [np.pi/4]\n",
    "\n",
    "# def get_rotated(X_all):\n",
    "#     list_features_rot = []\n",
    "#     list_X_all_rot = [] \n",
    "#     for j ,angle in enumerate(angles):\n",
    "#         list_rot_0 = []\n",
    "#         list_rot_1 = []\n",
    "#         feature_names_0 = []\n",
    "#         feature_names_1 = []\n",
    "#         c,s = np.cos(angle), np.sin(angle)\n",
    "#         rot_mat = np.array([[c,-s],[s,c]])\n",
    "#         for i in tqdm(range(len(features))):\n",
    "#             vars_to_rot = [feat[i] for feat in features_to_rot]\n",
    "#             arr = X_all[vars_to_rot].values\n",
    "#             arr_rot = np.dot(arr, rot_mat)\n",
    "#             list_rot_0.append(arr_rot[:,0])\n",
    "#             list_rot_1.append(arr_rot[:,1])\n",
    "#             feature_names_0.append('var_%d_angle_%d_rot_0' %(i,j))        \n",
    "#             feature_names_1.append('var_%d_angle_%d_rot_1' %(i,j))        \n",
    "#         arr_rot_0 = np.stack(list_rot_0).transpose()\n",
    "#         arr_rot_1 = np.stack(list_rot_1).transpose()\n",
    "#         list_features_rot.append(feature_names_0)\n",
    "#         list_features_rot.append(feature_names_1)\n",
    "#         X_all_rot_0 = pd.DataFrame(columns=feature_names_0, data = arr_rot_0)\n",
    "#         X_all_rot_0.index = X_all.index\n",
    "#         X_all_rot_1 = pd.DataFrame(columns=feature_names_1, data = arr_rot_1)\n",
    "#         X_all_rot_1.index = X_all.index\n",
    "#         list_X_all_rot.append(X_all_rot_0)\n",
    "#         list_X_all_rot.append(X_all_rot_1)\n",
    "\n",
    "#     X_all_rot = pd.concat(list_X_all_rot, axis=1)\n",
    "#     X_all = pd.concat([X_all, X_all_rot], axis=1)\n",
    "#     return X_all, feature_names_0, feature_names_1\n",
    "\n",
    "# X_all, feature_names_rot_0, feature_names_rot_1 = get_rotated(X_all)\n",
    "\n",
    "# print(X_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": false
   },
   "source": [
    "## PCA (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# features_to_pca = [features, features_count]\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# def get_pca(X_all):\n",
    "#     list_X_all_pca = [] \n",
    "#     list_pca_0 = []\n",
    "#     list_pca_1 = []\n",
    "#     feature_names_0 = []\n",
    "#     feature_names_1 = []\n",
    "#     for i in tqdm(range(len(features))):\n",
    "#         vars_to_pca = [feat[i] for feat in features_to_rot]\n",
    "#         arr = X_all[vars_to_pca].values\n",
    "#         pca = PCA(n_components = 2)\n",
    "#         arr_pca = pca.fit_transform(arr)\n",
    "#         list_pca_0.append(arr_pca[:,0])\n",
    "#         list_pca_1.append(arr_pca[:,1])\n",
    "#         feature_names_0.append('var_%d_pca_0' %i)        \n",
    "#         feature_names_1.append('var_%d_pca_1' %i)        \n",
    "#     arr_pca_0 = np.stack(list_pca_0).transpose()\n",
    "#     arr_pca_1 = np.stack(list_pca_1).transpose()\n",
    "#     X_all_pca_0 = pd.DataFrame(columns=feature_names_0, data = arr_pca_0)\n",
    "#     X_all_pca_0.index = X_all.index\n",
    "#     X_all_pca_1 = pd.DataFrame(columns=feature_names_1, data = arr_pca_1)\n",
    "#     X_all_pca_1.index = X_all.index\n",
    "#     list_X_all_pca.append(X_all_pca_0)\n",
    "#     list_X_all_pca.append(X_all_pca_1)\n",
    "\n",
    "#     X_all_pca = pd.concat(list_X_all_pca, axis=1)\n",
    "#     X_all = pd.concat([X_all, X_all_pca], axis=1)\n",
    "#     return X_all, feature_names_0, feature_names_1\n",
    "\n",
    "# X_all, feature_names_pca_0, feature_names_pca_1 = get_pca(X_all)\n",
    "\n",
    "# print(X_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Dataframes\n",
    "After performing FE on `X_all`, I split it back into train/test and delete the obsolete dataframe. The latter is a reoccuring theme in this kernel and was necessary as I often experienced memory overflow. This is also the reason why I wrote most of the code inside of functions. Shoutout to kaggle however for providing fast GPUs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 802) (100000, 802)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_all.iloc[:train_length,:]\n",
    "X_test = X_all.iloc[train_length:,:]\n",
    "del X_all\n",
    "import gc\n",
    "gc.collect()\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM \n",
    "Many public kernels indicated that the features are independent, conditional on the target. For this reason I train seperate trees for each feature and their respective counts. Using a simple average (of the square root) of all tree predictors achieves around 0.9225 / 0.9205 on public/private LB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_used = [features, features_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params\n",
    "Parameters of the LGBM model. I choose l1 regularization / max_bin / learning rate and num_leaves seaprately for each of the 200 var_x through earlier hyperparam search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boost_from_average':'false',\n",
    "    'boost': 'gbdt',\n",
    "    'feature_fraction': 1,\n",
    "    'learning_rate': 0.08,\n",
    "    'max_depth': -1,\n",
    "    'metric':'binary_logloss',\n",
    "    'num_leaves': 4,\n",
    "    'num_threads': 8,\n",
    "    'tree_learner': 'serial',\n",
    "    'objective': 'binary',\n",
    "    'reg_alpha': 2,\n",
    "    'reg_lambda': 0,\n",
    "    'verbosity': 1,\n",
    "    'max_bin':256,\n",
    "}\n",
    "\n",
    "# reg_alpha\n",
    "reg_alpha_values = [0.75, 1, 2, 3]\n",
    "reg_alpha_var = [3, 0, 2, 3, 2, 0, 1, 1, 3, 2, 2, 0, 2, 0, 2, 2, 2, 1, 1, 2, 1, 2, 3, 3, 2, 1, 3, 1, 3, 2, 2, 3, 1, 1, 3, 2, 0, 1, 0, 2, 1, 1, 2, 3, 0, 3, 3, 3, 2, 0, 3, 1, 3, 1, 1, 0, 2, 2, 0, 0, 0, 1, 2, 1, 0, 1, 3, 2, 0, 2, 1, 2, 0, 0, 1, 3, 3, 1, 2, 3, 3, 2, 0, 1, 2, 3, 3, 2, 3, 3, 0, 0, 3, 0, 1, 0, 1, 0, 2, 3, 1, 0, 3, 1, 3, 2, 3, 1, 3, 3, 3, 1, 3, 2, 3, 2, 1, 0, 1, 2, 0, 3, 0, 3, 0, 3, 2, 1, 0, 0, 2, 2, 2, 0, 1, 0, 0, 2, 3, 2, 2, 1, 1, 0, 1, 2, 2, 2, 1, 0, 2, 3, 2, 3, 1, 1, 3, 1, 1, 2, 1, 2, 0, 3, 1, 3, 3, 2, 0, 1, 3, 3, 0, 1, 0, 3, 1, 3, 1, 3, 0, 3, 0, 3, 1, 0, 0, 0, 3, 0, 3, 0, 0, 2, 0, 3, 1, 0, 3, 2]\n",
    "\n",
    "# max_bin\n",
    "max_bin_values = [256, 512, 1024]\n",
    "max_bin_var = [0, 0, 1, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 1, 0, 0, 1, 2, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 0, 1, 1, 0, 1, 0, 0, 1, 2, 1, 2, 1, 0, 0, 1, 0, 2, 0, 1, 0, 0, 2, 1, 1, 1, 0, 0, 0, 2, 0, 0, 2, 1, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 2, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 2, 0, 1, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 1, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 1, 1, 0, 2, 0, 0, 0, 1, 2, 0, 0, 1, 0, 2]\n",
    "\n",
    "# learning_rate\n",
    "learning_rate_values = [0.06, 0.08, 0.12]\n",
    "learning_rate_var = [2, 2, 2, 1, 2, 2, 2, 0, 1, 2, 0, 2, 2, 2, 0, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 0, 1, 0, 2, 0, 0, 2, 0, 2, 2, 2, 1, 2, 0, 0, 2, 0, 0, 1, 2, 1, 2, 0, 0, 2, 1, 2, 2, 2, 2, 0, 0, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 1, 0, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 0, 2, 0, 2, 0, 2, 1, 0, 0, 1, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 1, 0, 2, 1, 2, 2, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 1, 2, 1, 0, 2, 1, 1, 2, 2, 2, 2, 0, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 0, 1, 2, 0, 2, 2, 0, 1, 2, 2, 2, 1, 0, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 0, 0, 2, 0, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1]\n",
    "\n",
    "# num_leaves\n",
    "num_leaves_values = [3, 4, 5]\n",
    "num_leaves_var = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 2, 0, 0, 0, 0, 1, 0, 1, 2, 1, 1, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 0, 2, 2, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 1, 2, 1, 1, 0, 0, 0, 2, 1, 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "A little discussion I had with Chua in this markdown cell. Thats probably not the most efficient way of communicating :D.\n",
    "\n",
    "Chua:\n",
    "\n",
    "One interesting finding is that any var with AUC lower than 0.504 does not help the model in any way. I checked the naive bayes kernel and they were the 'saw-like' probability curves. Went but to magic ecdf kernel and they were those that have ecdf that lined up a bit too perfectly. Considering that we are training features one by one. I think it may make sense to actually remove them all together.\n",
    "\n",
    "Nawid:\n",
    "\n",
    "I think your explanation is generally correct, however I would not exclude them already during training especially because you do it separately for every fold, I believe this leads to some sort of overfitting thus leading to differences in CV and LB, I also think that it is better practice to remove them **after** the trees are trained for example with a Lasso model. Moreover I believe that using AUC as validation metric for early stopping leads to overfitting CV. I therefore removed this part of the code and only train a single model. Similarly a very high number of early stopping rounds can also lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on: ['var_0', 'var_0_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5582   - loss: 323.269  - train AUC: 0.5552   - loss: 323.639 \n",
      "Fold: 2 - val AUC: 0.5479   - loss: 323.996  - train AUC: 0.5552   - loss: 323.584 \n",
      "Fold: 3 - val AUC: 0.5419   - loss: 324.190  - train AUC: 0.5561   - loss: 323.559 \n",
      "Fold: 4 - val AUC: 0.5524   - loss: 323.488  - train AUC: 0.5569   - loss: 323.556 \n",
      "Fold: 5 - val AUC: 0.5482   - loss: 324.035  - train AUC: 0.5592   - loss: 323.411 \n",
      "Score:  - val AUC: 0.5491   - loss: 31068.694 - train AUC: 0.5585   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.5491   - loss: 31068.694\n",
      "Cum CV train: 0.5585   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_1', 'var_1_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5547   - loss: 323.721  - train AUC: 0.5492   - loss: 323.713 \n",
      "Fold: 2 - val AUC: 0.5396   - loss: 324.233  - train AUC: 0.5529   - loss: 323.625 \n",
      "Fold: 3 - val AUC: 0.5418   - loss: 324.060  - train AUC: 0.5534   - loss: 323.589 \n",
      "Fold: 4 - val AUC: 0.5473   - loss: 324.210  - train AUC: 0.5516   - loss: 323.572 \n",
      "Fold: 5 - val AUC: 0.5491   - loss: 323.384  - train AUC: 0.5516   - loss: 323.692 \n",
      "Score:  - val AUC: 0.5461   - loss: 31068.694 - train AUC: 0.5529   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.5749   - loss: 31068.694\n",
      "Cum CV train: 0.5823   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_2', 'var_2_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5526   - loss: 323.449  - train AUC: 0.5567   - loss: 323.332 \n",
      "Fold: 2 - val AUC: 0.5549   - loss: 323.277  - train AUC: 0.5612   - loss: 323.088 \n",
      "Fold: 3 - val AUC: 0.5425   - loss: 324.232  - train AUC: 0.5591   - loss: 323.098 \n",
      "Fold: 4 - val AUC: 0.5555   - loss: 322.947  - train AUC: 0.5583   - loss: 323.386 \n",
      "Fold: 5 - val AUC: 0.5474   - loss: 323.812  - train AUC: 0.5589   - loss: 323.205 \n",
      "Score:  - val AUC: 0.5499   - loss: 31068.694 - train AUC: 0.5613   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.5972   - loss: 31068.694\n",
      "Cum CV train: 0.6054   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_3', 'var_3_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5109   - loss: 326.021  - train AUC: 0.5191   - loss: 325.877 \n",
      "Fold: 2 - val AUC: 0.5094   - loss: 325.980  - train AUC: 0.5176   - loss: 325.904 \n",
      "Fold: 3 - val AUC: 0.5095   - loss: 326.188  - train AUC: 0.5164   - loss: 325.863 \n",
      "Fold: 4 - val AUC: 0.5101   - loss: 325.960  - train AUC: 0.5181   - loss: 325.894 \n",
      "Fold: 5 - val AUC: 0.5166   - loss: 325.902  - train AUC: 0.5182   - loss: 325.880 \n",
      "Score:  - val AUC: 0.5108   - loss: 31068.694 - train AUC: 0.5196   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.5989   - loss: 31068.694\n",
      "Cum CV train: 0.6080   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_4', 'var_4_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5016   - loss: 326.016  - train AUC: 0.5197   - loss: 325.937 \n",
      "Fold: 2 - val AUC: 0.5014   - loss: 326.064  - train AUC: 0.5212   - loss: 325.926 \n",
      "Fold: 3 - val AUC: 0.5066   - loss: 326.079  - train AUC: 0.5213   - loss: 325.892 \n",
      "Fold: 4 - val AUC: 0.4997   - loss: 326.198  - train AUC: 0.5204   - loss: 325.871 \n",
      "Fold: 5 - val AUC: 0.5027   - loss: 326.101  - train AUC: 0.5182   - loss: 325.914 \n",
      "Score:  - val AUC: 0.5026   - loss: 31068.694 - train AUC: 0.5249   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.5993   - loss: 31068.694\n",
      "Cum CV train: 0.6100   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_5', 'var_5_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5330   - loss: 324.015  - train AUC: 0.5345   - loss: 324.508 \n",
      "Fold: 2 - val AUC: 0.5359   - loss: 324.289  - train AUC: 0.5338   - loss: 324.428 \n",
      "Fold: 3 - val AUC: 0.5264   - loss: 324.956  - train AUC: 0.5399   - loss: 324.113 \n",
      "Fold: 4 - val AUC: 0.5225   - loss: 324.896  - train AUC: 0.5353   - loss: 324.316 \n",
      "Fold: 5 - val AUC: 0.5260   - loss: 324.688  - train AUC: 0.5335   - loss: 324.404 \n",
      "Score:  - val AUC: 0.5276   - loss: 31068.694 - train AUC: 0.5375   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6084   - loss: 31068.694\n",
      "Cum CV train: 0.6200   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_6', 'var_6_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5540   - loss: 323.534  - train AUC: 0.5649   - loss: 322.983 \n",
      "Fold: 2 - val AUC: 0.5620   - loss: 322.956  - train AUC: 0.5619   - loss: 323.101 \n",
      "Fold: 3 - val AUC: 0.5609   - loss: 323.207  - train AUC: 0.5622   - loss: 323.058 \n",
      "Fold: 4 - val AUC: 0.5589   - loss: 323.225  - train AUC: 0.5646   - loss: 322.990 \n",
      "Fold: 5 - val AUC: 0.5596   - loss: 323.270  - train AUC: 0.5644   - loss: 323.026 \n",
      "Score:  - val AUC: 0.5581   - loss: 31068.694 - train AUC: 0.5659   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6285   - loss: 31068.694\n",
      "Cum CV train: 0.6388   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_7', 'var_7_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6285   - loss: 31068.694\n",
      "Cum CV train: 0.6388   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_8', 'var_8_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5156   - loss: 325.846  - train AUC: 0.5267   - loss: 325.660 \n",
      "Fold: 2 - val AUC: 0.5143   - loss: 325.843  - train AUC: 0.5253   - loss: 325.696 \n",
      "Fold: 3 - val AUC: 0.5098   - loss: 326.079  - train AUC: 0.5276   - loss: 325.612 \n",
      "Fold: 4 - val AUC: 0.5252   - loss: 325.666  - train AUC: 0.5277   - loss: 325.618 \n",
      "Fold: 5 - val AUC: 0.5186   - loss: 325.735  - train AUC: 0.5241   - loss: 325.698 \n",
      "Score:  - val AUC: 0.5161   - loss: 31068.694 - train AUC: 0.5304   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6305   - loss: 31068.694\n",
      "Cum CV train: 0.6417   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_9', 'var_9_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5438   - loss: 324.741  - train AUC: 0.5483   - loss: 324.133 \n",
      "Fold: 2 - val AUC: 0.5392   - loss: 324.530  - train AUC: 0.5486   - loss: 324.189 \n",
      "Fold: 3 - val AUC: 0.5404   - loss: 324.391  - train AUC: 0.5478   - loss: 324.325 \n",
      "Fold: 4 - val AUC: 0.5478   - loss: 324.019  - train AUC: 0.5482   - loss: 324.314 \n",
      "Fold: 5 - val AUC: 0.5458   - loss: 324.703  - train AUC: 0.5473   - loss: 324.158 \n",
      "Score:  - val AUC: 0.5431   - loss: 31068.694 - train AUC: 0.5499   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6412   - loss: 31068.694\n",
      "Cum CV train: 0.6527   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_10', 'var_10_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6412   - loss: 31068.694\n",
      "Cum CV train: 0.6527   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_11', 'var_11_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5125   - loss: 325.909  - train AUC: 0.5285   - loss: 325.396 \n",
      "Fold: 2 - val AUC: 0.5230   - loss: 325.657  - train AUC: 0.5293   - loss: 325.355 \n",
      "Fold: 3 - val AUC: 0.5251   - loss: 325.593  - train AUC: 0.5307   - loss: 325.321 \n",
      "Fold: 4 - val AUC: 0.5187   - loss: 325.429  - train AUC: 0.5291   - loss: 325.455 \n",
      "Fold: 5 - val AUC: 0.5173   - loss: 325.820  - train AUC: 0.5283   - loss: 325.373 \n",
      "Score:  - val AUC: 0.5187   - loss: 31068.694 - train AUC: 0.5309   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6437   - loss: 31068.694\n",
      "Cum CV train: 0.6567   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_12', 'var_12_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5540   - loss: 323.411  - train AUC: 0.5672   - loss: 322.234 \n",
      "Fold: 2 - val AUC: 0.5679   - loss: 322.053  - train AUC: 0.5635   - loss: 322.572 \n",
      "Fold: 3 - val AUC: 0.5528   - loss: 323.602  - train AUC: 0.5655   - loss: 322.260 \n",
      "Fold: 4 - val AUC: 0.5571   - loss: 322.422  - train AUC: 0.5658   - loss: 322.517 \n",
      "Fold: 5 - val AUC: 0.5694   - loss: 321.897  - train AUC: 0.5635   - loss: 322.627 \n",
      "Score:  - val AUC: 0.5589   - loss: 31068.694 - train AUC: 0.5667   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6605   - loss: 31068.694\n",
      "Cum CV train: 0.6730   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_13', 'var_13_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5572   - loss: 323.231  - train AUC: 0.5582   - loss: 323.256 \n",
      "Fold: 2 - val AUC: 0.5575   - loss: 323.422  - train AUC: 0.5572   - loss: 323.252 \n",
      "Fold: 3 - val AUC: 0.5543   - loss: 323.417  - train AUC: 0.5573   - loss: 323.313 \n",
      "Fold: 4 - val AUC: 0.5536   - loss: 323.444  - train AUC: 0.5596   - loss: 323.202 \n",
      "Fold: 5 - val AUC: 0.5503   - loss: 323.678  - train AUC: 0.5583   - loss: 323.270 \n",
      "Score:  - val AUC: 0.5544   - loss: 31068.694 - train AUC: 0.5589   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6718   - loss: 31068.694\n",
      "Cum CV train: 0.6841   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_14', 'var_14_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6718   - loss: 31068.694\n",
      "Cum CV train: 0.6841   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_15', 'var_15_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5160   - loss: 325.947  - train AUC: 0.5262   - loss: 325.717 \n",
      "Fold: 2 - val AUC: 0.5145   - loss: 325.964  - train AUC: 0.5251   - loss: 325.736 \n",
      "Fold: 3 - val AUC: 0.5165   - loss: 325.930  - train AUC: 0.5247   - loss: 325.735 \n",
      "Fold: 4 - val AUC: 0.5109   - loss: 325.918  - train AUC: 0.5236   - loss: 325.807 \n",
      "Fold: 5 - val AUC: 0.5169   - loss: 325.914  - train AUC: 0.5231   - loss: 325.780 \n",
      "Score:  - val AUC: 0.5146   - loss: 31068.694 - train AUC: 0.5277   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6728   - loss: 31068.694\n",
      "Cum CV train: 0.6858   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_16', 'var_16_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5047   - loss: 326.208  - train AUC: 0.5194   - loss: 325.889 \n",
      "Fold: 2 - val AUC: 0.5002   - loss: 326.166  - train AUC: 0.5199   - loss: 325.907 \n",
      "Fold: 3 - val AUC: 0.5124   - loss: 326.014  - train AUC: 0.5194   - loss: 325.905 \n",
      "Fold: 4 - val AUC: 0.5107   - loss: 326.028  - train AUC: 0.5201   - loss: 325.936 \n",
      "Fold: 5 - val AUC: 0.5077   - loss: 326.057  - train AUC: 0.5186   - loss: 325.955 \n",
      "Score:  - val AUC: 0.5060   - loss: 31068.694 - train AUC: 0.5255   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6731   - loss: 31068.694\n",
      "Cum CV train: 0.6867   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_17', 'var_17_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6731   - loss: 31068.694\n",
      "Cum CV train: 0.6867   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_18', 'var_18_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5377   - loss: 324.374  - train AUC: 0.5433   - loss: 323.970 \n",
      "Fold: 2 - val AUC: 0.5363   - loss: 323.889  - train AUC: 0.5452   - loss: 324.060 \n",
      "Fold: 3 - val AUC: 0.5443   - loss: 324.211  - train AUC: 0.5457   - loss: 323.844 \n",
      "Fold: 4 - val AUC: 0.5351   - loss: 324.092  - train AUC: 0.5487   - loss: 323.819 \n",
      "Fold: 5 - val AUC: 0.5442   - loss: 324.483  - train AUC: 0.5452   - loss: 323.779 \n",
      "Score:  - val AUC: 0.5387   - loss: 31068.694 - train AUC: 0.5478   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6808   - loss: 31068.694\n",
      "Cum CV train: 0.6952   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_19', 'var_19_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5140   - loss: 326.017  - train AUC: 0.5204   - loss: 325.733 \n",
      "Fold: 2 - val AUC: 0.5044   - loss: 325.991  - train AUC: 0.5218   - loss: 325.711 \n",
      "Fold: 3 - val AUC: 0.5063   - loss: 325.896  - train AUC: 0.5210   - loss: 325.766 \n",
      "Fold: 4 - val AUC: 0.5150   - loss: 325.746  - train AUC: 0.5218   - loss: 325.757 \n",
      "Fold: 5 - val AUC: 0.5158   - loss: 325.829  - train AUC: 0.5214   - loss: 325.768 \n",
      "Score:  - val AUC: 0.5103   - loss: 31068.694 - train AUC: 0.5240   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6818   - loss: 31068.694\n",
      "Cum CV train: 0.6968   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_20', 'var_20_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5239   - loss: 325.548  - train AUC: 0.5214   - loss: 325.660 \n",
      "Fold: 2 - val AUC: 0.5188   - loss: 325.774  - train AUC: 0.5270   - loss: 325.491 \n",
      "Fold: 3 - val AUC: 0.5172   - loss: 325.838  - train AUC: 0.5263   - loss: 325.562 \n",
      "Fold: 4 - val AUC: 0.5242   - loss: 325.546  - train AUC: 0.5297   - loss: 325.521 \n",
      "Fold: 5 - val AUC: 0.5101   - loss: 325.900  - train AUC: 0.5275   - loss: 325.515 \n",
      "Score:  - val AUC: 0.5183   - loss: 31068.694 - train AUC: 0.5284   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6837   - loss: 31068.694\n",
      "Cum CV train: 0.6992   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_21', 'var_21_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5580   - loss: 323.203  - train AUC: 0.5638   - loss: 322.989 \n",
      "Fold: 2 - val AUC: 0.5549   - loss: 323.363  - train AUC: 0.5622   - loss: 323.206 \n",
      "Fold: 3 - val AUC: 0.5610   - loss: 323.370  - train AUC: 0.5631   - loss: 322.909 \n",
      "Fold: 4 - val AUC: 0.5593   - loss: 323.084  - train AUC: 0.5627   - loss: 323.087 \n",
      "Fold: 5 - val AUC: 0.5553   - loss: 323.664  - train AUC: 0.5624   - loss: 322.942 \n",
      "Score:  - val AUC: 0.5574   - loss: 31068.694 - train AUC: 0.5642   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.6945   - loss: 31068.694\n",
      "Cum CV train: 0.7099   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_22', 'var_22_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5516   - loss: 323.833  - train AUC: 0.5578   - loss: 322.865 \n",
      "Fold: 2 - val AUC: 0.5553   - loss: 322.838  - train AUC: 0.5615   - loss: 322.893 \n",
      "Fold: 3 - val AUC: 0.5500   - loss: 323.396  - train AUC: 0.5596   - loss: 322.872 \n",
      "Fold: 4 - val AUC: 0.5630   - loss: 322.521  - train AUC: 0.5588   - loss: 323.069 \n",
      "Fold: 5 - val AUC: 0.5454   - loss: 323.709  - train AUC: 0.5595   - loss: 322.934 \n",
      "Score:  - val AUC: 0.5525   - loss: 31068.694 - train AUC: 0.5611   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7039   - loss: 31068.694\n",
      "Cum CV train: 0.7194   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_23', 'var_23_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5220   - loss: 325.693  - train AUC: 0.5286   - loss: 325.547 \n",
      "Fold: 2 - val AUC: 0.5255   - loss: 325.705  - train AUC: 0.5291   - loss: 325.495 \n",
      "Fold: 3 - val AUC: 0.5230   - loss: 325.765  - train AUC: 0.5272   - loss: 325.540 \n",
      "Fold: 4 - val AUC: 0.5245   - loss: 325.652  - train AUC: 0.5280   - loss: 325.607 \n",
      "Fold: 5 - val AUC: 0.5241   - loss: 325.750  - train AUC: 0.5283   - loss: 325.546 \n",
      "Score:  - val AUC: 0.5233   - loss: 31068.694 - train AUC: 0.5299   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7054   - loss: 31068.694\n",
      "Cum CV train: 0.7214   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_24', 'var_24_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5349   - loss: 325.332  - train AUC: 0.5387   - loss: 325.155 \n",
      "Fold: 2 - val AUC: 0.5253   - loss: 325.402  - train AUC: 0.5402   - loss: 325.076 \n",
      "Fold: 3 - val AUC: 0.5241   - loss: 325.611  - train AUC: 0.5389   - loss: 325.113 \n",
      "Fold: 4 - val AUC: 0.5320   - loss: 325.526  - train AUC: 0.5341   - loss: 325.245 \n",
      "Fold: 5 - val AUC: 0.5267   - loss: 325.510  - train AUC: 0.5374   - loss: 325.209 \n",
      "Score:  - val AUC: 0.5282   - loss: 31068.694 - train AUC: 0.5402   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7076   - loss: 31068.694\n",
      "Cum CV train: 0.7243   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_25', 'var_25_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5080   - loss: 326.167  - train AUC: 0.5193   - loss: 325.909 \n",
      "Fold: 2 - val AUC: 0.5018   - loss: 326.241  - train AUC: 0.5197   - loss: 325.889 \n",
      "Fold: 3 - val AUC: 0.5113   - loss: 326.083  - train AUC: 0.5191   - loss: 325.902 \n",
      "Fold: 4 - val AUC: 0.5071   - loss: 326.016  - train AUC: 0.5194   - loss: 325.933 \n",
      "Fold: 5 - val AUC: 0.5133   - loss: 326.002  - train AUC: 0.5198   - loss: 325.889 \n",
      "Score:  - val AUC: 0.5075   - loss: 31068.694 - train AUC: 0.5232   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7078   - loss: 31068.694\n",
      "Cum CV train: 0.7251   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_26', 'var_26_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5517   - loss: 322.551  - train AUC: 0.5639   - loss: 322.201 \n",
      "Fold: 2 - val AUC: 0.5628   - loss: 322.659  - train AUC: 0.5639   - loss: 322.011 \n",
      "Fold: 3 - val AUC: 0.5660   - loss: 321.627  - train AUC: 0.5616   - loss: 322.326 \n",
      "Fold: 4 - val AUC: 0.5587   - loss: 322.644  - train AUC: 0.5645   - loss: 322.039 \n",
      "Fold: 5 - val AUC: 0.5531   - loss: 322.653  - train AUC: 0.5642   - loss: 322.117 \n",
      "Score:  - val AUC: 0.5583   - loss: 31068.694 - train AUC: 0.5647   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7184   - loss: 31068.694\n",
      "Cum CV train: 0.7355   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_27', 'var_27_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.4990   - loss: 623.076  - train AUC: 0.5012   - loss: 623.079 \n",
      "Score:  - val AUC: 0.4997   - loss: 31068.694 - train AUC: 0.5012   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7184   - loss: 31068.694\n",
      "Cum CV train: 0.7355   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_28', 'var_28_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5211   - loss: 325.902  - train AUC: 0.5295   - loss: 325.597 \n",
      "Fold: 2 - val AUC: 0.5199   - loss: 325.858  - train AUC: 0.5300   - loss: 325.600 \n",
      "Fold: 3 - val AUC: 0.5243   - loss: 325.711  - train AUC: 0.5303   - loss: 325.616 \n",
      "Fold: 4 - val AUC: 0.5212   - loss: 325.759  - train AUC: 0.5312   - loss: 325.603 \n",
      "Fold: 5 - val AUC: 0.5246   - loss: 325.710  - train AUC: 0.5308   - loss: 325.606 \n",
      "Score:  - val AUC: 0.5220   - loss: 31068.694 - train AUC: 0.5325   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7197   - loss: 31068.694\n",
      "Cum CV train: 0.7373   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_29', 'var_29_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7197   - loss: 31068.694\n",
      "Cum CV train: 0.7373   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_30', 'var_30_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7197   - loss: 31068.694\n",
      "Cum CV train: 0.7373   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_31', 'var_31_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5301   - loss: 325.754  - train AUC: 0.5315   - loss: 325.608 \n",
      "Fold: 2 - val AUC: 0.5264   - loss: 325.789  - train AUC: 0.5322   - loss: 325.572 \n",
      "Fold: 3 - val AUC: 0.5264   - loss: 325.794  - train AUC: 0.5295   - loss: 325.667 \n",
      "Fold: 4 - val AUC: 0.5273   - loss: 325.770  - train AUC: 0.5304   - loss: 325.649 \n",
      "Fold: 5 - val AUC: 0.5245   - loss: 325.793  - train AUC: 0.5316   - loss: 325.637 \n",
      "Score:  - val AUC: 0.5265   - loss: 31068.694 - train AUC: 0.5323   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7208   - loss: 31068.694\n",
      "Cum CV train: 0.7387   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_32', 'var_32_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5305   - loss: 325.405  - train AUC: 0.5322   - loss: 325.033 \n",
      "Fold: 2 - val AUC: 0.5264   - loss: 325.140  - train AUC: 0.5330   - loss: 325.096 \n",
      "Fold: 3 - val AUC: 0.5348   - loss: 325.177  - train AUC: 0.5314   - loss: 325.101 \n",
      "Fold: 4 - val AUC: 0.5267   - loss: 325.303  - train AUC: 0.5348   - loss: 325.033 \n",
      "Fold: 5 - val AUC: 0.5254   - loss: 325.257  - train AUC: 0.5381   - loss: 324.930 \n",
      "Score:  - val AUC: 0.5281   - loss: 31068.694 - train AUC: 0.5354   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7239   - loss: 31068.694\n",
      "Cum CV train: 0.7421   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_33', 'var_33_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5329   - loss: 324.409  - train AUC: 0.5504   - loss: 323.560 \n",
      "Fold: 2 - val AUC: 0.5509   - loss: 323.840  - train AUC: 0.5477   - loss: 323.568 \n",
      "Fold: 3 - val AUC: 0.5385   - loss: 323.771  - train AUC: 0.5484   - loss: 323.720 \n",
      "Fold: 4 - val AUC: 0.5396   - loss: 324.268  - train AUC: 0.5480   - loss: 323.591 \n",
      "Fold: 5 - val AUC: 0.5411   - loss: 323.941  - train AUC: 0.5465   - loss: 323.762 \n",
      "Score:  - val AUC: 0.5399   - loss: 31068.694 - train AUC: 0.5505   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7298   - loss: 31068.694\n",
      "Cum CV train: 0.7483   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_34', 'var_34_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5467   - loss: 324.212  - train AUC: 0.5505   - loss: 324.145 \n",
      "Fold: 2 - val AUC: 0.5418   - loss: 324.796  - train AUC: 0.5512   - loss: 324.088 \n",
      "Fold: 3 - val AUC: 0.5514   - loss: 324.055  - train AUC: 0.5494   - loss: 324.266 \n",
      "Fold: 4 - val AUC: 0.5463   - loss: 324.383  - train AUC: 0.5497   - loss: 324.174 \n",
      "Fold: 5 - val AUC: 0.5458   - loss: 324.398  - train AUC: 0.5503   - loss: 324.194 \n",
      "Score:  - val AUC: 0.5452   - loss: 31068.694 - train AUC: 0.5513   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7348   - loss: 31068.694\n",
      "Cum CV train: 0.7532   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_35', 'var_35_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5395   - loss: 324.911  - train AUC: 0.5416   - loss: 324.953 \n",
      "Fold: 2 - val AUC: 0.5279   - loss: 325.542  - train AUC: 0.5418   - loss: 324.900 \n",
      "Fold: 3 - val AUC: 0.5326   - loss: 325.398  - train AUC: 0.5396   - loss: 324.931 \n",
      "Fold: 4 - val AUC: 0.5417   - loss: 324.991  - train AUC: 0.5408   - loss: 324.942 \n",
      "Fold: 5 - val AUC: 0.5358   - loss: 324.902  - train AUC: 0.5396   - loss: 325.063 \n",
      "Score:  - val AUC: 0.5348   - loss: 31068.694 - train AUC: 0.5428   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7378   - loss: 31068.694\n",
      "Cum CV train: 0.7565   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_36', 'var_36_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5363   - loss: 325.424  - train AUC: 0.5449   - loss: 324.674 \n",
      "Fold: 2 - val AUC: 0.5407   - loss: 324.933  - train AUC: 0.5425   - loss: 324.868 \n",
      "Fold: 3 - val AUC: 0.5389   - loss: 324.864  - train AUC: 0.5432   - loss: 324.831 \n",
      "Fold: 4 - val AUC: 0.5364   - loss: 325.054  - train AUC: 0.5437   - loss: 324.856 \n",
      "Fold: 5 - val AUC: 0.5399   - loss: 324.965  - train AUC: 0.5432   - loss: 324.801 \n",
      "Score:  - val AUC: 0.5379   - loss: 31068.694 - train AUC: 0.5448   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7409   - loss: 31068.694\n",
      "Cum CV train: 0.7599   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_37', 'var_37_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5027   - loss: 326.200  - train AUC: 0.5205   - loss: 325.899 \n",
      "Fold: 2 - val AUC: 0.5051   - loss: 326.169  - train AUC: 0.5227   - loss: 325.852 \n",
      "Fold: 3 - val AUC: 0.4928   - loss: 326.218  - train AUC: 0.5185   - loss: 325.884 \n",
      "Fold: 4 - val AUC: 0.5062   - loss: 326.003  - train AUC: 0.5186   - loss: 325.969 \n",
      "Fold: 5 - val AUC: 0.5094   - loss: 325.997  - train AUC: 0.5200   - loss: 325.940 \n",
      "Score:  - val AUC: 0.5030   - loss: 31068.694 - train AUC: 0.5244   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7411   - loss: 31068.694\n",
      "Cum CV train: 0.7606   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_38', 'var_38_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7411   - loss: 31068.694\n",
      "Cum CV train: 0.7606   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_39', 'var_39_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7411   - loss: 31068.694\n",
      "Cum CV train: 0.7606   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_40', 'var_40_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5393   - loss: 324.322  - train AUC: 0.5516   - loss: 323.400 \n",
      "Fold: 2 - val AUC: 0.5436   - loss: 324.120  - train AUC: 0.5494   - loss: 323.419 \n",
      "Fold: 3 - val AUC: 0.5516   - loss: 323.293  - train AUC: 0.5497   - loss: 323.528 \n",
      "Fold: 4 - val AUC: 0.5450   - loss: 323.592  - train AUC: 0.5502   - loss: 323.503 \n",
      "Fold: 5 - val AUC: 0.5514   - loss: 323.058  - train AUC: 0.5486   - loss: 323.673 \n",
      "Score:  - val AUC: 0.5454   - loss: 31068.694 - train AUC: 0.5514   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7472   - loss: 31068.694\n",
      "Cum CV train: 0.7663   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_41', 'var_41_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.4993   - loss: 326.263  - train AUC: 0.5196   - loss: 326.001 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.4996   - loss: 31068.694 - train AUC: 0.5165   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7472   - loss: 31068.694\n",
      "Cum CV train: 0.7664   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_42', 'var_42_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5028   - loss: 326.199  - train AUC: 0.5162   - loss: 325.989 \n",
      "Fold: 5 - val AUC: 0.4992   - loss: 655.992  - train AUC: 0.5089   - loss: 655.990 \n",
      "Score:  - val AUC: 0.4993   - loss: 31068.694 - train AUC: 0.5141   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7471   - loss: 31068.694\n",
      "Cum CV train: 0.7666   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_43', 'var_43_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5308   - loss: 325.343  - train AUC: 0.5325   - loss: 325.371 \n",
      "Fold: 2 - val AUC: 0.5232   - loss: 325.516  - train AUC: 0.5325   - loss: 325.383 \n",
      "Fold: 3 - val AUC: 0.5236   - loss: 325.582  - train AUC: 0.5328   - loss: 325.366 \n",
      "Fold: 4 - val AUC: 0.5241   - loss: 325.622  - train AUC: 0.5331   - loss: 325.326 \n",
      "Fold: 5 - val AUC: 0.5205   - loss: 325.679  - train AUC: 0.5332   - loss: 325.324 \n",
      "Score:  - val AUC: 0.5245   - loss: 31068.694 - train AUC: 0.5342   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7486   - loss: 31068.694\n",
      "Cum CV train: 0.7683   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_44', 'var_44_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5476   - loss: 322.687  - train AUC: 0.5503   - loss: 323.371 \n",
      "Fold: 2 - val AUC: 0.5403   - loss: 323.482  - train AUC: 0.5527   - loss: 323.166 \n",
      "Fold: 3 - val AUC: 0.5378   - loss: 323.859  - train AUC: 0.5510   - loss: 323.103 \n",
      "Fold: 4 - val AUC: 0.5525   - loss: 323.384  - train AUC: 0.5524   - loss: 323.083 \n",
      "Fold: 5 - val AUC: 0.5460   - loss: 323.461  - train AUC: 0.5502   - loss: 323.171 \n",
      "Score:  - val AUC: 0.5449   - loss: 31068.694 - train AUC: 0.5541   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7555   - loss: 31068.694\n",
      "Cum CV train: 0.7748   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_45', 'var_45_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5197   - loss: 325.576  - train AUC: 0.5313   - loss: 325.402 \n",
      "Fold: 2 - val AUC: 0.5150   - loss: 325.824  - train AUC: 0.5288   - loss: 325.406 \n",
      "Fold: 3 - val AUC: 0.5148   - loss: 325.778  - train AUC: 0.5296   - loss: 325.450 \n",
      "Fold: 4 - val AUC: 0.5209   - loss: 325.581  - train AUC: 0.5281   - loss: 325.475 \n",
      "Fold: 5 - val AUC: 0.5184   - loss: 325.632  - train AUC: 0.5303   - loss: 325.436 \n",
      "Score:  - val AUC: 0.5176   - loss: 31068.694 - train AUC: 0.5325   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7569   - loss: 31068.694\n",
      "Cum CV train: 0.7766   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_46', 'var_46_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7569   - loss: 31068.694\n",
      "Cum CV train: 0.7766   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_47', 'var_47_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7569   - loss: 31068.694\n",
      "Cum CV train: 0.7766   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_48', 'var_48_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5329   - loss: 325.192  - train AUC: 0.5366   - loss: 325.205 \n",
      "Fold: 2 - val AUC: 0.5317   - loss: 325.214  - train AUC: 0.5364   - loss: 325.207 \n",
      "Fold: 3 - val AUC: 0.5351   - loss: 325.381  - train AUC: 0.5352   - loss: 325.177 \n",
      "Fold: 4 - val AUC: 0.5293   - loss: 325.604  - train AUC: 0.5367   - loss: 325.145 \n",
      "Fold: 5 - val AUC: 0.5355   - loss: 325.174  - train AUC: 0.5356   - loss: 325.190 \n",
      "Score:  - val AUC: 0.5324   - loss: 31068.694 - train AUC: 0.5373   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7590   - loss: 31068.694\n",
      "Cum CV train: 0.7788   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_49', 'var_49_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5277   - loss: 324.941  - train AUC: 0.5335   - loss: 324.803 \n",
      "Fold: 2 - val AUC: 0.5311   - loss: 325.109  - train AUC: 0.5307   - loss: 324.740 \n",
      "Fold: 3 - val AUC: 0.5238   - loss: 325.171  - train AUC: 0.5325   - loss: 324.777 \n",
      "Fold: 4 - val AUC: 0.5320   - loss: 324.613  - train AUC: 0.5371   - loss: 324.733 \n",
      "Fold: 5 - val AUC: 0.5270   - loss: 324.981  - train AUC: 0.5339   - loss: 324.792 \n",
      "Score:  - val AUC: 0.5278   - loss: 31068.694 - train AUC: 0.5364   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7617   - loss: 31068.694\n",
      "Cum CV train: 0.7816   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_50', 'var_50_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5111   - loss: 326.072  - train AUC: 0.5311   - loss: 325.614 \n",
      "Fold: 2 - val AUC: 0.5146   - loss: 325.969  - train AUC: 0.5286   - loss: 325.694 \n",
      "Fold: 3 - val AUC: 0.5116   - loss: 326.074  - train AUC: 0.5301   - loss: 325.693 \n",
      "Fold: 4 - val AUC: 0.5148   - loss: 325.914  - train AUC: 0.5299   - loss: 325.731 \n",
      "Fold: 5 - val AUC: 0.5050   - loss: 326.162  - train AUC: 0.5332   - loss: 325.576 \n",
      "Score:  - val AUC: 0.5110   - loss: 31068.694 - train AUC: 0.5344   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7620   - loss: 31068.694\n",
      "Cum CV train: 0.7827   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_51', 'var_51_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5160   - loss: 325.228  - train AUC: 0.5353   - loss: 324.851 \n",
      "Fold: 2 - val AUC: 0.5212   - loss: 325.251  - train AUC: 0.5353   - loss: 324.855 \n",
      "Fold: 3 - val AUC: 0.5279   - loss: 324.953  - train AUC: 0.5329   - loss: 324.971 \n",
      "Fold: 4 - val AUC: 0.5275   - loss: 325.223  - train AUC: 0.5331   - loss: 324.894 \n",
      "Fold: 5 - val AUC: 0.5210   - loss: 325.223  - train AUC: 0.5393   - loss: 324.729 \n",
      "Score:  - val AUC: 0.5223   - loss: 31068.694 - train AUC: 0.5388   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7641   - loss: 31068.694\n",
      "Cum CV train: 0.7852   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_52', 'var_52_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5267   - loss: 325.619  - train AUC: 0.5360   - loss: 325.165 \n",
      "Fold: 2 - val AUC: 0.5275   - loss: 325.666  - train AUC: 0.5361   - loss: 325.138 \n",
      "Fold: 3 - val AUC: 0.5319   - loss: 325.231  - train AUC: 0.5355   - loss: 325.270 \n",
      "Fold: 4 - val AUC: 0.5307   - loss: 325.487  - train AUC: 0.5358   - loss: 325.225 \n",
      "Fold: 5 - val AUC: 0.5354   - loss: 325.135  - train AUC: 0.5373   - loss: 325.165 \n",
      "Score:  - val AUC: 0.5296   - loss: 31068.694 - train AUC: 0.5377   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7661   - loss: 31068.694\n",
      "Cum CV train: 0.7875   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_53', 'var_53_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5519   - loss: 323.929  - train AUC: 0.5614   - loss: 322.647 \n",
      "Fold: 2 - val AUC: 0.5666   - loss: 322.251  - train AUC: 0.5587   - loss: 323.021 \n",
      "Fold: 3 - val AUC: 0.5548   - loss: 323.077  - train AUC: 0.5609   - loss: 322.843 \n",
      "Fold: 4 - val AUC: 0.5545   - loss: 322.925  - train AUC: 0.5612   - loss: 322.901 \n",
      "Fold: 5 - val AUC: 0.5502   - loss: 323.323  - train AUC: 0.5603   - loss: 322.847 \n",
      "Score:  - val AUC: 0.5552   - loss: 31068.694 - train AUC: 0.5620   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7730   - loss: 31068.694\n",
      "Cum CV train: 0.7939   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_54', 'var_54_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5170   - loss: 325.923  - train AUC: 0.5254   - loss: 325.743 \n",
      "Fold: 2 - val AUC: 0.5222   - loss: 325.897  - train AUC: 0.5267   - loss: 325.692 \n",
      "Fold: 3 - val AUC: 0.5171   - loss: 325.976  - train AUC: 0.5262   - loss: 325.695 \n",
      "Fold: 4 - val AUC: 0.5227   - loss: 325.720  - train AUC: 0.5282   - loss: 325.666 \n",
      "Fold: 5 - val AUC: 0.5186   - loss: 325.964  - train AUC: 0.5263   - loss: 325.702 \n",
      "Score:  - val AUC: 0.5190   - loss: 31068.694 - train AUC: 0.5287   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7736   - loss: 31068.694\n",
      "Cum CV train: 0.7948   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_55', 'var_55_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5137   - loss: 325.839  - train AUC: 0.5219   - loss: 325.679 \n",
      "Fold: 2 - val AUC: 0.5157   - loss: 325.861  - train AUC: 0.5208   - loss: 325.682 \n",
      "Fold: 3 - val AUC: 0.5103   - loss: 325.900  - train AUC: 0.5227   - loss: 325.652 \n",
      "Fold: 4 - val AUC: 0.5094   - loss: 325.873  - train AUC: 0.5212   - loss: 325.678 \n",
      "Fold: 5 - val AUC: 0.5170   - loss: 325.786  - train AUC: 0.5196   - loss: 325.712 \n",
      "Score:  - val AUC: 0.5121   - loss: 31068.694 - train AUC: 0.5230   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7743   - loss: 31068.694\n",
      "Cum CV train: 0.7958   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_56', 'var_56_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5269   - loss: 325.088  - train AUC: 0.5360   - loss: 324.576 \n",
      "Fold: 2 - val AUC: 0.5261   - loss: 324.997  - train AUC: 0.5347   - loss: 324.639 \n",
      "Fold: 3 - val AUC: 0.5279   - loss: 324.669  - train AUC: 0.5348   - loss: 324.701 \n",
      "Fold: 4 - val AUC: 0.5276   - loss: 324.898  - train AUC: 0.5356   - loss: 324.608 \n",
      "Fold: 5 - val AUC: 0.5296   - loss: 324.785  - train AUC: 0.5344   - loss: 324.632 \n",
      "Score:  - val AUC: 0.5268   - loss: 31068.694 - train AUC: 0.5362   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7766   - loss: 31068.694\n",
      "Cum CV train: 0.7982   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_57', 'var_57_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5213   - loss: 325.928  - train AUC: 0.5199   - loss: 325.897 \n",
      "Fold: 2 - val AUC: 0.5137   - loss: 326.046  - train AUC: 0.5253   - loss: 325.782 \n",
      "Fold: 3 - val AUC: 0.5129   - loss: 326.040  - train AUC: 0.5231   - loss: 325.839 \n",
      "Fold: 4 - val AUC: 0.5114   - loss: 326.070  - train AUC: 0.5250   - loss: 325.808 \n",
      "Fold: 5 - val AUC: 0.5096   - loss: 326.068  - train AUC: 0.5239   - loss: 325.844 \n",
      "Score:  - val AUC: 0.5133   - loss: 31068.694 - train AUC: 0.5256   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7769   - loss: 31068.694\n",
      "Cum CV train: 0.7989   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_58', 'var_58_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5213   - loss: 325.905  - train AUC: 0.5338   - loss: 325.395 \n",
      "Fold: 2 - val AUC: 0.5236   - loss: 325.905  - train AUC: 0.5327   - loss: 325.447 \n",
      "Fold: 3 - val AUC: 0.5299   - loss: 325.536  - train AUC: 0.5310   - loss: 325.510 \n",
      "Fold: 4 - val AUC: 0.5275   - loss: 325.553  - train AUC: 0.5318   - loss: 325.507 \n",
      "Fold: 5 - val AUC: 0.5313   - loss: 325.433  - train AUC: 0.5308   - loss: 325.522 \n",
      "Score:  - val AUC: 0.5259   - loss: 31068.694 - train AUC: 0.5331   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7780   - loss: 31068.694\n",
      "Cum CV train: 0.8003   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_59', 'var_59_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5094   - loss: 326.179  - train AUC: 0.5178   - loss: 325.835 \n",
      "Fold: 2 - val AUC: 0.5069   - loss: 326.002  - train AUC: 0.5176   - loss: 325.907 \n",
      "Fold: 3 - val AUC: 0.4947   - loss: 326.298  - train AUC: 0.5210   - loss: 325.817 \n",
      "Fold: 4 - val AUC: 0.5077   - loss: 326.038  - train AUC: 0.5197   - loss: 325.869 \n",
      "Fold: 5 - val AUC: 0.5173   - loss: 325.831  - train AUC: 0.5229   - loss: 325.782 \n",
      "Score:  - val AUC: 0.5063   - loss: 31068.694 - train AUC: 0.5238   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7782   - loss: 31068.694\n",
      "Cum CV train: 0.8009   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_60', 'var_60_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5101   - loss: 326.002  - train AUC: 0.5199   - loss: 325.912 \n",
      "Fold: 2 - val AUC: 0.5058   - loss: 326.119  - train AUC: 0.5180   - loss: 325.871 \n",
      "Fold: 3 - val AUC: 0.5055   - loss: 326.120  - train AUC: 0.5186   - loss: 325.882 \n",
      "Fold: 4 - val AUC: 0.5114   - loss: 326.003  - train AUC: 0.5185   - loss: 325.891 \n",
      "Fold: 5 - val AUC: 0.5010   - loss: 326.138  - train AUC: 0.5153   - loss: 325.921 \n",
      "Score:  - val AUC: 0.5064   - loss: 31068.694 - train AUC: 0.5212   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7784   - loss: 31068.694\n",
      "Cum CV train: 0.8015   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_61', 'var_61_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5035   - loss: 326.125  - train AUC: 0.5168   - loss: 325.899 \n",
      "Fold: 2 - val AUC: 0.5020   - loss: 326.068  - train AUC: 0.5181   - loss: 325.914 \n",
      "Fold: 3 - val AUC: 0.5033   - loss: 326.014  - train AUC: 0.5191   - loss: 325.910 \n",
      "Fold: 4 - val AUC: 0.5054   - loss: 326.113  - train AUC: 0.5160   - loss: 325.910 \n",
      "Fold: 5 - val AUC: 0.5073   - loss: 326.014  - train AUC: 0.5195   - loss: 325.900 \n",
      "Score:  - val AUC: 0.5038   - loss: 31068.694 - train AUC: 0.5222   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7785   - loss: 31068.694\n",
      "Cum CV train: 0.8019   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_62', 'var_62_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5141   - loss: 325.968  - train AUC: 0.5229   - loss: 325.820 \n",
      "Fold: 2 - val AUC: 0.5055   - loss: 326.192  - train AUC: 0.5275   - loss: 325.660 \n",
      "Fold: 3 - val AUC: 0.5121   - loss: 325.944  - train AUC: 0.5252   - loss: 325.743 \n",
      "Fold: 4 - val AUC: 0.5069   - loss: 326.155  - train AUC: 0.5247   - loss: 325.735 \n",
      "Fold: 5 - val AUC: 0.5114   - loss: 325.990  - train AUC: 0.5256   - loss: 325.747 \n",
      "Score:  - val AUC: 0.5093   - loss: 31068.694 - train AUC: 0.5291   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7787   - loss: 31068.694\n",
      "Cum CV train: 0.8027   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_63', 'var_63_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5207   - loss: 325.725  - train AUC: 0.5264   - loss: 325.565 \n",
      "Fold: 2 - val AUC: 0.5121   - loss: 325.993  - train AUC: 0.5181   - loss: 325.700 \n",
      "Fold: 3 - val AUC: 0.5068   - loss: 325.898  - train AUC: 0.5220   - loss: 325.672 \n",
      "Fold: 4 - val AUC: 0.5134   - loss: 325.825  - train AUC: 0.5245   - loss: 325.625 \n",
      "Fold: 5 - val AUC: 0.5093   - loss: 325.883  - train AUC: 0.5218   - loss: 325.712 \n",
      "Score:  - val AUC: 0.5122   - loss: 31068.694 - train AUC: 0.5269   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7795   - loss: 31068.694\n",
      "Cum CV train: 0.8038   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_64', 'var_64_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5161   - loss: 326.028  - train AUC: 0.5207   - loss: 325.806 \n",
      "Fold: 2 - val AUC: 0.4985   - loss: 326.357  - train AUC: 0.5206   - loss: 325.772 \n",
      "Fold: 3 - val AUC: 0.5154   - loss: 325.986  - train AUC: 0.5187   - loss: 325.851 \n",
      "Fold: 4 - val AUC: 0.5132   - loss: 325.910  - train AUC: 0.5194   - loss: 325.865 \n",
      "Fold: 5 - val AUC: 0.5148   - loss: 325.960  - train AUC: 0.5180   - loss: 325.882 \n",
      "Score:  - val AUC: 0.5108   - loss: 31068.694 - train AUC: 0.5221   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7797   - loss: 31068.694\n",
      "Cum CV train: 0.8045   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_65', 'var_65_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5079   - loss: 326.145  - train AUC: 0.5186   - loss: 325.959 \n",
      "Fold: 2 - val AUC: 0.5097   - loss: 326.155  - train AUC: 0.5190   - loss: 325.903 \n",
      "Fold: 3 - val AUC: 0.5100   - loss: 326.151  - train AUC: 0.5173   - loss: 325.967 \n",
      "Fold: 4 - val AUC: 0.5095   - loss: 326.079  - train AUC: 0.5175   - loss: 325.947 \n",
      "Fold: 5 - val AUC: 0.5124   - loss: 326.060  - train AUC: 0.5203   - loss: 325.915 \n",
      "Score:  - val AUC: 0.5094   - loss: 31068.694 - train AUC: 0.5201   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7799   - loss: 31068.694\n",
      "Cum CV train: 0.8050   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_66', 'var_66_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5232   - loss: 325.857  - train AUC: 0.5273   - loss: 325.646 \n",
      "Fold: 2 - val AUC: 0.5187   - loss: 325.875  - train AUC: 0.5279   - loss: 325.631 \n",
      "Fold: 3 - val AUC: 0.5237   - loss: 325.795  - train AUC: 0.5261   - loss: 325.671 \n",
      "Fold: 4 - val AUC: 0.5210   - loss: 325.722  - train AUC: 0.5276   - loss: 325.662 \n",
      "Fold: 5 - val AUC: 0.5164   - loss: 325.934  - train AUC: 0.5272   - loss: 325.659 \n",
      "Score:  - val AUC: 0.5196   - loss: 31068.694 - train AUC: 0.5293   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7806   - loss: 31068.694\n",
      "Cum CV train: 0.8060   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_67', 'var_67_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5363   - loss: 325.063  - train AUC: 0.5506   - loss: 324.292 \n",
      "Fold: 2 - val AUC: 0.5413   - loss: 324.700  - train AUC: 0.5500   - loss: 324.305 \n",
      "Fold: 3 - val AUC: 0.5373   - loss: 325.052  - train AUC: 0.5510   - loss: 324.261 \n",
      "Fold: 4 - val AUC: 0.5518   - loss: 324.308  - train AUC: 0.5480   - loss: 324.395 \n",
      "Fold: 5 - val AUC: 0.5571   - loss: 323.908  - train AUC: 0.5487   - loss: 324.412 \n",
      "Score:  - val AUC: 0.5441   - loss: 31068.694 - train AUC: 0.5508   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7838   - loss: 31068.694\n",
      "Cum CV train: 0.8091   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_68', 'var_68_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5029   - loss: 326.125  - train AUC: 0.5202   - loss: 325.914 \n",
      "Fold: 2 - val AUC: 0.5094   - loss: 326.167  - train AUC: 0.5192   - loss: 325.895 \n",
      "Fold: 3 - val AUC: 0.5130   - loss: 326.093  - train AUC: 0.5193   - loss: 325.904 \n",
      "Fold: 4 - val AUC: 0.5122   - loss: 326.043  - train AUC: 0.5226   - loss: 325.886 \n",
      "Fold: 5 - val AUC: 0.5118   - loss: 326.023  - train AUC: 0.5208   - loss: 325.894 \n",
      "Score:  - val AUC: 0.5093   - loss: 31068.694 - train AUC: 0.5239   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7840   - loss: 31068.694\n",
      "Cum CV train: 0.8096   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_69', 'var_69_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5077   - loss: 325.967  - train AUC: 0.5201   - loss: 325.751 \n",
      "Fold: 2 - val AUC: 0.4976   - loss: 326.046  - train AUC: 0.5206   - loss: 325.773 \n",
      "Fold: 3 - val AUC: 0.5070   - loss: 325.917  - train AUC: 0.5223   - loss: 325.762 \n",
      "Fold: 4 - val AUC: 0.5055   - loss: 325.991  - train AUC: 0.5189   - loss: 325.747 \n",
      "Fold: 5 - val AUC: 0.5006   - loss: 326.082  - train AUC: 0.5212   - loss: 325.783 \n",
      "Score:  - val AUC: 0.5032   - loss: 31068.694 - train AUC: 0.5241   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7844   - loss: 31068.694\n",
      "Cum CV train: 0.8104   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_70', 'var_70_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5228   - loss: 325.551  - train AUC: 0.5301   - loss: 325.143 \n",
      "Fold: 2 - val AUC: 0.5279   - loss: 325.237  - train AUC: 0.5310   - loss: 325.209 \n",
      "Fold: 3 - val AUC: 0.5355   - loss: 325.150  - train AUC: 0.5316   - loss: 325.180 \n",
      "Fold: 4 - val AUC: 0.5206   - loss: 325.418  - train AUC: 0.5317   - loss: 325.170 \n",
      "Fold: 5 - val AUC: 0.5278   - loss: 325.328  - train AUC: 0.5318   - loss: 325.162 \n",
      "Score:  - val AUC: 0.5261   - loss: 31068.694 - train AUC: 0.5343   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7861   - loss: 31068.694\n",
      "Cum CV train: 0.8122   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_71', 'var_71_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5247   - loss: 325.867  - train AUC: 0.5378   - loss: 325.194 \n",
      "Fold: 2 - val AUC: 0.5294   - loss: 325.550  - train AUC: 0.5380   - loss: 325.208 \n",
      "Fold: 3 - val AUC: 0.5344   - loss: 325.403  - train AUC: 0.5364   - loss: 325.275 \n",
      "Fold: 4 - val AUC: 0.5354   - loss: 325.330  - train AUC: 0.5375   - loss: 325.246 \n",
      "Fold: 5 - val AUC: 0.5343   - loss: 325.260  - train AUC: 0.5413   - loss: 325.073 \n",
      "Score:  - val AUC: 0.5308   - loss: 31068.694 - train AUC: 0.5401   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7874   - loss: 31068.694\n",
      "Cum CV train: 0.8138   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_72', 'var_72_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5017   - loss: 326.086  - train AUC: 0.5217   - loss: 325.817 \n",
      "Fold: 2 - val AUC: 0.5106   - loss: 326.070  - train AUC: 0.5199   - loss: 325.813 \n",
      "Fold: 3 - val AUC: 0.5143   - loss: 326.063  - train AUC: 0.5185   - loss: 325.842 \n",
      "Fold: 4 - val AUC: 0.5089   - loss: 326.087  - train AUC: 0.5198   - loss: 325.817 \n",
      "Fold: 5 - val AUC: 0.5157   - loss: 325.848  - train AUC: 0.5238   - loss: 325.804 \n",
      "Score:  - val AUC: 0.5093   - loss: 31068.694 - train AUC: 0.5224   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7875   - loss: 31068.694\n",
      "Cum CV train: 0.8143   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_73', 'var_73_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5036   - loss: 326.107  - train AUC: 0.5166   - loss: 325.946 \n",
      "Fold: 2 - val AUC: 0.4996   - loss: 326.156  - train AUC: 0.5166   - loss: 325.923 \n",
      "Fold: 3 - val AUC: 0.5068   - loss: 326.082  - train AUC: 0.5161   - loss: 325.956 \n",
      "Fold: 4 - val AUC: 0.5004   - loss: 326.138  - train AUC: 0.5157   - loss: 325.952 \n",
      "Fold: 5 - val AUC: 0.5038   - loss: 326.065  - train AUC: 0.5170   - loss: 325.951 \n",
      "Score:  - val AUC: 0.5026   - loss: 31068.694 - train AUC: 0.5211   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7877   - loss: 31068.694\n",
      "Cum CV train: 0.8147   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_74', 'var_74_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5243   - loss: 325.813  - train AUC: 0.5298   - loss: 325.502 \n",
      "Fold: 2 - val AUC: 0.5143   - loss: 326.050  - train AUC: 0.5337   - loss: 325.387 \n",
      "Fold: 3 - val AUC: 0.5237   - loss: 325.687  - train AUC: 0.5359   - loss: 325.284 \n",
      "Fold: 4 - val AUC: 0.5189   - loss: 325.714  - train AUC: 0.5318   - loss: 325.453 \n",
      "Fold: 5 - val AUC: 0.5257   - loss: 325.686  - train AUC: 0.5334   - loss: 325.405 \n",
      "Score:  - val AUC: 0.5210   - loss: 31068.694 - train AUC: 0.5356   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7885   - loss: 31068.694\n",
      "Cum CV train: 0.8161   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_75', 'var_75_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5330   - loss: 324.988  - train AUC: 0.5424   - loss: 324.111 \n",
      "Fold: 2 - val AUC: 0.5387   - loss: 324.440  - train AUC: 0.5444   - loss: 324.147 \n",
      "Fold: 3 - val AUC: 0.5385   - loss: 324.296  - train AUC: 0.5470   - loss: 324.084 \n",
      "Fold: 4 - val AUC: 0.5426   - loss: 323.962  - train AUC: 0.5420   - loss: 324.332 \n",
      "Fold: 5 - val AUC: 0.5369   - loss: 324.345  - train AUC: 0.5466   - loss: 324.082 \n",
      "Score:  - val AUC: 0.5377   - loss: 31068.694 - train AUC: 0.5458   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7916   - loss: 31068.694\n",
      "Cum CV train: 0.8191   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_76', 'var_76_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5537   - loss: 323.732  - train AUC: 0.5667   - loss: 322.358 \n",
      "Fold: 2 - val AUC: 0.5588   - loss: 322.814  - train AUC: 0.5655   - loss: 322.560 \n",
      "Fold: 3 - val AUC: 0.5677   - loss: 322.164  - train AUC: 0.5661   - loss: 322.601 \n",
      "Fold: 4 - val AUC: 0.5588   - loss: 322.668  - train AUC: 0.5654   - loss: 322.591 \n",
      "Fold: 5 - val AUC: 0.5614   - loss: 322.879  - train AUC: 0.5657   - loss: 322.477 \n",
      "Score:  - val AUC: 0.5597   - loss: 31068.694 - train AUC: 0.5667   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7972   - loss: 31068.694\n",
      "Cum CV train: 0.8242   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_77', 'var_77_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5091   - loss: 326.127  - train AUC: 0.5243   - loss: 325.675 \n",
      "Fold: 2 - val AUC: 0.5123   - loss: 326.073  - train AUC: 0.5236   - loss: 325.723 \n",
      "Fold: 3 - val AUC: 0.5165   - loss: 325.739  - train AUC: 0.5234   - loss: 325.780 \n",
      "Fold: 4 - val AUC: 0.5185   - loss: 325.793  - train AUC: 0.5238   - loss: 325.746 \n",
      "Fold: 5 - val AUC: 0.5155   - loss: 325.928  - train AUC: 0.5234   - loss: 325.702 \n",
      "Score:  - val AUC: 0.5139   - loss: 31068.694 - train AUC: 0.5250   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.7976   - loss: 31068.694\n",
      "Cum CV train: 0.8249   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_78', 'var_78_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5379   - loss: 324.258  - train AUC: 0.5475   - loss: 323.667 \n",
      "Fold: 2 - val AUC: 0.5444   - loss: 324.036  - train AUC: 0.5478   - loss: 323.637 \n",
      "Fold: 3 - val AUC: 0.5456   - loss: 323.752  - train AUC: 0.5460   - loss: 323.719 \n",
      "Fold: 4 - val AUC: 0.5486   - loss: 323.365  - train AUC: 0.5476   - loss: 323.796 \n",
      "Fold: 5 - val AUC: 0.5398   - loss: 324.110  - train AUC: 0.5477   - loss: 323.663 \n",
      "Score:  - val AUC: 0.5426   - loss: 31068.694 - train AUC: 0.5493   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8015   - loss: 31068.694\n",
      "Cum CV train: 0.8283   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_79', 'var_79_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8015   - loss: 31068.694\n",
      "Cum CV train: 0.8283   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_80', 'var_80_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5611   - loss: 322.644  - train AUC: 0.5641   - loss: 322.400 \n",
      "Fold: 2 - val AUC: 0.5552   - loss: 322.760  - train AUC: 0.5642   - loss: 322.480 \n",
      "Fold: 3 - val AUC: 0.5550   - loss: 323.204  - train AUC: 0.5600   - loss: 322.520 \n",
      "Fold: 4 - val AUC: 0.5707   - loss: 321.784  - train AUC: 0.5613   - loss: 322.689 \n",
      "Fold: 5 - val AUC: 0.5518   - loss: 323.185  - train AUC: 0.5640   - loss: 322.451 \n",
      "Score:  - val AUC: 0.5581   - loss: 31068.694 - train AUC: 0.5644   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8071   - loss: 31068.694\n",
      "Cum CV train: 0.8333   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_81', 'var_81_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5734   - loss: 320.931  - train AUC: 0.5788   - loss: 320.743 \n",
      "Fold: 2 - val AUC: 0.5681   - loss: 321.454  - train AUC: 0.5769   - loss: 320.684 \n",
      "Fold: 3 - val AUC: 0.5680   - loss: 320.892  - train AUC: 0.5777   - loss: 320.877 \n",
      "Fold: 4 - val AUC: 0.5757   - loss: 321.369  - train AUC: 0.5807   - loss: 320.591 \n",
      "Fold: 5 - val AUC: 0.5808   - loss: 320.378  - train AUC: 0.5776   - loss: 320.872 \n",
      "Score:  - val AUC: 0.5731   - loss: 31068.694 - train AUC: 0.5798   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8156   - loss: 31068.694\n",
      "Cum CV train: 0.8404   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_82', 'var_82_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5178   - loss: 325.521  - train AUC: 0.5311   - loss: 325.132 \n",
      "Fold: 2 - val AUC: 0.5236   - loss: 325.140  - train AUC: 0.5332   - loss: 325.085 \n",
      "Fold: 3 - val AUC: 0.5127   - loss: 325.840  - train AUC: 0.5311   - loss: 325.107 \n",
      "Fold: 4 - val AUC: 0.5257   - loss: 325.182  - train AUC: 0.5313   - loss: 325.120 \n",
      "Fold: 5 - val AUC: 0.5222   - loss: 325.478  - train AUC: 0.5282   - loss: 325.173 \n",
      "Score:  - val AUC: 0.5196   - loss: 31068.694 - train AUC: 0.5335   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8170   - loss: 31068.694\n",
      "Cum CV train: 0.8420   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_83', 'var_83_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5197   - loss: 325.513  - train AUC: 0.5290   - loss: 325.117 \n",
      "Fold: 2 - val AUC: 0.5147   - loss: 325.314  - train AUC: 0.5279   - loss: 325.203 \n",
      "Fold: 3 - val AUC: 0.5224   - loss: 325.504  - train AUC: 0.5289   - loss: 325.098 \n",
      "Fold: 4 - val AUC: 0.5179   - loss: 325.408  - train AUC: 0.5289   - loss: 325.109 \n",
      "Fold: 5 - val AUC: 0.5159   - loss: 325.204  - train AUC: 0.5312   - loss: 325.159 \n",
      "Score:  - val AUC: 0.5183   - loss: 31068.694 - train AUC: 0.5316   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8182   - loss: 31068.694\n",
      "Cum CV train: 0.8433   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_84', 'var_84_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5098   - loss: 326.097  - train AUC: 0.5166   - loss: 325.937 \n",
      "Fold: 2 - val AUC: 0.5037   - loss: 326.198  - train AUC: 0.5205   - loss: 325.880 \n",
      "Fold: 3 - val AUC: 0.5101   - loss: 326.113  - train AUC: 0.5194   - loss: 325.933 \n",
      "Fold: 4 - val AUC: 0.5117   - loss: 326.024  - train AUC: 0.5180   - loss: 325.935 \n",
      "Fold: 5 - val AUC: 0.5086   - loss: 326.054  - train AUC: 0.5182   - loss: 325.914 \n",
      "Score:  - val AUC: 0.5076   - loss: 31068.694 - train AUC: 0.5213   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8183   - loss: 31068.694\n",
      "Cum CV train: 0.8436   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_85', 'var_85_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5150   - loss: 325.626  - train AUC: 0.5308   - loss: 325.338 \n",
      "Fold: 2 - val AUC: 0.5205   - loss: 325.526  - train AUC: 0.5273   - loss: 325.441 \n",
      "Fold: 3 - val AUC: 0.5212   - loss: 325.512  - train AUC: 0.5275   - loss: 325.421 \n",
      "Fold: 4 - val AUC: 0.5292   - loss: 325.348  - train AUC: 0.5256   - loss: 325.463 \n",
      "Fold: 5 - val AUC: 0.5181   - loss: 325.881  - train AUC: 0.5303   - loss: 325.334 \n",
      "Score:  - val AUC: 0.5204   - loss: 31068.694 - train AUC: 0.5303   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8192   - loss: 31068.694\n",
      "Cum CV train: 0.8446   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_86', 'var_86_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5301   - loss: 324.092  - train AUC: 0.5480   - loss: 323.888 \n",
      "Fold: 2 - val AUC: 0.5432   - loss: 323.702  - train AUC: 0.5439   - loss: 323.943 \n",
      "Fold: 3 - val AUC: 0.5416   - loss: 324.157  - train AUC: 0.5439   - loss: 323.844 \n",
      "Fold: 4 - val AUC: 0.5455   - loss: 324.122  - train AUC: 0.5463   - loss: 323.715 \n",
      "Fold: 5 - val AUC: 0.5355   - loss: 324.550  - train AUC: 0.5469   - loss: 323.703 \n",
      "Score:  - val AUC: 0.5386   - loss: 31068.694 - train AUC: 0.5472   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8223   - loss: 31068.694\n",
      "Cum CV train: 0.8476   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_87', 'var_87_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5362   - loss: 325.184  - train AUC: 0.5417   - loss: 324.803 \n",
      "Fold: 2 - val AUC: 0.5263   - loss: 325.415  - train AUC: 0.5428   - loss: 324.823 \n",
      "Fold: 3 - val AUC: 0.5445   - loss: 324.742  - train AUC: 0.5403   - loss: 324.924 \n",
      "Fold: 4 - val AUC: 0.5348   - loss: 325.170  - train AUC: 0.5424   - loss: 324.777 \n",
      "Fold: 5 - val AUC: 0.5356   - loss: 325.100  - train AUC: 0.5428   - loss: 324.766 \n",
      "Score:  - val AUC: 0.5349   - loss: 31068.694 - train AUC: 0.5432   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8236   - loss: 31068.694\n",
      "Cum CV train: 0.8491   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_88', 'var_88_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5223   - loss: 325.532  - train AUC: 0.5248   - loss: 325.631 \n",
      "Fold: 2 - val AUC: 0.5196   - loss: 325.695  - train AUC: 0.5244   - loss: 325.605 \n",
      "Fold: 3 - val AUC: 0.5155   - loss: 326.010  - train AUC: 0.5250   - loss: 325.523 \n",
      "Fold: 4 - val AUC: 0.5175   - loss: 325.721  - train AUC: 0.5273   - loss: 325.550 \n",
      "Fold: 5 - val AUC: 0.5181   - loss: 325.672  - train AUC: 0.5256   - loss: 325.582 \n",
      "Score:  - val AUC: 0.5182   - loss: 31068.694 - train AUC: 0.5274   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8242   - loss: 31068.694\n",
      "Cum CV train: 0.8497   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_89', 'var_89_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5270   - loss: 325.301  - train AUC: 0.5417   - loss: 324.620 \n",
      "Fold: 2 - val AUC: 0.5336   - loss: 324.773  - train AUC: 0.5413   - loss: 324.682 \n",
      "Fold: 3 - val AUC: 0.5263   - loss: 325.343  - train AUC: 0.5419   - loss: 324.568 \n",
      "Fold: 4 - val AUC: 0.5358   - loss: 324.865  - train AUC: 0.5407   - loss: 324.710 \n",
      "Fold: 5 - val AUC: 0.5425   - loss: 324.557  - train AUC: 0.5381   - loss: 324.797 \n",
      "Score:  - val AUC: 0.5322   - loss: 31068.694 - train AUC: 0.5417   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8258   - loss: 31068.694\n",
      "Cum CV train: 0.8514   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_90', 'var_90_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5279   - loss: 325.348  - train AUC: 0.5328   - loss: 325.055 \n",
      "Fold: 2 - val AUC: 0.5304   - loss: 325.169  - train AUC: 0.5318   - loss: 325.108 \n",
      "Fold: 3 - val AUC: 0.5265   - loss: 325.345  - train AUC: 0.5332   - loss: 325.060 \n",
      "Fold: 4 - val AUC: 0.5287   - loss: 325.068  - train AUC: 0.5332   - loss: 325.130 \n",
      "Fold: 5 - val AUC: 0.5303   - loss: 325.401  - train AUC: 0.5319   - loss: 325.136 \n",
      "Score:  - val AUC: 0.5281   - loss: 31068.694 - train AUC: 0.5335   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8271   - loss: 31068.694\n",
      "Cum CV train: 0.8527   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_91', 'var_91_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5399   - loss: 324.956  - train AUC: 0.5398   - loss: 324.797 \n",
      "Fold: 2 - val AUC: 0.5268   - loss: 325.468  - train AUC: 0.5408   - loss: 324.737 \n",
      "Fold: 3 - val AUC: 0.5364   - loss: 324.910  - train AUC: 0.5406   - loss: 324.804 \n",
      "Fold: 4 - val AUC: 0.5349   - loss: 325.047  - train AUC: 0.5407   - loss: 324.825 \n",
      "Fold: 5 - val AUC: 0.5395   - loss: 324.706  - train AUC: 0.5395   - loss: 324.857 \n",
      "Score:  - val AUC: 0.5348   - loss: 31068.694 - train AUC: 0.5418   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8288   - loss: 31068.694\n",
      "Cum CV train: 0.8543   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_92', 'var_92_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5358   - loss: 324.280  - train AUC: 0.5493   - loss: 323.785 \n",
      "Fold: 2 - val AUC: 0.5427   - loss: 324.361  - train AUC: 0.5476   - loss: 323.763 \n",
      "Fold: 3 - val AUC: 0.5412   - loss: 324.317  - train AUC: 0.5492   - loss: 323.766 \n",
      "Fold: 4 - val AUC: 0.5439   - loss: 323.780  - train AUC: 0.5485   - loss: 323.893 \n",
      "Fold: 5 - val AUC: 0.5475   - loss: 323.806  - train AUC: 0.5483   - loss: 323.903 \n",
      "Score:  - val AUC: 0.5415   - loss: 31068.694 - train AUC: 0.5500   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8317   - loss: 31068.694\n",
      "Cum CV train: 0.8570   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_93', 'var_93_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5338   - loss: 325.107  - train AUC: 0.5372   - loss: 324.984 \n",
      "Fold: 2 - val AUC: 0.5184   - loss: 325.534  - train AUC: 0.5404   - loss: 324.882 \n",
      "Fold: 3 - val AUC: 0.5244   - loss: 325.336  - train AUC: 0.5378   - loss: 324.954 \n",
      "Fold: 4 - val AUC: 0.5341   - loss: 325.139  - train AUC: 0.5371   - loss: 324.952 \n",
      "Fold: 5 - val AUC: 0.5337   - loss: 325.144  - train AUC: 0.5385   - loss: 324.945 \n",
      "Score:  - val AUC: 0.5281   - loss: 31068.694 - train AUC: 0.5413   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8328   - loss: 31068.694\n",
      "Cum CV train: 0.8582   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_94', 'var_94_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5459   - loss: 324.238  - train AUC: 0.5500   - loss: 323.778 \n",
      "Fold: 2 - val AUC: 0.5387   - loss: 324.114  - train AUC: 0.5507   - loss: 323.777 \n",
      "Fold: 3 - val AUC: 0.5479   - loss: 323.971  - train AUC: 0.5506   - loss: 323.743 \n",
      "Fold: 4 - val AUC: 0.5391   - loss: 324.180  - train AUC: 0.5515   - loss: 323.808 \n",
      "Fold: 5 - val AUC: 0.5482   - loss: 323.909  - train AUC: 0.5488   - loss: 323.831 \n",
      "Score:  - val AUC: 0.5436   - loss: 31068.694 - train AUC: 0.5518   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8353   - loss: 31068.694\n",
      "Cum CV train: 0.8606   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_95', 'var_95_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5313   - loss: 325.041  - train AUC: 0.5408   - loss: 324.857 \n",
      "Fold: 2 - val AUC: 0.5396   - loss: 325.267  - train AUC: 0.5402   - loss: 324.779 \n",
      "Fold: 3 - val AUC: 0.5373   - loss: 324.868  - train AUC: 0.5397   - loss: 324.894 \n",
      "Fold: 4 - val AUC: 0.5274   - loss: 325.313  - train AUC: 0.5410   - loss: 324.809 \n",
      "Fold: 5 - val AUC: 0.5363   - loss: 324.808  - train AUC: 0.5403   - loss: 324.901 \n",
      "Score:  - val AUC: 0.5338   - loss: 31068.694 - train AUC: 0.5420   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8368   - loss: 31068.694\n",
      "Cum CV train: 0.8620   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_96', 'var_96_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.4992   - loss: 567.664  - train AUC: 0.5027   - loss: 567.657 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.4997   - loss: 31068.694 - train AUC: 0.5026   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8368   - loss: 31068.694\n",
      "Cum CV train: 0.8621   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_97', 'var_97_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5163   - loss: 325.820  - train AUC: 0.5241   - loss: 325.573 \n",
      "Fold: 2 - val AUC: 0.5117   - loss: 325.756  - train AUC: 0.5213   - loss: 325.628 \n",
      "Fold: 3 - val AUC: 0.5171   - loss: 325.930  - train AUC: 0.5193   - loss: 325.594 \n",
      "Fold: 4 - val AUC: 0.5171   - loss: 325.762  - train AUC: 0.5214   - loss: 325.661 \n",
      "Fold: 5 - val AUC: 0.5134   - loss: 325.667  - train AUC: 0.5235   - loss: 325.621 \n",
      "Score:  - val AUC: 0.5143   - loss: 31068.694 - train AUC: 0.5243   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8374   - loss: 31068.694\n",
      "Cum CV train: 0.8628   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_98', 'var_98_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.4997   - loss: 655.993  - train AUC: 0.5051   - loss: 655.989 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.4999   - loss: 31068.694 - train AUC: 0.5048   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8374   - loss: 31068.694\n",
      "Cum CV train: 0.8628   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_99', 'var_99_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5542   - loss: 323.734  - train AUC: 0.5558   - loss: 323.288 \n",
      "Fold: 2 - val AUC: 0.5524   - loss: 323.452  - train AUC: 0.5563   - loss: 323.362 \n",
      "Fold: 3 - val AUC: 0.5594   - loss: 323.084  - train AUC: 0.5550   - loss: 323.424 \n",
      "Fold: 4 - val AUC: 0.5427   - loss: 324.219  - train AUC: 0.5583   - loss: 323.205 \n",
      "Fold: 5 - val AUC: 0.5530   - loss: 323.142  - train AUC: 0.5567   - loss: 323.394 \n",
      "Score:  - val AUC: 0.5516   - loss: 31068.694 - train AUC: 0.5576   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8409   - loss: 31068.694\n",
      "Cum CV train: 0.8659   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_100', 'var_100_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8409   - loss: 31068.694\n",
      "Cum CV train: 0.8659   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_101', 'var_101_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5141   - loss: 326.023  - train AUC: 0.5176   - loss: 325.861 \n",
      "Fold: 2 - val AUC: 0.5077   - loss: 326.044  - train AUC: 0.5205   - loss: 325.863 \n",
      "Fold: 3 - val AUC: 0.5099   - loss: 326.127  - train AUC: 0.5184   - loss: 325.853 \n",
      "Fold: 4 - val AUC: 0.5096   - loss: 325.986  - train AUC: 0.5203   - loss: 325.878 \n",
      "Fold: 5 - val AUC: 0.5056   - loss: 326.109  - train AUC: 0.5202   - loss: 325.841 \n",
      "Score:  - val AUC: 0.5081   - loss: 31068.694 - train AUC: 0.5214   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8409   - loss: 31068.694\n",
      "Cum CV train: 0.8661   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_102', 'var_102_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5212   - loss: 325.550  - train AUC: 0.5267   - loss: 325.367 \n",
      "Fold: 2 - val AUC: 0.5149   - loss: 325.759  - train AUC: 0.5278   - loss: 325.316 \n",
      "Fold: 3 - val AUC: 0.5172   - loss: 325.350  - train AUC: 0.5279   - loss: 325.376 \n",
      "Fold: 4 - val AUC: 0.5214   - loss: 325.690  - train AUC: 0.5240   - loss: 325.374 \n",
      "Fold: 5 - val AUC: 0.5227   - loss: 325.581  - train AUC: 0.5222   - loss: 325.456 \n",
      "Score:  - val AUC: 0.5189   - loss: 31068.694 - train AUC: 0.5275   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8417   - loss: 31068.694\n",
      "Cum CV train: 0.8670   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_103', 'var_103_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8417   - loss: 31068.694\n",
      "Cum CV train: 0.8670   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_104', 'var_104_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5268   - loss: 325.719  - train AUC: 0.5309   - loss: 325.634 \n",
      "Fold: 2 - val AUC: 0.5240   - loss: 325.775  - train AUC: 0.5304   - loss: 325.600 \n",
      "Fold: 3 - val AUC: 0.5287   - loss: 325.742  - train AUC: 0.5308   - loss: 325.567 \n",
      "Fold: 4 - val AUC: 0.5207   - loss: 325.859  - train AUC: 0.5325   - loss: 325.585 \n",
      "Fold: 5 - val AUC: 0.5260   - loss: 325.723  - train AUC: 0.5305   - loss: 325.596 \n",
      "Score:  - val AUC: 0.5247   - loss: 31068.694 - train AUC: 0.5323   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8422   - loss: 31068.694\n",
      "Cum CV train: 0.8676   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_105', 'var_105_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5174   - loss: 325.857  - train AUC: 0.5334   - loss: 325.420 \n",
      "Fold: 2 - val AUC: 0.5206   - loss: 325.933  - train AUC: 0.5304   - loss: 325.548 \n",
      "Fold: 3 - val AUC: 0.5252   - loss: 325.659  - train AUC: 0.5308   - loss: 325.561 \n",
      "Fold: 4 - val AUC: 0.5272   - loss: 325.624  - train AUC: 0.5302   - loss: 325.593 \n",
      "Fold: 5 - val AUC: 0.5197   - loss: 325.779  - train AUC: 0.5301   - loss: 325.615 \n",
      "Score:  - val AUC: 0.5213   - loss: 31068.694 - train AUC: 0.5329   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8427   - loss: 31068.694\n",
      "Cum CV train: 0.8682   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_106', 'var_106_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5329   - loss: 325.275  - train AUC: 0.5395   - loss: 324.974 \n",
      "Fold: 2 - val AUC: 0.5292   - loss: 325.443  - train AUC: 0.5364   - loss: 325.073 \n",
      "Fold: 3 - val AUC: 0.5332   - loss: 325.094  - train AUC: 0.5363   - loss: 325.130 \n",
      "Fold: 4 - val AUC: 0.5255   - loss: 325.268  - train AUC: 0.5365   - loss: 325.155 \n",
      "Fold: 5 - val AUC: 0.5201   - loss: 325.587  - train AUC: 0.5410   - loss: 324.973 \n",
      "Score:  - val AUC: 0.5277   - loss: 31068.694 - train AUC: 0.5408   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8438   - loss: 31068.694\n",
      "Cum CV train: 0.8694   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_107', 'var_107_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5377   - loss: 324.705  - train AUC: 0.5443   - loss: 324.379 \n",
      "Fold: 2 - val AUC: 0.5346   - loss: 324.830  - train AUC: 0.5430   - loss: 324.447 \n",
      "Fold: 3 - val AUC: 0.5433   - loss: 324.445  - train AUC: 0.5433   - loss: 324.444 \n",
      "Fold: 4 - val AUC: 0.5453   - loss: 324.075  - train AUC: 0.5460   - loss: 324.390 \n",
      "Fold: 5 - val AUC: 0.5341   - loss: 325.187  - train AUC: 0.5444   - loss: 324.260 \n",
      "Score:  - val AUC: 0.5385   - loss: 31068.694 - train AUC: 0.5455   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8456   - loss: 31068.694\n",
      "Cum CV train: 0.8712   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_108', 'var_108_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5433   - loss: 323.680  - train AUC: 0.5525   - loss: 323.815 \n",
      "Fold: 2 - val AUC: 0.5486   - loss: 323.804  - train AUC: 0.5524   - loss: 323.785 \n",
      "Fold: 3 - val AUC: 0.5432   - loss: 324.274  - train AUC: 0.5534   - loss: 323.651 \n",
      "Fold: 4 - val AUC: 0.5469   - loss: 324.309  - train AUC: 0.5537   - loss: 323.587 \n",
      "Fold: 5 - val AUC: 0.5499   - loss: 323.880  - train AUC: 0.5544   - loss: 323.636 \n",
      "Score:  - val AUC: 0.5456   - loss: 31068.694 - train AUC: 0.5552   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8483   - loss: 31068.694\n",
      "Cum CV train: 0.8736   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_109', 'var_109_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5453   - loss: 322.756  - train AUC: 0.5497   - loss: 322.882 \n",
      "Fold: 2 - val AUC: 0.5487   - loss: 322.710  - train AUC: 0.5499   - loss: 322.824 \n",
      "Fold: 3 - val AUC: 0.5457   - loss: 323.294  - train AUC: 0.5542   - loss: 322.616 \n",
      "Fold: 4 - val AUC: 0.5472   - loss: 322.678  - train AUC: 0.5547   - loss: 322.747 \n",
      "Fold: 5 - val AUC: 0.5427   - loss: 323.569  - train AUC: 0.5494   - loss: 322.666 \n",
      "Score:  - val AUC: 0.5451   - loss: 31068.694 - train AUC: 0.5540   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8516   - loss: 31068.694\n",
      "Cum CV train: 0.8765   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_110', 'var_110_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5502   - loss: 323.885  - train AUC: 0.5657   - loss: 322.119 \n",
      "Fold: 2 - val AUC: 0.5577   - loss: 322.070  - train AUC: 0.5653   - loss: 322.495 \n",
      "Fold: 3 - val AUC: 0.5597   - loss: 322.853  - train AUC: 0.5658   - loss: 322.248 \n",
      "Fold: 4 - val AUC: 0.5685   - loss: 321.697  - train AUC: 0.5619   - loss: 322.626 \n",
      "Fold: 5 - val AUC: 0.5537   - loss: 322.829  - train AUC: 0.5655   - loss: 322.327 \n",
      "Score:  - val AUC: 0.5573   - loss: 31068.694 - train AUC: 0.5662   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8553   - loss: 31068.694\n",
      "Cum CV train: 0.8797   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_111', 'var_111_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5164   - loss: 325.875  - train AUC: 0.5309   - loss: 325.329 \n",
      "Fold: 2 - val AUC: 0.5268   - loss: 325.543  - train AUC: 0.5292   - loss: 325.355 \n",
      "Fold: 3 - val AUC: 0.5276   - loss: 325.548  - train AUC: 0.5297   - loss: 325.326 \n",
      "Fold: 4 - val AUC: 0.5220   - loss: 325.467  - train AUC: 0.5302   - loss: 325.386 \n",
      "Fold: 5 - val AUC: 0.5283   - loss: 325.332  - train AUC: 0.5307   - loss: 325.373 \n",
      "Score:  - val AUC: 0.5232   - loss: 31068.694 - train AUC: 0.5319   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8560   - loss: 31068.694\n",
      "Cum CV train: 0.8804   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_112', 'var_112_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5310   - loss: 325.454  - train AUC: 0.5336   - loss: 325.387 \n",
      "Fold: 2 - val AUC: 0.5313   - loss: 325.454  - train AUC: 0.5346   - loss: 325.340 \n",
      "Fold: 3 - val AUC: 0.5275   - loss: 325.781  - train AUC: 0.5336   - loss: 325.328 \n",
      "Fold: 4 - val AUC: 0.5269   - loss: 325.559  - train AUC: 0.5331   - loss: 325.423 \n",
      "Fold: 5 - val AUC: 0.5260   - loss: 325.627  - train AUC: 0.5339   - loss: 325.389 \n",
      "Score:  - val AUC: 0.5279   - loss: 31068.694 - train AUC: 0.5349   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8567   - loss: 31068.694\n",
      "Cum CV train: 0.8813   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_113', 'var_113_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5083   - loss: 325.968  - train AUC: 0.5180   - loss: 325.751 \n",
      "Fold: 2 - val AUC: 0.5069   - loss: 325.961  - train AUC: 0.5189   - loss: 325.723 \n",
      "Fold: 3 - val AUC: 0.5082   - loss: 325.945  - train AUC: 0.5209   - loss: 325.730 \n",
      "Fold: 4 - val AUC: 0.5124   - loss: 325.770  - train AUC: 0.5247   - loss: 325.677 \n",
      "Fold: 5 - val AUC: 0.5131   - loss: 325.806  - train AUC: 0.5203   - loss: 325.726 \n",
      "Score:  - val AUC: 0.5093   - loss: 31068.694 - train AUC: 0.5237   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8571   - loss: 31068.694\n",
      "Cum CV train: 0.8817   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_114', 'var_114_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5277   - loss: 325.680  - train AUC: 0.5270   - loss: 325.429 \n",
      "Fold: 2 - val AUC: 0.5184   - loss: 325.831  - train AUC: 0.5305   - loss: 325.390 \n",
      "Fold: 3 - val AUC: 0.5182   - loss: 325.685  - train AUC: 0.5307   - loss: 325.406 \n",
      "Fold: 4 - val AUC: 0.5218   - loss: 325.480  - train AUC: 0.5301   - loss: 325.452 \n",
      "Fold: 5 - val AUC: 0.5248   - loss: 325.456  - train AUC: 0.5297   - loss: 325.436 \n",
      "Score:  - val AUC: 0.5213   - loss: 31068.694 - train AUC: 0.5316   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8578   - loss: 31068.694\n",
      "Cum CV train: 0.8825   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_115', 'var_115_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5383   - loss: 324.647  - train AUC: 0.5522   - loss: 324.187 \n",
      "Fold: 2 - val AUC: 0.5572   - loss: 324.222  - train AUC: 0.5498   - loss: 324.264 \n",
      "Fold: 3 - val AUC: 0.5454   - loss: 324.530  - train AUC: 0.5511   - loss: 324.240 \n",
      "Fold: 4 - val AUC: 0.5409   - loss: 324.763  - train AUC: 0.5511   - loss: 324.177 \n",
      "Fold: 5 - val AUC: 0.5455   - loss: 324.396  - train AUC: 0.5505   - loss: 324.301 \n",
      "Score:  - val AUC: 0.5439   - loss: 31068.694 - train AUC: 0.5536   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8595   - loss: 31068.694\n",
      "Cum CV train: 0.8841   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_116', 'var_116_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5255   - loss: 325.657  - train AUC: 0.5296   - loss: 325.576 \n",
      "Fold: 2 - val AUC: 0.5158   - loss: 325.856  - train AUC: 0.5288   - loss: 325.618 \n",
      "Fold: 3 - val AUC: 0.5231   - loss: 325.801  - train AUC: 0.5273   - loss: 325.584 \n",
      "Fold: 4 - val AUC: 0.5243   - loss: 325.706  - train AUC: 0.5298   - loss: 325.585 \n",
      "Fold: 5 - val AUC: 0.5248   - loss: 325.772  - train AUC: 0.5295   - loss: 325.587 \n",
      "Score:  - val AUC: 0.5221   - loss: 31068.694 - train AUC: 0.5309   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8598   - loss: 31068.694\n",
      "Cum CV train: 0.8845   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_117', 'var_117_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5176   - loss: 326.085  - train AUC: 0.5266   - loss: 325.782 \n",
      "Fold: 4 - val AUC: 0.5133   - loss: 326.000  - train AUC: 0.5231   - loss: 325.863 \n",
      "Fold: 5 - val AUC: 0.5027   - loss: 326.166  - train AUC: 0.5233   - loss: 325.902 \n",
      "Score:  - val AUC: 0.5066   - loss: 31068.694 - train AUC: 0.5266   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8599   - loss: 31068.694\n",
      "Cum CV train: 0.8848   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_118', 'var_118_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5434   - loss: 324.779  - train AUC: 0.5463   - loss: 324.299 \n",
      "Fold: 2 - val AUC: 0.5405   - loss: 324.620  - train AUC: 0.5481   - loss: 324.300 \n",
      "Fold: 3 - val AUC: 0.5404   - loss: 324.874  - train AUC: 0.5480   - loss: 324.293 \n",
      "Fold: 4 - val AUC: 0.5436   - loss: 324.588  - train AUC: 0.5477   - loss: 324.313 \n",
      "Fold: 5 - val AUC: 0.5378   - loss: 324.688  - train AUC: 0.5463   - loss: 324.443 \n",
      "Score:  - val AUC: 0.5408   - loss: 31068.694 - train AUC: 0.5486   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8614   - loss: 31068.694\n",
      "Cum CV train: 0.8863   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_119', 'var_119_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5168   - loss: 325.060  - train AUC: 0.5356   - loss: 324.750 \n",
      "Fold: 2 - val AUC: 0.5288   - loss: 325.269  - train AUC: 0.5335   - loss: 324.677 \n",
      "Fold: 3 - val AUC: 0.5346   - loss: 324.647  - train AUC: 0.5340   - loss: 324.766 \n",
      "Fold: 4 - val AUC: 0.5230   - loss: 325.061  - train AUC: 0.5373   - loss: 324.658 \n",
      "Fold: 5 - val AUC: 0.5265   - loss: 324.904  - train AUC: 0.5391   - loss: 324.598 \n",
      "Score:  - val AUC: 0.5254   - loss: 31068.694 - train AUC: 0.5376   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8627   - loss: 31068.694\n",
      "Cum CV train: 0.8876   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_120', 'var_120_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5147   - loss: 325.985  - train AUC: 0.5246   - loss: 325.707 \n",
      "Fold: 2 - val AUC: 0.5144   - loss: 325.903  - train AUC: 0.5252   - loss: 325.714 \n",
      "Fold: 3 - val AUC: 0.5090   - loss: 326.104  - train AUC: 0.5246   - loss: 325.728 \n",
      "Fold: 4 - val AUC: 0.5074   - loss: 326.085  - train AUC: 0.5250   - loss: 325.747 \n",
      "Fold: 5 - val AUC: 0.5137   - loss: 325.980  - train AUC: 0.5210   - loss: 325.805 \n",
      "Score:  - val AUC: 0.5121   - loss: 31068.694 - train AUC: 0.5272   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8628   - loss: 31068.694\n",
      "Cum CV train: 0.8880   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_121', 'var_121_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5324   - loss: 324.853  - train AUC: 0.5442   - loss: 324.432 \n",
      "Fold: 2 - val AUC: 0.5410   - loss: 324.643  - train AUC: 0.5410   - loss: 324.548 \n",
      "Fold: 3 - val AUC: 0.5306   - loss: 324.951  - train AUC: 0.5420   - loss: 324.512 \n",
      "Fold: 4 - val AUC: 0.5285   - loss: 325.048  - train AUC: 0.5422   - loss: 324.431 \n",
      "Fold: 5 - val AUC: 0.5435   - loss: 324.340  - train AUC: 0.5444   - loss: 324.497 \n",
      "Score:  - val AUC: 0.5350   - loss: 31068.694 - train AUC: 0.5453   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8646   - loss: 31068.694\n",
      "Cum CV train: 0.8896   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_122', 'var_122_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5378   - loss: 324.386  - train AUC: 0.5440   - loss: 324.410 \n",
      "Fold: 2 - val AUC: 0.5436   - loss: 324.674  - train AUC: 0.5427   - loss: 324.369 \n",
      "Fold: 3 - val AUC: 0.5377   - loss: 324.781  - train AUC: 0.5442   - loss: 324.328 \n",
      "Fold: 4 - val AUC: 0.5413   - loss: 324.387  - train AUC: 0.5426   - loss: 324.447 \n",
      "Fold: 5 - val AUC: 0.5383   - loss: 324.721  - train AUC: 0.5430   - loss: 324.335 \n",
      "Score:  - val AUC: 0.5390   - loss: 31068.694 - train AUC: 0.5445   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8664   - loss: 31068.694\n",
      "Cum CV train: 0.8912   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_123', 'var_123_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5439   - loss: 324.229  - train AUC: 0.5478   - loss: 323.847 \n",
      "Fold: 2 - val AUC: 0.5391   - loss: 323.906  - train AUC: 0.5497   - loss: 323.935 \n",
      "Fold: 3 - val AUC: 0.5482   - loss: 323.747  - train AUC: 0.5495   - loss: 323.832 \n",
      "Fold: 4 - val AUC: 0.5442   - loss: 324.322  - train AUC: 0.5469   - loss: 323.859 \n",
      "Fold: 5 - val AUC: 0.5414   - loss: 324.572  - train AUC: 0.5488   - loss: 323.759 \n",
      "Score:  - val AUC: 0.5429   - loss: 31068.694 - train AUC: 0.5495   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8682   - loss: 31068.694\n",
      "Cum CV train: 0.8928   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_124', 'var_124_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.4937   - loss: 326.308  - train AUC: 0.5169   - loss: 326.006 \n",
      "Fold: 2 - val AUC: 0.5041   - loss: 567.660  - train AUC: 0.5051   - loss: 567.657 \n",
      "Fold: 3 - val AUC: 0.5018   - loss: 655.992  - train AUC: 0.5055   - loss: 655.989 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.4997   - loss: 31068.694 - train AUC: 0.5142   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8682   - loss: 31068.694\n",
      "Cum CV train: 0.8928   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_125', 'var_125_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5177   - loss: 325.853  - train AUC: 0.5286   - loss: 325.400 \n",
      "Fold: 2 - val AUC: 0.5210   - loss: 325.819  - train AUC: 0.5297   - loss: 325.378 \n",
      "Fold: 3 - val AUC: 0.5252   - loss: 325.412  - train AUC: 0.5282   - loss: 325.467 \n",
      "Fold: 4 - val AUC: 0.5240   - loss: 325.438  - train AUC: 0.5303   - loss: 325.448 \n",
      "Fold: 5 - val AUC: 0.5177   - loss: 325.545  - train AUC: 0.5318   - loss: 325.414 \n",
      "Score:  - val AUC: 0.5208   - loss: 31068.694 - train AUC: 0.5318   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8687   - loss: 31068.694\n",
      "Cum CV train: 0.8935   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_126', 'var_126_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8687   - loss: 31068.694\n",
      "Cum CV train: 0.8935   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_127', 'var_127_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5328   - loss: 324.968  - train AUC: 0.5460   - loss: 324.489 \n",
      "Fold: 2 - val AUC: 0.5373   - loss: 324.782  - train AUC: 0.5440   - loss: 324.515 \n",
      "Fold: 3 - val AUC: 0.5421   - loss: 324.714  - train AUC: 0.5432   - loss: 324.506 \n",
      "Fold: 4 - val AUC: 0.5434   - loss: 324.481  - train AUC: 0.5466   - loss: 324.456 \n",
      "Fold: 5 - val AUC: 0.5372   - loss: 324.894  - train AUC: 0.5468   - loss: 324.447 \n",
      "Score:  - val AUC: 0.5381   - loss: 31068.694 - train AUC: 0.5468   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8703   - loss: 31068.694\n",
      "Cum CV train: 0.8949   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_128', 'var_128_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5191   - loss: 325.618  - train AUC: 0.5298   - loss: 325.391 \n",
      "Fold: 2 - val AUC: 0.5187   - loss: 325.870  - train AUC: 0.5303   - loss: 325.280 \n",
      "Fold: 3 - val AUC: 0.5275   - loss: 325.434  - train AUC: 0.5302   - loss: 325.361 \n",
      "Fold: 4 - val AUC: 0.5274   - loss: 325.510  - train AUC: 0.5282   - loss: 325.397 \n",
      "Fold: 5 - val AUC: 0.5279   - loss: 325.578  - train AUC: 0.5283   - loss: 325.359 \n",
      "Score:  - val AUC: 0.5232   - loss: 31068.694 - train AUC: 0.5302   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8710   - loss: 31068.694\n",
      "Cum CV train: 0.8957   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_129', 'var_129_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5087   - loss: 325.945  - train AUC: 0.5252   - loss: 325.757 \n",
      "Fold: 2 - val AUC: 0.5038   - loss: 326.047  - train AUC: 0.5157   - loss: 325.920 \n",
      "Fold: 3 - val AUC: 0.4988   - loss: 326.102  - train AUC: 0.5206   - loss: 325.861 \n",
      "Fold: 4 - val AUC: 0.5044   - loss: 326.127  - train AUC: 0.5187   - loss: 325.902 \n",
      "Fold: 5 - val AUC: 0.4973   - loss: 326.211  - train AUC: 0.5162   - loss: 325.893 \n",
      "Score:  - val AUC: 0.5027   - loss: 31068.694 - train AUC: 0.5251   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8711   - loss: 31068.694\n",
      "Cum CV train: 0.8960   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_130', 'var_130_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5291   - loss: 325.242  - train AUC: 0.5317   - loss: 325.083 \n",
      "Fold: 2 - val AUC: 0.5263   - loss: 325.303  - train AUC: 0.5330   - loss: 325.034 \n",
      "Fold: 3 - val AUC: 0.5186   - loss: 325.561  - train AUC: 0.5323   - loss: 325.025 \n",
      "Fold: 4 - val AUC: 0.5286   - loss: 325.102  - train AUC: 0.5345   - loss: 324.999 \n",
      "Fold: 5 - val AUC: 0.5265   - loss: 325.217  - train AUC: 0.5311   - loss: 325.114 \n",
      "Score:  - val AUC: 0.5249   - loss: 31068.694 - train AUC: 0.5339   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8721   - loss: 31068.694\n",
      "Cum CV train: 0.8970   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_131', 'var_131_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5286   - loss: 325.408  - train AUC: 0.5361   - loss: 325.256 \n",
      "Fold: 2 - val AUC: 0.5316   - loss: 325.422  - train AUC: 0.5340   - loss: 325.273 \n",
      "Fold: 3 - val AUC: 0.5313   - loss: 325.680  - train AUC: 0.5349   - loss: 325.272 \n",
      "Fold: 4 - val AUC: 0.5287   - loss: 325.291  - train AUC: 0.5363   - loss: 325.222 \n",
      "Fold: 5 - val AUC: 0.5235   - loss: 325.552  - train AUC: 0.5366   - loss: 325.265 \n",
      "Score:  - val AUC: 0.5275   - loss: 31068.694 - train AUC: 0.5376   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8728   - loss: 31068.694\n",
      "Cum CV train: 0.8977   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_132', 'var_132_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5161   - loss: 325.653  - train AUC: 0.5279   - loss: 325.462 \n",
      "Fold: 2 - val AUC: 0.5212   - loss: 325.715  - train AUC: 0.5284   - loss: 325.412 \n",
      "Fold: 3 - val AUC: 0.5212   - loss: 325.647  - train AUC: 0.5295   - loss: 325.378 \n",
      "Fold: 4 - val AUC: 0.5317   - loss: 325.229  - train AUC: 0.5275   - loss: 325.489 \n",
      "Fold: 5 - val AUC: 0.5148   - loss: 325.819  - train AUC: 0.5301   - loss: 325.350 \n",
      "Score:  - val AUC: 0.5200   - loss: 31068.694 - train AUC: 0.5302   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8734   - loss: 31068.694\n",
      "Cum CV train: 0.8983   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_133', 'var_133_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5444   - loss: 324.224  - train AUC: 0.5506   - loss: 323.842 \n",
      "Fold: 2 - val AUC: 0.5427   - loss: 324.289  - train AUC: 0.5526   - loss: 323.780 \n",
      "Fold: 3 - val AUC: 0.5434   - loss: 324.617  - train AUC: 0.5508   - loss: 323.764 \n",
      "Fold: 4 - val AUC: 0.5499   - loss: 323.857  - train AUC: 0.5510   - loss: 323.882 \n",
      "Fold: 5 - val AUC: 0.5506   - loss: 323.416  - train AUC: 0.5508   - loss: 323.970 \n",
      "Score:  - val AUC: 0.5457   - loss: 31068.694 - train AUC: 0.5532   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8754   - loss: 31068.694\n",
      "Cum CV train: 0.9000   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_134', 'var_134_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5110   - loss: 325.707  - train AUC: 0.5248   - loss: 325.438 \n",
      "Fold: 2 - val AUC: 0.5167   - loss: 325.567  - train AUC: 0.5223   - loss: 325.486 \n",
      "Fold: 3 - val AUC: 0.5092   - loss: 325.952  - train AUC: 0.5255   - loss: 325.411 \n",
      "Fold: 4 - val AUC: 0.5211   - loss: 325.482  - train AUC: 0.5256   - loss: 325.445 \n",
      "Fold: 5 - val AUC: 0.5250   - loss: 325.487  - train AUC: 0.5244   - loss: 325.500 \n",
      "Score:  - val AUC: 0.5152   - loss: 31068.694 - train AUC: 0.5271   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8757   - loss: 31068.694\n",
      "Cum CV train: 0.9004   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_135', 'var_135_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5278   - loss: 325.032  - train AUC: 0.5323   - loss: 324.877 \n",
      "Fold: 2 - val AUC: 0.5194   - loss: 325.479  - train AUC: 0.5318   - loss: 324.840 \n",
      "Fold: 3 - val AUC: 0.5279   - loss: 324.983  - train AUC: 0.5303   - loss: 324.888 \n",
      "Fold: 4 - val AUC: 0.5250   - loss: 324.855  - train AUC: 0.5332   - loss: 324.910 \n",
      "Fold: 5 - val AUC: 0.5214   - loss: 325.062  - train AUC: 0.5313   - loss: 324.932 \n",
      "Score:  - val AUC: 0.5237   - loss: 31068.694 - train AUC: 0.5345   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8766   - loss: 31068.694\n",
      "Cum CV train: 0.9013   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_136', 'var_136_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.4987   - loss: 544.264  - train AUC: 0.5028   - loss: 544.249 \n",
      "Fold: 3 - val AUC: 0.4998   - loss: 655.991  - train AUC: 0.5054   - loss: 655.989 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.4995   - loss: 31068.694 - train AUC: 0.5051   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8766   - loss: 31068.694\n",
      "Cum CV train: 0.9013   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_137', 'var_137_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5257   - loss: 325.475  - train AUC: 0.5313   - loss: 325.233 \n",
      "Fold: 2 - val AUC: 0.5275   - loss: 325.386  - train AUC: 0.5310   - loss: 325.243 \n",
      "Fold: 3 - val AUC: 0.5292   - loss: 325.522  - train AUC: 0.5313   - loss: 325.216 \n",
      "Fold: 4 - val AUC: 0.5265   - loss: 325.291  - train AUC: 0.5327   - loss: 325.245 \n",
      "Fold: 5 - val AUC: 0.5195   - loss: 325.306  - train AUC: 0.5303   - loss: 325.269 \n",
      "Score:  - val AUC: 0.5252   - loss: 31068.694 - train AUC: 0.5333   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8774   - loss: 31068.694\n",
      "Cum CV train: 0.9020   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_138', 'var_138_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5102   - loss: 326.031  - train AUC: 0.5251   - loss: 325.622 \n",
      "Fold: 2 - val AUC: 0.5216   - loss: 325.812  - train AUC: 0.5243   - loss: 325.643 \n",
      "Fold: 3 - val AUC: 0.5205   - loss: 325.829  - train AUC: 0.5249   - loss: 325.639 \n",
      "Fold: 4 - val AUC: 0.5211   - loss: 325.727  - train AUC: 0.5249   - loss: 325.681 \n",
      "Fold: 5 - val AUC: 0.5194   - loss: 325.846  - train AUC: 0.5257   - loss: 325.628 \n",
      "Score:  - val AUC: 0.5172   - loss: 31068.694 - train AUC: 0.5273   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8777   - loss: 31068.694\n",
      "Cum CV train: 0.9024   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_139', 'var_139_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5745   - loss: 321.605  - train AUC: 0.5786   - loss: 320.906 \n",
      "Fold: 2 - val AUC: 0.5717   - loss: 321.748  - train AUC: 0.5805   - loss: 320.896 \n",
      "Fold: 3 - val AUC: 0.5779   - loss: 321.216  - train AUC: 0.5796   - loss: 320.956 \n",
      "Fold: 4 - val AUC: 0.5744   - loss: 321.014  - train AUC: 0.5794   - loss: 321.070 \n",
      "Fold: 5 - val AUC: 0.5780   - loss: 320.793  - train AUC: 0.5799   - loss: 321.031 \n",
      "Score:  - val AUC: 0.5748   - loss: 31068.694 - train AUC: 0.5805   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8820   - loss: 31068.694\n",
      "Cum CV train: 0.9058   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_140', 'var_140_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5140   - loss: 326.013  - train AUC: 0.5200   - loss: 325.920 \n",
      "Fold: 2 - val AUC: 0.5146   - loss: 326.075  - train AUC: 0.5201   - loss: 325.911 \n",
      "Fold: 3 - val AUC: 0.5077   - loss: 326.165  - train AUC: 0.5198   - loss: 325.902 \n",
      "Fold: 4 - val AUC: 0.5146   - loss: 325.955  - train AUC: 0.5189   - loss: 325.950 \n",
      "Fold: 5 - val AUC: 0.5067   - loss: 326.141  - train AUC: 0.5198   - loss: 325.909 \n",
      "Score:  - val AUC: 0.5114   - loss: 31068.694 - train AUC: 0.5212   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8820   - loss: 31068.694\n",
      "Cum CV train: 0.9060   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_141', 'var_141_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5261   - loss: 325.059  - train AUC: 0.5340   - loss: 324.764 \n",
      "Fold: 2 - val AUC: 0.5279   - loss: 325.361  - train AUC: 0.5359   - loss: 324.771 \n",
      "Fold: 3 - val AUC: 0.5281   - loss: 324.985  - train AUC: 0.5363   - loss: 324.808 \n",
      "Fold: 4 - val AUC: 0.5331   - loss: 324.776  - train AUC: 0.5362   - loss: 324.770 \n",
      "Fold: 5 - val AUC: 0.5283   - loss: 324.978  - train AUC: 0.5391   - loss: 324.693 \n",
      "Score:  - val AUC: 0.5280   - loss: 31068.694 - train AUC: 0.5388   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8830   - loss: 31068.694\n",
      "Cum CV train: 0.9070   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_142', 'var_142_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5223   - loss: 325.687  - train AUC: 0.5301   - loss: 325.454 \n",
      "Fold: 2 - val AUC: 0.5221   - loss: 325.837  - train AUC: 0.5314   - loss: 325.340 \n",
      "Fold: 3 - val AUC: 0.5106   - loss: 326.014  - train AUC: 0.5313   - loss: 325.445 \n",
      "Fold: 4 - val AUC: 0.5158   - loss: 325.826  - train AUC: 0.5299   - loss: 325.503 \n",
      "Fold: 5 - val AUC: 0.5211   - loss: 325.699  - train AUC: 0.5321   - loss: 325.371 \n",
      "Score:  - val AUC: 0.5178   - loss: 31068.694 - train AUC: 0.5340   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8834   - loss: 31068.694\n",
      "Cum CV train: 0.9075   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_143', 'var_143_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5098   - loss: 325.966  - train AUC: 0.5196   - loss: 325.882 \n",
      "Fold: 2 - val AUC: 0.4999   - loss: 326.225  - train AUC: 0.5173   - loss: 325.876 \n",
      "Fold: 3 - val AUC: 0.5002   - loss: 326.131  - train AUC: 0.5166   - loss: 325.886 \n",
      "Fold: 4 - val AUC: 0.5111   - loss: 325.946  - train AUC: 0.5159   - loss: 325.939 \n",
      "Fold: 5 - val AUC: 0.5036   - loss: 326.070  - train AUC: 0.5180   - loss: 325.869 \n",
      "Score:  - val AUC: 0.5047   - loss: 31068.694 - train AUC: 0.5201   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8834   - loss: 31068.694\n",
      "Cum CV train: 0.9077   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_144', 'var_144_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5109   - loss: 326.161  - train AUC: 0.5316   - loss: 325.398 \n",
      "Fold: 2 - val AUC: 0.5291   - loss: 325.481  - train AUC: 0.5280   - loss: 325.539 \n",
      "Fold: 3 - val AUC: 0.5134   - loss: 326.044  - train AUC: 0.5311   - loss: 325.490 \n",
      "Fold: 4 - val AUC: 0.5211   - loss: 325.744  - train AUC: 0.5291   - loss: 325.573 \n",
      "Fold: 5 - val AUC: 0.5217   - loss: 325.617  - train AUC: 0.5308   - loss: 325.548 \n",
      "Score:  - val AUC: 0.5186   - loss: 31068.694 - train AUC: 0.5328   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8838   - loss: 31068.694\n",
      "Cum CV train: 0.9083   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_145', 'var_145_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5318   - loss: 324.909  - train AUC: 0.5320   - loss: 325.062 \n",
      "Fold: 2 - val AUC: 0.5266   - loss: 325.285  - train AUC: 0.5333   - loss: 324.925 \n",
      "Fold: 3 - val AUC: 0.5255   - loss: 325.190  - train AUC: 0.5319   - loss: 325.028 \n",
      "Fold: 4 - val AUC: 0.5280   - loss: 325.120  - train AUC: 0.5317   - loss: 325.041 \n",
      "Fold: 5 - val AUC: 0.5266   - loss: 325.188  - train AUC: 0.5314   - loss: 325.043 \n",
      "Score:  - val AUC: 0.5274   - loss: 31068.694 - train AUC: 0.5328   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8846   - loss: 31068.694\n",
      "Cum CV train: 0.9090   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_146', 'var_146_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5581   - loss: 323.092  - train AUC: 0.5646   - loss: 322.539 \n",
      "Fold: 2 - val AUC: 0.5571   - loss: 322.726  - train AUC: 0.5660   - loss: 322.551 \n",
      "Fold: 3 - val AUC: 0.5532   - loss: 323.039  - train AUC: 0.5664   - loss: 322.493 \n",
      "Fold: 4 - val AUC: 0.5544   - loss: 323.112  - train AUC: 0.5649   - loss: 322.489 \n",
      "Fold: 5 - val AUC: 0.5622   - loss: 323.011  - train AUC: 0.5630   - loss: 322.557 \n",
      "Score:  - val AUC: 0.5563   - loss: 31068.694 - train AUC: 0.5658   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8872   - loss: 31068.694\n",
      "Cum CV train: 0.9113   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_147', 'var_147_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5445   - loss: 323.424  - train AUC: 0.5491   - loss: 323.793 \n",
      "Fold: 2 - val AUC: 0.5353   - loss: 324.682  - train AUC: 0.5486   - loss: 323.596 \n",
      "Fold: 3 - val AUC: 0.5464   - loss: 323.709  - train AUC: 0.5462   - loss: 323.832 \n",
      "Fold: 4 - val AUC: 0.5359   - loss: 324.758  - train AUC: 0.5482   - loss: 323.566 \n",
      "Fold: 5 - val AUC: 0.5466   - loss: 323.585  - train AUC: 0.5471   - loss: 323.847 \n",
      "Score:  - val AUC: 0.5411   - loss: 31068.694 - train AUC: 0.5492   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8888   - loss: 31068.694\n",
      "Cum CV train: 0.9127   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_148', 'var_148_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5389   - loss: 324.514  - train AUC: 0.5583   - loss: 323.753 \n",
      "Fold: 2 - val AUC: 0.5537   - loss: 324.031  - train AUC: 0.5602   - loss: 323.683 \n",
      "Fold: 3 - val AUC: 0.5534   - loss: 323.811  - train AUC: 0.5576   - loss: 323.834 \n",
      "Fold: 4 - val AUC: 0.5548   - loss: 323.998  - train AUC: 0.5557   - loss: 323.857 \n",
      "Fold: 5 - val AUC: 0.5506   - loss: 323.872  - train AUC: 0.5575   - loss: 323.849 \n",
      "Score:  - val AUC: 0.5497   - loss: 31068.694 - train AUC: 0.5604   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8903   - loss: 31068.694\n",
      "Cum CV train: 0.9140   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_149', 'var_149_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5448   - loss: 324.824  - train AUC: 0.5562   - loss: 324.176 \n",
      "Fold: 2 - val AUC: 0.5553   - loss: 324.289  - train AUC: 0.5531   - loss: 324.327 \n",
      "Fold: 3 - val AUC: 0.5532   - loss: 324.417  - train AUC: 0.5545   - loss: 324.288 \n",
      "Fold: 4 - val AUC: 0.5505   - loss: 324.440  - train AUC: 0.5544   - loss: 324.289 \n",
      "Fold: 5 - val AUC: 0.5480   - loss: 324.738  - train AUC: 0.5534   - loss: 324.301 \n",
      "Score:  - val AUC: 0.5494   - loss: 31068.694 - train AUC: 0.5554   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8917   - loss: 31068.694\n",
      "Cum CV train: 0.9152   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_150', 'var_150_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5127   - loss: 325.880  - train AUC: 0.5364   - loss: 325.185 \n",
      "Fold: 2 - val AUC: 0.5282   - loss: 325.315  - train AUC: 0.5335   - loss: 325.309 \n",
      "Fold: 3 - val AUC: 0.5274   - loss: 325.568  - train AUC: 0.5310   - loss: 325.333 \n",
      "Fold: 4 - val AUC: 0.5309   - loss: 325.388  - train AUC: 0.5315   - loss: 325.323 \n",
      "Fold: 5 - val AUC: 0.5239   - loss: 325.737  - train AUC: 0.5331   - loss: 325.273 \n",
      "Score:  - val AUC: 0.5227   - loss: 31068.694 - train AUC: 0.5350   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8922   - loss: 31068.694\n",
      "Cum CV train: 0.9158   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_151', 'var_151_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5318   - loss: 325.112  - train AUC: 0.5299   - loss: 325.234 \n",
      "Fold: 2 - val AUC: 0.5300   - loss: 325.351  - train AUC: 0.5317   - loss: 325.100 \n",
      "Fold: 3 - val AUC: 0.5217   - loss: 325.639  - train AUC: 0.5308   - loss: 325.153 \n",
      "Fold: 4 - val AUC: 0.5175   - loss: 325.559  - train AUC: 0.5310   - loss: 325.191 \n",
      "Fold: 5 - val AUC: 0.5270   - loss: 325.285  - train AUC: 0.5312   - loss: 325.173 \n",
      "Score:  - val AUC: 0.5249   - loss: 31068.694 - train AUC: 0.5329   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8927   - loss: 31068.694\n",
      "Cum CV train: 0.9163   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_152', 'var_152_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5076   - loss: 326.080  - train AUC: 0.5205   - loss: 325.793 \n",
      "Fold: 2 - val AUC: 0.5076   - loss: 325.938  - train AUC: 0.5174   - loss: 325.881 \n",
      "Fold: 3 - val AUC: 0.5065   - loss: 326.108  - train AUC: 0.5190   - loss: 325.826 \n",
      "Fold: 4 - val AUC: 0.5042   - loss: 325.980  - train AUC: 0.5187   - loss: 325.866 \n",
      "Fold: 5 - val AUC: 0.5135   - loss: 325.949  - train AUC: 0.5197   - loss: 325.865 \n",
      "Score:  - val AUC: 0.5074   - loss: 31068.694 - train AUC: 0.5216   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8928   - loss: 31068.694\n",
      "Cum CV train: 0.9165   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_153', 'var_153_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5004   - loss: 621.129  - train AUC: 0.5050   - loss: 621.123 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5002   - loss: 31068.694 - train AUC: 0.5048   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8928   - loss: 31068.694\n",
      "Cum CV train: 0.9165   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_154', 'var_154_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5427   - loss: 323.890  - train AUC: 0.5496   - loss: 323.813 \n",
      "Fold: 2 - val AUC: 0.5472   - loss: 323.851  - train AUC: 0.5491   - loss: 323.805 \n",
      "Fold: 3 - val AUC: 0.5464   - loss: 323.953  - train AUC: 0.5487   - loss: 323.858 \n",
      "Fold: 4 - val AUC: 0.5378   - loss: 324.365  - train AUC: 0.5504   - loss: 323.713 \n",
      "Fold: 5 - val AUC: 0.5462   - loss: 324.402  - train AUC: 0.5487   - loss: 323.768 \n",
      "Score:  - val AUC: 0.5430   - loss: 31068.694 - train AUC: 0.5510   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8941   - loss: 31068.694\n",
      "Cum CV train: 0.9176   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_155', 'var_155_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5352   - loss: 324.860  - train AUC: 0.5378   - loss: 324.345 \n",
      "Fold: 2 - val AUC: 0.5274   - loss: 324.663  - train AUC: 0.5421   - loss: 324.315 \n",
      "Fold: 3 - val AUC: 0.5274   - loss: 325.100  - train AUC: 0.5388   - loss: 324.324 \n",
      "Fold: 4 - val AUC: 0.5330   - loss: 324.390  - train AUC: 0.5396   - loss: 324.410 \n",
      "Fold: 5 - val AUC: 0.5367   - loss: 324.381  - train AUC: 0.5403   - loss: 324.373 \n",
      "Score:  - val AUC: 0.5312   - loss: 31068.694 - train AUC: 0.5413   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8952   - loss: 31068.694\n",
      "Cum CV train: 0.9186   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_156', 'var_156_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5124   - loss: 326.050  - train AUC: 0.5260   - loss: 325.632 \n",
      "Fold: 2 - val AUC: 0.5160   - loss: 326.040  - train AUC: 0.5243   - loss: 325.661 \n",
      "Fold: 3 - val AUC: 0.5202   - loss: 325.738  - train AUC: 0.5260   - loss: 325.677 \n",
      "Fold: 4 - val AUC: 0.5149   - loss: 325.842  - train AUC: 0.5281   - loss: 325.606 \n",
      "Fold: 5 - val AUC: 0.5189   - loss: 325.707  - train AUC: 0.5241   - loss: 325.701 \n",
      "Score:  - val AUC: 0.5155   - loss: 31068.694 - train AUC: 0.5274   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8954   - loss: 31068.694\n",
      "Cum CV train: 0.9190   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_157', 'var_157_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5232   - loss: 325.414  - train AUC: 0.5370   - loss: 324.783 \n",
      "Fold: 2 - val AUC: 0.5271   - loss: 325.139  - train AUC: 0.5350   - loss: 324.868 \n",
      "Fold: 3 - val AUC: 0.5301   - loss: 325.046  - train AUC: 0.5341   - loss: 324.885 \n",
      "Fold: 4 - val AUC: 0.5359   - loss: 324.601  - train AUC: 0.5346   - loss: 324.985 \n",
      "Fold: 5 - val AUC: 0.5298   - loss: 325.095  - train AUC: 0.5346   - loss: 324.885 \n",
      "Score:  - val AUC: 0.5283   - loss: 31068.694 - train AUC: 0.5355   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8962   - loss: 31068.694\n",
      "Cum CV train: 0.9197   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_158', 'var_158_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.4950   - loss: 326.253  - train AUC: 0.5158   - loss: 326.019 \n",
      "Score:  - val AUC: 0.4998   - loss: 31068.694 - train AUC: 0.5131   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8962   - loss: 31068.694\n",
      "Cum CV train: 0.9197   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_159', 'var_159_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5144   - loss: 326.001  - train AUC: 0.5201   - loss: 325.923 \n",
      "Fold: 2 - val AUC: 0.5076   - loss: 326.216  - train AUC: 0.5209   - loss: 325.879 \n",
      "Fold: 3 - val AUC: 0.5193   - loss: 326.031  - train AUC: 0.5183   - loss: 325.917 \n",
      "Fold: 4 - val AUC: 0.5056   - loss: 326.173  - train AUC: 0.5217   - loss: 325.888 \n",
      "Fold: 5 - val AUC: 0.5156   - loss: 325.991  - train AUC: 0.5207   - loss: 325.897 \n",
      "Score:  - val AUC: 0.5118   - loss: 31068.694 - train AUC: 0.5220   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8962   - loss: 31068.694\n",
      "Cum CV train: 0.9199   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_160', 'var_160_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5056   - loss: 326.083  - train AUC: 0.5214   - loss: 325.854 \n",
      "Fold: 2 - val AUC: 0.4985   - loss: 326.202  - train AUC: 0.5203   - loss: 325.852 \n",
      "Fold: 3 - val AUC: 0.5038   - loss: 326.240  - train AUC: 0.5217   - loss: 325.761 \n",
      "Fold: 4 - val AUC: 0.5053   - loss: 326.084  - train AUC: 0.5250   - loss: 325.785 \n",
      "Fold: 5 - val AUC: 0.5031   - loss: 326.005  - train AUC: 0.5229   - loss: 325.821 \n",
      "Score:  - val AUC: 0.5022   - loss: 31068.694 - train AUC: 0.5316   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8963   - loss: 31068.694\n",
      "Cum CV train: 0.9201   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_161', 'var_161_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8963   - loss: 31068.694\n",
      "Cum CV train: 0.9201   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_162', 'var_162_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5364   - loss: 325.133  - train AUC: 0.5328   - loss: 325.107 \n",
      "Fold: 2 - val AUC: 0.5290   - loss: 325.188  - train AUC: 0.5376   - loss: 324.983 \n",
      "Fold: 3 - val AUC: 0.5321   - loss: 325.200  - train AUC: 0.5351   - loss: 325.054 \n",
      "Fold: 4 - val AUC: 0.5250   - loss: 325.415  - train AUC: 0.5394   - loss: 324.883 \n",
      "Fold: 5 - val AUC: 0.5269   - loss: 325.376  - train AUC: 0.5385   - loss: 324.888 \n",
      "Score:  - val AUC: 0.5289   - loss: 31068.694 - train AUC: 0.5389   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8968   - loss: 31068.694\n",
      "Cum CV train: 0.9207   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_163', 'var_163_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5345   - loss: 325.176  - train AUC: 0.5384   - loss: 324.584 \n",
      "Fold: 2 - val AUC: 0.5347   - loss: 324.637  - train AUC: 0.5401   - loss: 324.653 \n",
      "Fold: 3 - val AUC: 0.5335   - loss: 324.565  - train AUC: 0.5411   - loss: 324.657 \n",
      "Fold: 4 - val AUC: 0.5283   - loss: 325.209  - train AUC: 0.5393   - loss: 324.593 \n",
      "Fold: 5 - val AUC: 0.5330   - loss: 324.824  - train AUC: 0.5384   - loss: 324.716 \n",
      "Score:  - val AUC: 0.5324   - loss: 31068.694 - train AUC: 0.5407   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8977   - loss: 31068.694\n",
      "Cum CV train: 0.9215   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_164', 'var_164_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5409   - loss: 323.802  - train AUC: 0.5468   - loss: 323.638 \n",
      "Fold: 2 - val AUC: 0.5410   - loss: 323.945  - train AUC: 0.5437   - loss: 323.640 \n",
      "Fold: 3 - val AUC: 0.5415   - loss: 323.606  - train AUC: 0.5464   - loss: 323.662 \n",
      "Fold: 4 - val AUC: 0.5435   - loss: 323.835  - train AUC: 0.5447   - loss: 323.656 \n",
      "Fold: 5 - val AUC: 0.5354   - loss: 323.964  - train AUC: 0.5476   - loss: 323.591 \n",
      "Score:  - val AUC: 0.5402   - loss: 31068.694 - train AUC: 0.5476   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.8992   - loss: 31068.694\n",
      "Cum CV train: 0.9227   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_165', 'var_165_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5529   - loss: 323.941  - train AUC: 0.5560   - loss: 323.178 \n",
      "Fold: 2 - val AUC: 0.5512   - loss: 323.615  - train AUC: 0.5576   - loss: 323.183 \n",
      "Fold: 3 - val AUC: 0.5541   - loss: 323.385  - train AUC: 0.5569   - loss: 323.254 \n",
      "Fold: 4 - val AUC: 0.5519   - loss: 323.178  - train AUC: 0.5576   - loss: 323.313 \n",
      "Fold: 5 - val AUC: 0.5560   - loss: 323.333  - train AUC: 0.5570   - loss: 323.203 \n",
      "Score:  - val AUC: 0.5526   - loss: 31068.694 - train AUC: 0.5579   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9011   - loss: 31068.694\n",
      "Cum CV train: 0.9243   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_166', 'var_166_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5460   - loss: 324.012  - train AUC: 0.5601   - loss: 323.208 \n",
      "Fold: 2 - val AUC: 0.5481   - loss: 323.691  - train AUC: 0.5609   - loss: 323.207 \n",
      "Fold: 3 - val AUC: 0.5565   - loss: 323.625  - train AUC: 0.5605   - loss: 323.171 \n",
      "Fold: 4 - val AUC: 0.5482   - loss: 323.919  - train AUC: 0.5621   - loss: 323.163 \n",
      "Fold: 5 - val AUC: 0.5629   - loss: 322.851  - train AUC: 0.5613   - loss: 323.245 \n",
      "Score:  - val AUC: 0.5517   - loss: 31068.694 - train AUC: 0.5627   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9029   - loss: 31068.694\n",
      "Cum CV train: 0.9258   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_167', 'var_167_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5424   - loss: 325.065  - train AUC: 0.5341   - loss: 324.968 \n",
      "Fold: 2 - val AUC: 0.5231   - loss: 325.343  - train AUC: 0.5366   - loss: 324.947 \n",
      "Fold: 3 - val AUC: 0.5309   - loss: 325.016  - train AUC: 0.5353   - loss: 325.001 \n",
      "Fold: 4 - val AUC: 0.5279   - loss: 325.142  - train AUC: 0.5350   - loss: 325.046 \n",
      "Fold: 5 - val AUC: 0.5271   - loss: 325.188  - train AUC: 0.5372   - loss: 324.937 \n",
      "Score:  - val AUC: 0.5296   - loss: 31068.694 - train AUC: 0.5367   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9036   - loss: 31068.694\n",
      "Cum CV train: 0.9264   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_168', 'var_168_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5187   - loss: 325.749  - train AUC: 0.5260   - loss: 325.713 \n",
      "Fold: 2 - val AUC: 0.5216   - loss: 325.788  - train AUC: 0.5244   - loss: 325.724 \n",
      "Fold: 3 - val AUC: 0.5114   - loss: 326.022  - train AUC: 0.5235   - loss: 325.683 \n",
      "Fold: 4 - val AUC: 0.5115   - loss: 325.818  - train AUC: 0.5224   - loss: 325.750 \n",
      "Fold: 5 - val AUC: 0.5083   - loss: 326.108  - train AUC: 0.5253   - loss: 325.698 \n",
      "Score:  - val AUC: 0.5142   - loss: 31068.694 - train AUC: 0.5277   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9038   - loss: 31068.694\n",
      "Cum CV train: 0.9267   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_169', 'var_169_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5509   - loss: 324.447  - train AUC: 0.5516   - loss: 324.423 \n",
      "Fold: 2 - val AUC: 0.5479   - loss: 324.346  - train AUC: 0.5493   - loss: 324.540 \n",
      "Fold: 3 - val AUC: 0.5452   - loss: 324.898  - train AUC: 0.5536   - loss: 324.257 \n",
      "Fold: 4 - val AUC: 0.5417   - loss: 324.984  - train AUC: 0.5524   - loss: 324.355 \n",
      "Fold: 5 - val AUC: 0.5490   - loss: 324.352  - train AUC: 0.5518   - loss: 324.491 \n",
      "Score:  - val AUC: 0.5465   - loss: 31068.694 - train AUC: 0.5535   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9050   - loss: 31068.694\n",
      "Cum CV train: 0.9277   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_170', 'var_170_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5427   - loss: 323.728  - train AUC: 0.5459   - loss: 323.671 \n",
      "Fold: 2 - val AUC: 0.5427   - loss: 323.579  - train AUC: 0.5469   - loss: 323.625 \n",
      "Fold: 3 - val AUC: 0.5332   - loss: 324.874  - train AUC: 0.5482   - loss: 323.344 \n",
      "Fold: 4 - val AUC: 0.5452   - loss: 323.466  - train AUC: 0.5467   - loss: 323.772 \n",
      "Fold: 5 - val AUC: 0.5373   - loss: 323.812  - train AUC: 0.5483   - loss: 323.634 \n",
      "Score:  - val AUC: 0.5394   - loss: 31068.694 - train AUC: 0.5484   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9063   - loss: 31068.694\n",
      "Cum CV train: 0.9288   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_171', 'var_171_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5140   - loss: 325.858  - train AUC: 0.5233   - loss: 325.714 \n",
      "Fold: 2 - val AUC: 0.5089   - loss: 325.926  - train AUC: 0.5217   - loss: 325.740 \n",
      "Fold: 3 - val AUC: 0.5131   - loss: 326.019  - train AUC: 0.5207   - loss: 325.696 \n",
      "Fold: 4 - val AUC: 0.5145   - loss: 325.887  - train AUC: 0.5200   - loss: 325.746 \n",
      "Fold: 5 - val AUC: 0.5187   - loss: 325.725  - train AUC: 0.5237   - loss: 325.732 \n",
      "Score:  - val AUC: 0.5132   - loss: 31068.694 - train AUC: 0.5243   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9065   - loss: 31068.694\n",
      "Cum CV train: 0.9290   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_172', 'var_172_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5376   - loss: 324.825  - train AUC: 0.5454   - loss: 324.447 \n",
      "Fold: 2 - val AUC: 0.5420   - loss: 324.723  - train AUC: 0.5431   - loss: 324.481 \n",
      "Fold: 3 - val AUC: 0.5439   - loss: 324.699  - train AUC: 0.5430   - loss: 324.464 \n",
      "Fold: 4 - val AUC: 0.5397   - loss: 324.628  - train AUC: 0.5441   - loss: 324.497 \n",
      "Fold: 5 - val AUC: 0.5377   - loss: 324.634  - train AUC: 0.5444   - loss: 324.557 \n",
      "Score:  - val AUC: 0.5396   - loss: 31068.694 - train AUC: 0.5454   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9075   - loss: 31068.694\n",
      "Cum CV train: 0.9299   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_173', 'var_173_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5320   - loss: 324.957  - train AUC: 0.5463   - loss: 324.276 \n",
      "Fold: 2 - val AUC: 0.5450   - loss: 324.607  - train AUC: 0.5425   - loss: 324.403 \n",
      "Fold: 3 - val AUC: 0.5426   - loss: 324.283  - train AUC: 0.5454   - loss: 324.361 \n",
      "Fold: 4 - val AUC: 0.5372   - loss: 324.744  - train AUC: 0.5451   - loss: 324.323 \n",
      "Fold: 5 - val AUC: 0.5416   - loss: 324.547  - train AUC: 0.5434   - loss: 324.381 \n",
      "Score:  - val AUC: 0.5389   - loss: 31068.694 - train AUC: 0.5459   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9085   - loss: 31068.694\n",
      "Cum CV train: 0.9308   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_174', 'var_174_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5569   - loss: 321.968  - train AUC: 0.5570   - loss: 322.442 \n",
      "Fold: 2 - val AUC: 0.5583   - loss: 322.183  - train AUC: 0.5571   - loss: 322.395 \n",
      "Fold: 3 - val AUC: 0.5513   - loss: 322.945  - train AUC: 0.5575   - loss: 322.262 \n",
      "Fold: 4 - val AUC: 0.5454   - loss: 322.851  - train AUC: 0.5590   - loss: 322.297 \n",
      "Fold: 5 - val AUC: 0.5584   - loss: 322.773  - train AUC: 0.5557   - loss: 322.302 \n",
      "Score:  - val AUC: 0.5532   - loss: 31068.694 - train AUC: 0.5581   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9104   - loss: 31068.694\n",
      "Cum CV train: 0.9322   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_175', 'var_175_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5185   - loss: 325.856  - train AUC: 0.5297   - loss: 325.472 \n",
      "Fold: 2 - val AUC: 0.5136   - loss: 326.083  - train AUC: 0.5315   - loss: 325.362 \n",
      "Fold: 3 - val AUC: 0.5237   - loss: 325.535  - train AUC: 0.5296   - loss: 325.519 \n",
      "Fold: 4 - val AUC: 0.5248   - loss: 325.697  - train AUC: 0.5291   - loss: 325.464 \n",
      "Fold: 5 - val AUC: 0.5271   - loss: 325.542  - train AUC: 0.5279   - loss: 325.572 \n",
      "Score:  - val AUC: 0.5212   - loss: 31068.694 - train AUC: 0.5306   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9107   - loss: 31068.694\n",
      "Cum CV train: 0.9326   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_176', 'var_176_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5016   - loss: 326.128  - train AUC: 0.5166   - loss: 325.971 \n",
      "Fold: 2 - val AUC: 0.5105   - loss: 326.083  - train AUC: 0.5181   - loss: 325.980 \n",
      "Fold: 3 - val AUC: 0.5029   - loss: 326.315  - train AUC: 0.5171   - loss: 325.914 \n",
      "Fold: 4 - val AUC: 0.5074   - loss: 326.075  - train AUC: 0.5166   - loss: 325.994 \n",
      "Fold: 5 - val AUC: 0.5015   - loss: 326.131  - train AUC: 0.5199   - loss: 325.935 \n",
      "Score:  - val AUC: 0.5046   - loss: 31068.694 - train AUC: 0.5215   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9107   - loss: 31068.694\n",
      "Cum CV train: 0.9327   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_177', 'var_177_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5328   - loss: 324.808  - train AUC: 0.5424   - loss: 324.134 \n",
      "Fold: 2 - val AUC: 0.5345   - loss: 324.345  - train AUC: 0.5428   - loss: 324.237 \n",
      "Fold: 3 - val AUC: 0.5335   - loss: 324.487  - train AUC: 0.5433   - loss: 324.202 \n",
      "Fold: 4 - val AUC: 0.5395   - loss: 324.049  - train AUC: 0.5433   - loss: 324.229 \n",
      "Fold: 5 - val AUC: 0.5308   - loss: 324.542  - train AUC: 0.5448   - loss: 324.123 \n",
      "Score:  - val AUC: 0.5339   - loss: 31068.694 - train AUC: 0.5449   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9118   - loss: 31068.694\n",
      "Cum CV train: 0.9336   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_178', 'var_178_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5189   - loss: 325.779  - train AUC: 0.5290   - loss: 325.564 \n",
      "Fold: 2 - val AUC: 0.5179   - loss: 325.798  - train AUC: 0.5328   - loss: 325.399 \n",
      "Fold: 3 - val AUC: 0.5263   - loss: 325.603  - train AUC: 0.5268   - loss: 325.595 \n",
      "Fold: 4 - val AUC: 0.5255   - loss: 325.740  - train AUC: 0.5254   - loss: 325.640 \n",
      "Fold: 5 - val AUC: 0.5119   - loss: 325.987  - train AUC: 0.5276   - loss: 325.568 \n",
      "Score:  - val AUC: 0.5194   - loss: 31068.694 - train AUC: 0.5301   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9121   - loss: 31068.694\n",
      "Cum CV train: 0.9339   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_179', 'var_179_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5441   - loss: 323.145  - train AUC: 0.5544   - loss: 323.228 \n",
      "Fold: 2 - val AUC: 0.5407   - loss: 323.771  - train AUC: 0.5528   - loss: 323.164 \n",
      "Fold: 3 - val AUC: 0.5437   - loss: 323.626  - train AUC: 0.5547   - loss: 323.127 \n",
      "Fold: 4 - val AUC: 0.5503   - loss: 323.417  - train AUC: 0.5529   - loss: 323.164 \n",
      "Fold: 5 - val AUC: 0.5426   - loss: 323.682  - train AUC: 0.5507   - loss: 323.192 \n",
      "Score:  - val AUC: 0.5438   - loss: 31068.694 - train AUC: 0.5560   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9133   - loss: 31068.694\n",
      "Cum CV train: 0.9349   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_180', 'var_180_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5190   - loss: 325.548  - train AUC: 0.5358   - loss: 324.801 \n",
      "Fold: 2 - val AUC: 0.5301   - loss: 325.063  - train AUC: 0.5334   - loss: 324.908 \n",
      "Fold: 3 - val AUC: 0.5367   - loss: 324.593  - train AUC: 0.5329   - loss: 325.039 \n",
      "Fold: 4 - val AUC: 0.5187   - loss: 325.300  - train AUC: 0.5343   - loss: 324.864 \n",
      "Fold: 5 - val AUC: 0.5329   - loss: 325.007  - train AUC: 0.5308   - loss: 324.951 \n",
      "Score:  - val AUC: 0.5263   - loss: 31068.694 - train AUC: 0.5348   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9140   - loss: 31068.694\n",
      "Cum CV train: 0.9355   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_181', 'var_181_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5142   - loss: 325.966  - train AUC: 0.5263   - loss: 325.714 \n",
      "Fold: 2 - val AUC: 0.5195   - loss: 325.924  - train AUC: 0.5237   - loss: 325.795 \n",
      "Fold: 3 - val AUC: 0.5109   - loss: 326.042  - train AUC: 0.5253   - loss: 325.749 \n",
      "Fold: 4 - val AUC: 0.5069   - loss: 326.058  - train AUC: 0.5246   - loss: 325.797 \n",
      "Fold: 5 - val AUC: 0.5147   - loss: 325.903  - train AUC: 0.5256   - loss: 325.779 \n",
      "Score:  - val AUC: 0.5131   - loss: 31068.694 - train AUC: 0.5275   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9141   - loss: 31068.694\n",
      "Cum CV train: 0.9357   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_182', 'var_182_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5039   - loss: 326.040  - train AUC: 0.5162   - loss: 325.992 \n",
      "Fold: 2 - val AUC: 0.5063   - loss: 326.084  - train AUC: 0.5150   - loss: 325.951 \n",
      "Fold: 3 - val AUC: 0.5029   - loss: 326.115  - train AUC: 0.5159   - loss: 325.962 \n",
      "Fold: 4 - val AUC: 0.5024   - loss: 326.187  - train AUC: 0.5131   - loss: 325.947 \n",
      "Fold: 5 - val AUC: 0.5031   - loss: 326.132  - train AUC: 0.5163   - loss: 325.944 \n",
      "Score:  - val AUC: 0.5031   - loss: 31068.694 - train AUC: 0.5175   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9142   - loss: 31068.694\n",
      "Cum CV train: 0.9358   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_183', 'var_183_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9142   - loss: 31068.694\n",
      "Cum CV train: 0.9358   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_184', 'var_184_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5483   - loss: 324.018  - train AUC: 0.5556   - loss: 323.419 \n",
      "Fold: 2 - val AUC: 0.5447   - loss: 324.033  - train AUC: 0.5532   - loss: 323.582 \n",
      "Fold: 3 - val AUC: 0.5605   - loss: 323.066  - train AUC: 0.5525   - loss: 323.747 \n",
      "Fold: 4 - val AUC: 0.5386   - loss: 324.482  - train AUC: 0.5546   - loss: 323.523 \n",
      "Fold: 5 - val AUC: 0.5509   - loss: 323.770  - train AUC: 0.5550   - loss: 323.482 \n",
      "Score:  - val AUC: 0.5481   - loss: 31068.694 - train AUC: 0.5559   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9156   - loss: 31068.694\n",
      "Cum CV train: 0.9370   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_185', 'var_185_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 2 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 3 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 4 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Fold: 5 - val AUC: 0.5000   - loss: 693.147  - train AUC: 0.5000   - loss: 693.147 \n",
      "Score:  - val AUC: 0.5000   - loss: 31068.694 - train AUC: 0.5000   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9156   - loss: 31068.694\n",
      "Cum CV train: 0.9370   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_186', 'var_186_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5267   - loss: 325.443  - train AUC: 0.5363   - loss: 325.294 \n",
      "Fold: 2 - val AUC: 0.5341   - loss: 325.345  - train AUC: 0.5337   - loss: 325.352 \n",
      "Fold: 3 - val AUC: 0.5199   - loss: 325.936  - train AUC: 0.5354   - loss: 325.225 \n",
      "Fold: 4 - val AUC: 0.5293   - loss: 325.492  - train AUC: 0.5350   - loss: 325.309 \n",
      "Fold: 5 - val AUC: 0.5344   - loss: 325.320  - train AUC: 0.5316   - loss: 325.372 \n",
      "Score:  - val AUC: 0.5285   - loss: 31068.694 - train AUC: 0.5355   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9159   - loss: 31068.694\n",
      "Cum CV train: 0.9373   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_187', 'var_187_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5201   - loss: 325.894  - train AUC: 0.5259   - loss: 325.322 \n",
      "Fold: 2 - val AUC: 0.5061   - loss: 325.969  - train AUC: 0.5288   - loss: 325.303 \n",
      "Fold: 3 - val AUC: 0.5132   - loss: 325.694  - train AUC: 0.5259   - loss: 325.414 \n",
      "Fold: 4 - val AUC: 0.5128   - loss: 325.468  - train AUC: 0.5273   - loss: 325.453 \n",
      "Fold: 5 - val AUC: 0.5141   - loss: 325.499  - train AUC: 0.5270   - loss: 325.410 \n",
      "Score:  - val AUC: 0.5127   - loss: 31068.694 - train AUC: 0.5312   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9163   - loss: 31068.694\n",
      "Cum CV train: 0.9378   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_188', 'var_188_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5312   - loss: 324.538  - train AUC: 0.5378   - loss: 324.717 \n",
      "Fold: 2 - val AUC: 0.5300   - loss: 324.942  - train AUC: 0.5371   - loss: 324.623 \n",
      "Fold: 3 - val AUC: 0.5285   - loss: 324.749  - train AUC: 0.5383   - loss: 324.638 \n",
      "Fold: 4 - val AUC: 0.5376   - loss: 324.710  - train AUC: 0.5369   - loss: 324.686 \n",
      "Fold: 5 - val AUC: 0.5235   - loss: 325.415  - train AUC: 0.5388   - loss: 324.521 \n",
      "Score:  - val AUC: 0.5294   - loss: 31068.694 - train AUC: 0.5389   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9171   - loss: 31068.694\n",
      "Cum CV train: 0.9384   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_189', 'var_189_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5090   - loss: 326.086  - train AUC: 0.5210   - loss: 325.915 \n",
      "Fold: 2 - val AUC: 0.4953   - loss: 326.308  - train AUC: 0.5200   - loss: 325.891 \n",
      "Fold: 3 - val AUC: 0.5127   - loss: 326.141  - train AUC: 0.5169   - loss: 325.916 \n",
      "Fold: 4 - val AUC: 0.5034   - loss: 326.191  - train AUC: 0.5193   - loss: 325.929 \n",
      "Fold: 5 - val AUC: 0.5137   - loss: 326.014  - train AUC: 0.5206   - loss: 325.894 \n",
      "Score:  - val AUC: 0.5071   - loss: 31068.694 - train AUC: 0.5222   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9171   - loss: 31068.694\n",
      "Cum CV train: 0.9385   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_190', 'var_190_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5536   - loss: 323.473  - train AUC: 0.5573   - loss: 323.260 \n",
      "Fold: 2 - val AUC: 0.5568   - loss: 323.299  - train AUC: 0.5553   - loss: 323.366 \n",
      "Fold: 3 - val AUC: 0.5475   - loss: 323.899  - train AUC: 0.5572   - loss: 323.196 \n",
      "Fold: 4 - val AUC: 0.5516   - loss: 323.270  - train AUC: 0.5582   - loss: 323.235 \n",
      "Fold: 5 - val AUC: 0.5524   - loss: 323.646  - train AUC: 0.5570   - loss: 323.216 \n",
      "Score:  - val AUC: 0.5514   - loss: 31068.694 - train AUC: 0.5580   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9186   - loss: 31068.694\n",
      "Cum CV train: 0.9397   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_191', 'var_191_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5460   - loss: 324.301  - train AUC: 0.5514   - loss: 323.819 \n",
      "Fold: 2 - val AUC: 0.5392   - loss: 324.651  - train AUC: 0.5522   - loss: 323.732 \n",
      "Fold: 3 - val AUC: 0.5535   - loss: 323.703  - train AUC: 0.5497   - loss: 323.958 \n",
      "Fold: 4 - val AUC: 0.5388   - loss: 324.213  - train AUC: 0.5514   - loss: 323.910 \n",
      "Fold: 5 - val AUC: 0.5443   - loss: 324.169  - train AUC: 0.5523   - loss: 323.782 \n",
      "Score:  - val AUC: 0.5439   - loss: 31068.694 - train AUC: 0.5527   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9196   - loss: 31068.694\n",
      "Cum CV train: 0.9406   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_192', 'var_192_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5436   - loss: 325.061  - train AUC: 0.5485   - loss: 324.822 \n",
      "Fold: 2 - val AUC: 0.5419   - loss: 325.177  - train AUC: 0.5490   - loss: 324.810 \n",
      "Fold: 3 - val AUC: 0.5406   - loss: 325.210  - train AUC: 0.5493   - loss: 324.788 \n",
      "Fold: 4 - val AUC: 0.5473   - loss: 324.914  - train AUC: 0.5482   - loss: 324.823 \n",
      "Fold: 5 - val AUC: 0.5437   - loss: 324.983  - train AUC: 0.5488   - loss: 324.810 \n",
      "Score:  - val AUC: 0.5423   - loss: 31068.694 - train AUC: 0.5505   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9203   - loss: 31068.694\n",
      "Cum CV train: 0.9412   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_193', 'var_193_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5138   - loss: 325.970  - train AUC: 0.5254   - loss: 325.675 \n",
      "Fold: 2 - val AUC: 0.5180   - loss: 325.846  - train AUC: 0.5253   - loss: 325.635 \n",
      "Fold: 3 - val AUC: 0.5115   - loss: 325.971  - train AUC: 0.5251   - loss: 325.644 \n",
      "Fold: 4 - val AUC: 0.5136   - loss: 325.858  - train AUC: 0.5244   - loss: 325.695 \n",
      "Fold: 5 - val AUC: 0.5106   - loss: 325.946  - train AUC: 0.5277   - loss: 325.629 \n",
      "Score:  - val AUC: 0.5129   - loss: 31068.694 - train AUC: 0.5290   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9204   - loss: 31068.694\n",
      "Cum CV train: 0.9415   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_194', 'var_194_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5249   - loss: 325.530  - train AUC: 0.5283   - loss: 325.440 \n",
      "Fold: 2 - val AUC: 0.5132   - loss: 325.968  - train AUC: 0.5266   - loss: 325.486 \n",
      "Fold: 3 - val AUC: 0.5164   - loss: 325.891  - train AUC: 0.5270   - loss: 325.487 \n",
      "Fold: 4 - val AUC: 0.5217   - loss: 325.581  - train AUC: 0.5261   - loss: 325.555 \n",
      "Fold: 5 - val AUC: 0.5214   - loss: 325.576  - train AUC: 0.5264   - loss: 325.471 \n",
      "Score:  - val AUC: 0.5192   - loss: 31068.694 - train AUC: 0.5292   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9206   - loss: 31068.694\n",
      "Cum CV train: 0.9417   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_195', 'var_195_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5255   - loss: 325.620  - train AUC: 0.5352   - loss: 325.221 \n",
      "Fold: 2 - val AUC: 0.5281   - loss: 325.554  - train AUC: 0.5341   - loss: 325.288 \n",
      "Fold: 3 - val AUC: 0.5314   - loss: 325.348  - train AUC: 0.5333   - loss: 325.351 \n",
      "Fold: 4 - val AUC: 0.5214   - loss: 325.736  - train AUC: 0.5330   - loss: 325.331 \n",
      "Fold: 5 - val AUC: 0.5263   - loss: 325.333  - train AUC: 0.5332   - loss: 325.337 \n",
      "Score:  - val AUC: 0.5256   - loss: 31068.694 - train AUC: 0.5352   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9210   - loss: 31068.694\n",
      "Cum CV train: 0.9420   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_196', 'var_196_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5294   - loss: 325.392  - train AUC: 0.5259   - loss: 325.335 \n",
      "Fold: 2 - val AUC: 0.5152   - loss: 325.750  - train AUC: 0.5282   - loss: 325.291 \n",
      "Fold: 3 - val AUC: 0.5168   - loss: 325.555  - train AUC: 0.5280   - loss: 325.323 \n",
      "Fold: 4 - val AUC: 0.5206   - loss: 325.392  - train AUC: 0.5279   - loss: 325.333 \n",
      "Fold: 5 - val AUC: 0.5265   - loss: 325.325  - train AUC: 0.5247   - loss: 325.377 \n",
      "Score:  - val AUC: 0.5210   - loss: 31068.694 - train AUC: 0.5288   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9213   - loss: 31068.694\n",
      "Cum CV train: 0.9423   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_197', 'var_197_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5356   - loss: 325.153  - train AUC: 0.5411   - loss: 324.777 \n",
      "Fold: 2 - val AUC: 0.5344   - loss: 325.061  - train AUC: 0.5428   - loss: 324.702 \n",
      "Fold: 3 - val AUC: 0.5276   - loss: 325.214  - train AUC: 0.5400   - loss: 324.861 \n",
      "Fold: 4 - val AUC: 0.5321   - loss: 325.030  - train AUC: 0.5388   - loss: 324.860 \n",
      "Fold: 5 - val AUC: 0.5266   - loss: 325.418  - train AUC: 0.5422   - loss: 324.752 \n",
      "Score:  - val AUC: 0.5310   - loss: 31068.694 - train AUC: 0.5429   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9218   - loss: 31068.694\n",
      "Cum CV train: 0.9429   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_198', 'var_198_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5513   - loss: 323.511  - train AUC: 0.5553   - loss: 323.257 \n",
      "Fold: 2 - val AUC: 0.5451   - loss: 324.244  - train AUC: 0.5573   - loss: 323.043 \n",
      "Fold: 3 - val AUC: 0.5520   - loss: 322.986  - train AUC: 0.5553   - loss: 323.367 \n",
      "Fold: 4 - val AUC: 0.5502   - loss: 323.352  - train AUC: 0.5548   - loss: 323.295 \n",
      "Fold: 5 - val AUC: 0.5498   - loss: 323.333  - train AUC: 0.5558   - loss: 323.289 \n",
      "Score:  - val AUC: 0.5495   - loss: 31068.694 - train AUC: 0.5567   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9233   - loss: 31068.694\n",
      "Cum CV train: 0.9440   - loss: 31068.694\n",
      "**************************************************\n",
      "\n",
      "Training on: ['var_199', 'var_199_count']\n",
      "\n",
      "setting:  4\n",
      "Fold: 1 - val AUC: 0.5296   - loss: 325.588  - train AUC: 0.5350   - loss: 325.282 \n",
      "Fold: 2 - val AUC: 0.5156   - loss: 325.914  - train AUC: 0.5361   - loss: 325.243 \n",
      "Fold: 3 - val AUC: 0.5282   - loss: 325.582  - train AUC: 0.5372   - loss: 325.314 \n",
      "Fold: 4 - val AUC: 0.5258   - loss: 325.516  - train AUC: 0.5381   - loss: 325.256 \n",
      "Fold: 5 - val AUC: 0.5233   - loss: 325.594  - train AUC: 0.5360   - loss: 325.332 \n",
      "Score:  - val AUC: 0.5242   - loss: 31068.694 - train AUC: 0.5405   - loss: 31068.694\n",
      "\n",
      "best setting:  4\n",
      "Cum CV val  : 0.9236   - loss: 31068.694\n",
      "Cum CV train: 0.9444   - loss: 31068.694\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "early_stopping_rounds=10\n",
    "settings = [4]\n",
    "np.random.seed(47)\n",
    "\n",
    "settings_best_ind = []\n",
    "\n",
    "def train_trees():\n",
    "    preds_oof = np.zeros((len(X_train), len(features)))\n",
    "    preds_test = np.zeros((len(X_test), len(features)))\n",
    "    preds_train = np.zeros((len(X_train), len(features)))\n",
    "    preds_fake = np.zeros((len(X_fake), len(features)))\n",
    "\n",
    "    features_used_flatten = [var for sublist in features_used for var in sublist]\n",
    "    X_train_used = X_train[features_used_flatten]\n",
    "    X_test_used = X_test[features_used_flatten]\n",
    "    X_fake_used = X_fake[features_used_flatten]\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        params['max_bin'] = max_bin_values[max_bin_var[i]]\n",
    "        params['learning_rate'] = learning_rate_values[learning_rate_var[i]]\n",
    "        params['reg_alpha'] = reg_alpha_values[reg_alpha_var[i]]\n",
    "        params['num_leaves'] = num_leaves_values[num_leaves_var[i]]\n",
    "        features_train = [feature_set[i] for feature_set in features_used] \n",
    "        print(f'Training on: {features_train}')\n",
    "        folds = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=np.random.randint(100000))\n",
    "        list_folds = list(folds.split(X_train_used.values, target_train.values))\n",
    "        preds_oof_temp = np.zeros((preds_oof.shape[0], len(settings)))\n",
    "        preds_test_temp = np.zeros((preds_test.shape[0], len(settings)))\n",
    "        preds_train_temp = np.zeros((preds_train.shape[0], len(settings)))\n",
    "        preds_fake_temp = np.zeros((preds_fake.shape[0], len(settings)))\n",
    "\n",
    "        scores = []\n",
    "        for j, setting in enumerate(settings):\n",
    "            # setting is used for hyperparameter tuning, here you can add sometinh like params['num_leaves'] = setting\n",
    "            print('\\nsetting: ', setting)\n",
    "            for k, (trn_idx, val_idx) in enumerate(list_folds):\n",
    "                print(\"Fold: {}\".format(k+1), end=\"\")\n",
    "                trn_data = lgb.Dataset(X_train_used.iloc[trn_idx][features_train], label=target_train.iloc[trn_idx])\n",
    "                val_data = lgb.Dataset(X_train_used.iloc[val_idx][features_train], label=target_train.iloc[val_idx])\n",
    "\n",
    "                # Binary Log Loss\n",
    "                clf = lgb.train(params, trn_data, 2000, valid_sets=[trn_data, val_data], verbose_eval=False, early_stopping_rounds=early_stopping_rounds) \n",
    "\n",
    "                prediction_val1 = clf.predict(X_train_used.iloc[val_idx][features_train])\n",
    "                prediction_test1 = clf.predict(X_test_used[features_train])\n",
    "                prediction_train1 = clf.predict(X_train_used.iloc[trn_idx][features_train])\n",
    "                prediction_fake1 = clf.predict(X_fake_used[features_train])\n",
    "\n",
    "                # Predictions\n",
    "                s1 = roc_auc_score(target_train.iloc[val_idx], prediction_val1)\n",
    "                s1_log = log_loss(target_train.iloc[val_idx], prediction_val1)\n",
    "                print(' - val AUC: {:<8.4f} - loss: {:<8.3f}'.format(s1, s1_log*1000), end='')\n",
    "\n",
    "                # Predictions Test\n",
    "                if use_experimental:\n",
    "                    s1_test = roc_auc_score(target_test, prediction_test1)\n",
    "                    s1_log_test = log_loss(target_test, prediction_test1)\n",
    "                    print(' - test AUC: {:<8.4f} - loss: {:<8.3f}'.format(s1_test, s1_log_test*1000), end='')\n",
    "\n",
    "                # Predictions Train\n",
    "                s1_train = roc_auc_score(target_train.iloc[trn_idx], prediction_train1)\n",
    "                s1_log_train = log_loss(target_train.iloc[trn_idx], prediction_train1)\n",
    "                print(' - train AUC: {:<8.4f} - loss: {:<8.3f}'.format(s1_train, s1_log_train*1000), end='')\n",
    "                if use_experimental:\n",
    "                    print('',clf.feature_importance(), end='')\n",
    "\n",
    "                print('')\n",
    "\n",
    "\n",
    "                preds_oof_temp[val_idx,j] += np.sqrt(prediction_val1 - prediction_val1.mean() + 0.1) \n",
    "                preds_test_temp[:,j] += np.sqrt(prediction_test1 - prediction_test1.mean() + 0.1) / n_folds\n",
    "                preds_train_temp[trn_idx,j] += np.sqrt(prediction_train1 - prediction_train1.mean() + 0.1) / (n_folds-1)\n",
    "                preds_fake_temp[:,j] += np.sqrt(prediction_fake1 - prediction_fake1.mean() + 0.1) / n_folds\n",
    "\n",
    "            score_setting = roc_auc_score(target_train, preds_oof_temp[:,j])\n",
    "            score_setting_log = 1000*log_loss(target_train, np.exp(preds_oof_temp[:,j]))\n",
    "            scores.append(score_setting_log)\n",
    "            print(\"Score:  - val AUC: {:<8.4f} - loss: {:<8.3f}\".format(score_setting, score_setting_log), end='')\n",
    "            if use_experimental:\n",
    "                score_setting_test = roc_auc_score(target_test, preds_test_temp[:,j])\n",
    "                score_setting_log_test = 1000*log_loss(target_test, np.exp(preds_test_temp[:,j]))  \n",
    "                print(\" - test AUC: {:<8.4f} - loss: {:<8.3f}\".format(score_setting_test, score_setting_log_test), end='')\n",
    "\n",
    "            score_setting_train = roc_auc_score(target_train, preds_train_temp[:,j])\n",
    "            score_setting_log_train = 1000*log_loss(target_train, np.exp(preds_train_temp[:,j]))\n",
    "            print(\" - train AUC: {:<8.4f} - loss: {:<8.3f}\".format(score_setting_train, score_setting_log_train))\n",
    "\n",
    "        best_ind = np.argmin(scores)\n",
    "        settings_best_ind.append(best_ind)\n",
    "        preds_oof[:,i] = preds_oof_temp[:,best_ind]\n",
    "        preds_test[:,i] = preds_test_temp[:,best_ind]\n",
    "        preds_train[:,i] = preds_train_temp[:,best_ind]\n",
    "        preds_fake[:,i] = preds_fake_temp[:,best_ind]\n",
    "\n",
    "\n",
    "        print('\\nbest setting: ', settings[best_ind])\n",
    "        preds_oof_cum = preds_oof[:,:i+1].mean(axis=1)\n",
    "        print(\"Cum CV val  : {:<8.4f} - loss: {:<8.3f}\".format(roc_auc_score(target_train, preds_oof_cum), 1000*log_loss(target_train, np.exp(preds_oof_cum))))\n",
    "        if use_experimental:        \n",
    "            preds_test_cum = preds_test[:,:i+1].mean(axis=1)\n",
    "            print(\"Cum CV test : {:<8.4f} - loss: {:<8.3f}\".format(roc_auc_score(target_test, preds_test_cum), 1000*log_loss(target_test, np.exp(preds_test_cum))))\n",
    "        preds_train_cum = preds_train[:,:i+1].mean(axis=1)\n",
    "        print(\"Cum CV train: {:<8.4f} - loss: {:<8.3f}\".format(roc_auc_score(target_train, preds_train_cum), 1000*log_loss(target_train, np.exp(preds_train_cum))))\n",
    "        print('*****' * 10 + '\\n')\n",
    "        \n",
    "    return preds_oof, preds_test, preds_train, preds_fake\n",
    "\n",
    "preds_oof, preds_test, preds_train, preds_fake = train_trees()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_0 Cum val: 0.54906  - train: 0.55850 \n",
      "var_1 Cum val: 0.57485  - train: 0.58226 \n",
      "var_2 Cum val: 0.59720  - train: 0.60543 \n",
      "var_3 Cum val: 0.59893  - train: 0.60803 \n",
      "var_4 Cum val: 0.59928  - train: 0.61003 \n",
      "var_5 Cum val: 0.60839  - train: 0.62002 \n",
      "var_6 Cum val: 0.62851  - train: 0.63880 \n",
      "var_7 Cum val: 0.62851  - train: 0.63880 \n",
      "var_8 Cum val: 0.63049  - train: 0.64175 \n",
      "var_9 Cum val: 0.64118  - train: 0.65268 \n",
      "var_10 Cum val: 0.64118  - train: 0.65268 \n",
      "var_11 Cum val: 0.64365  - train: 0.65668 \n",
      "var_12 Cum val: 0.66050  - train: 0.67297 \n",
      "var_13 Cum val: 0.67179  - train: 0.68405 \n",
      "var_14 Cum val: 0.67179  - train: 0.68405 \n",
      "var_15 Cum val: 0.67279  - train: 0.68580 \n",
      "var_16 Cum val: 0.67305  - train: 0.68673 \n",
      "var_17 Cum val: 0.67305  - train: 0.68673 \n",
      "var_18 Cum val: 0.68082  - train: 0.69517 \n",
      "var_19 Cum val: 0.68176  - train: 0.69678 \n",
      "var_20 Cum val: 0.68365  - train: 0.69920 \n",
      "var_21 Cum val: 0.69445  - train: 0.70989 \n",
      "var_22 Cum val: 0.70388  - train: 0.71943 \n",
      "var_23 Cum val: 0.70541  - train: 0.72138 \n",
      "var_24 Cum val: 0.70763  - train: 0.72435 \n",
      "var_25 Cum val: 0.70777  - train: 0.72514 \n",
      "var_26 Cum val: 0.71844  - train: 0.73553 \n",
      "var_27 Cum val: 0.71844  - train: 0.73553 \n",
      "var_28 Cum val: 0.71966  - train: 0.73728 \n",
      "var_29 Cum val: 0.71966  - train: 0.73728 \n",
      "var_30 Cum val: 0.71966  - train: 0.73728 \n",
      "var_31 Cum val: 0.72081  - train: 0.73874 \n",
      "var_32 Cum val: 0.72389  - train: 0.74205 \n",
      "var_33 Cum val: 0.72979  - train: 0.74828 \n",
      "var_34 Cum val: 0.73476  - train: 0.75321 \n",
      "var_35 Cum val: 0.73782  - train: 0.75650 \n",
      "var_36 Cum val: 0.74092  - train: 0.75985 \n",
      "var_37 Cum val: 0.74112  - train: 0.76055 \n",
      "var_38 Cum val: 0.74112  - train: 0.76055 \n",
      "var_39 Cum val: 0.74112  - train: 0.76055 \n",
      "var_40 Cum val: 0.74718  - train: 0.76635 \n",
      "var_41 Cum val: 0.74715  - train: 0.76643 \n",
      "var_42 Cum val: 0.74713  - train: 0.76657 \n",
      "var_43 Cum val: 0.74858  - train: 0.76834 \n",
      "var_44 Cum val: 0.75548  - train: 0.77481 \n",
      "var_45 Cum val: 0.75686  - train: 0.77657 \n",
      "var_46 Cum val: 0.75686  - train: 0.77657 \n",
      "var_47 Cum val: 0.75686  - train: 0.77657 \n",
      "var_48 Cum val: 0.75902  - train: 0.77879 \n",
      "var_49 Cum val: 0.76173  - train: 0.78159 \n",
      "var_50 Cum val: 0.76198  - train: 0.78268 \n",
      "var_51 Cum val: 0.76414  - train: 0.78517 \n",
      "var_52 Cum val: 0.76610  - train: 0.78746 \n",
      "var_53 Cum val: 0.77300  - train: 0.79390 \n",
      "var_54 Cum val: 0.77356  - train: 0.79482 \n",
      "var_55 Cum val: 0.77428  - train: 0.79581 \n",
      "var_56 Cum val: 0.77658  - train: 0.79824 \n",
      "var_57 Cum val: 0.77685  - train: 0.79890 \n",
      "var_58 Cum val: 0.77796  - train: 0.80026 \n",
      "var_59 Cum val: 0.77815  - train: 0.80092 \n",
      "var_60 Cum val: 0.77837  - train: 0.80149 \n",
      "var_61 Cum val: 0.77851  - train: 0.80193 \n",
      "var_62 Cum val: 0.77874  - train: 0.80274 \n",
      "var_63 Cum val: 0.77949  - train: 0.80383 \n",
      "var_64 Cum val: 0.77975  - train: 0.80451 \n",
      "var_65 Cum val: 0.77990  - train: 0.80496 \n",
      "var_66 Cum val: 0.78063  - train: 0.80595 \n",
      "var_67 Cum val: 0.78379  - train: 0.80908 \n",
      "var_68 Cum val: 0.78402  - train: 0.80964 \n",
      "var_69 Cum val: 0.78438  - train: 0.81041 \n",
      "var_70 Cum val: 0.78612  - train: 0.81223 \n",
      "var_71 Cum val: 0.78735  - train: 0.81377 \n",
      "var_72 Cum val: 0.78754  - train: 0.81432 \n",
      "var_73 Cum val: 0.78769  - train: 0.81472 \n",
      "var_74 Cum val: 0.78848  - train: 0.81606 \n",
      "var_75 Cum val: 0.79158  - train: 0.81909 \n",
      "var_76 Cum val: 0.79715  - train: 0.82416 \n",
      "var_77 Cum val: 0.79757  - train: 0.82487 \n",
      "var_78 Cum val: 0.80152  - train: 0.82835 \n",
      "var_79 Cum val: 0.80152  - train: 0.82835 \n",
      "var_80 Cum val: 0.80714  - train: 0.83326 \n",
      "var_81 Cum val: 0.81556  - train: 0.84035 \n",
      "var_82 Cum val: 0.81704  - train: 0.84205 \n",
      "var_83 Cum val: 0.81815  - train: 0.84328 \n",
      "var_84 Cum val: 0.81828  - train: 0.84365 \n",
      "var_85 Cum val: 0.81918  - train: 0.84462 \n",
      "var_86 Cum val: 0.82225  - train: 0.84759 \n",
      "var_87 Cum val: 0.82362  - train: 0.84906 \n",
      "var_88 Cum val: 0.82420  - train: 0.84969 \n",
      "var_89 Cum val: 0.82577  - train: 0.85139 \n",
      "var_90 Cum val: 0.82705  - train: 0.85265 \n",
      "var_91 Cum val: 0.82876  - train: 0.85430 \n",
      "var_92 Cum val: 0.83170  - train: 0.85700 \n",
      "var_93 Cum val: 0.83278  - train: 0.85824 \n",
      "var_94 Cum val: 0.83534  - train: 0.86062 \n",
      "var_95 Cum val: 0.83678  - train: 0.86205 \n",
      "var_96 Cum val: 0.83678  - train: 0.86205 \n",
      "var_97 Cum val: 0.83741  - train: 0.86282 \n",
      "var_98 Cum val: 0.83741  - train: 0.86282 \n",
      "var_99 Cum val: 0.84093  - train: 0.86590 \n",
      "var_100 Cum val: 0.84093  - train: 0.86590 \n",
      "var_101 Cum val: 0.84094  - train: 0.86611 \n",
      "var_102 Cum val: 0.84167  - train: 0.86695 \n",
      "var_103 Cum val: 0.84167  - train: 0.86695 \n",
      "var_104 Cum val: 0.84216  - train: 0.86756 \n",
      "var_105 Cum val: 0.84269  - train: 0.86824 \n",
      "var_106 Cum val: 0.84375  - train: 0.86940 \n",
      "var_107 Cum val: 0.84564  - train: 0.87124 \n",
      "var_108 Cum val: 0.84833  - train: 0.87364 \n",
      "var_109 Cum val: 0.85161  - train: 0.87650 \n",
      "var_110 Cum val: 0.85527  - train: 0.87966 \n",
      "var_111 Cum val: 0.85596  - train: 0.88041 \n",
      "var_112 Cum val: 0.85671  - train: 0.88125 \n",
      "var_113 Cum val: 0.85706  - train: 0.88174 \n",
      "var_114 Cum val: 0.85778  - train: 0.88253 \n",
      "var_115 Cum val: 0.85947  - train: 0.88406 \n",
      "var_116 Cum val: 0.85984  - train: 0.88454 \n",
      "var_117 Cum val: 0.85988  - train: 0.88477 \n",
      "var_118 Cum val: 0.86144  - train: 0.88634 \n",
      "var_119 Cum val: 0.86267  - train: 0.88759 \n",
      "var_120 Cum val: 0.86285  - train: 0.88798 \n",
      "var_121 Cum val: 0.86456  - train: 0.88961 \n",
      "var_122 Cum val: 0.86637  - train: 0.89123 \n",
      "var_123 Cum val: 0.86816  - train: 0.89281 \n",
      "var_124 Cum val: 0.86815  - train: 0.89285 \n",
      "var_125 Cum val: 0.86871  - train: 0.89345 \n",
      "var_126 Cum val: 0.86871  - train: 0.89345 \n",
      "var_127 Cum val: 0.87031  - train: 0.89494 \n",
      "var_128 Cum val: 0.87101  - train: 0.89572 \n",
      "var_129 Cum val: 0.87111  - train: 0.89599 \n",
      "var_130 Cum val: 0.87215  - train: 0.89697 \n",
      "var_131 Cum val: 0.87283  - train: 0.89768 \n",
      "var_132 Cum val: 0.87337  - train: 0.89827 \n",
      "var_133 Cum val: 0.87535  - train: 0.90000 \n",
      "var_134 Cum val: 0.87570  - train: 0.90041 \n",
      "var_135 Cum val: 0.87664  - train: 0.90129 \n",
      "var_136 Cum val: 0.87663  - train: 0.90130 \n",
      "var_137 Cum val: 0.87740  - train: 0.90203 \n",
      "var_138 Cum val: 0.87766  - train: 0.90238 \n",
      "var_139 Cum val: 0.88198  - train: 0.90584 \n",
      "var_140 Cum val: 0.88203  - train: 0.90602 \n",
      "var_141 Cum val: 0.88305  - train: 0.90701 \n",
      "var_142 Cum val: 0.88335  - train: 0.90753 \n",
      "var_143 Cum val: 0.88343  - train: 0.90775 \n",
      "var_144 Cum val: 0.88375  - train: 0.90825 \n",
      "var_145 Cum val: 0.88456  - train: 0.90895 \n",
      "var_146 Cum val: 0.88717  - train: 0.91130 \n",
      "var_147 Cum val: 0.88881  - train: 0.91272 \n",
      "var_148 Cum val: 0.89026  - train: 0.91396 \n",
      "var_149 Cum val: 0.89166  - train: 0.91522 \n",
      "var_150 Cum val: 0.89221  - train: 0.91584 \n",
      "var_151 Cum val: 0.89270  - train: 0.91634 \n",
      "var_152 Cum val: 0.89282  - train: 0.91652 \n",
      "var_153 Cum val: 0.89281  - train: 0.91653 \n",
      "var_154 Cum val: 0.89408  - train: 0.91760 \n",
      "var_155 Cum val: 0.89517  - train: 0.91865 \n",
      "var_156 Cum val: 0.89541  - train: 0.91898 \n",
      "var_157 Cum val: 0.89621  - train: 0.91969 \n",
      "var_158 Cum val: 0.89620  - train: 0.91972 \n",
      "var_159 Cum val: 0.89625  - train: 0.91987 \n",
      "var_160 Cum val: 0.89627  - train: 0.92010 \n",
      "var_161 Cum val: 0.89627  - train: 0.92010 \n",
      "var_162 Cum val: 0.89683  - train: 0.92070 \n",
      "var_163 Cum val: 0.89775  - train: 0.92154 \n",
      "var_164 Cum val: 0.89923  - train: 0.92269 \n",
      "var_165 Cum val: 0.90110  - train: 0.92429 \n",
      "var_166 Cum val: 0.90290  - train: 0.92583 \n",
      "var_167 Cum val: 0.90358  - train: 0.92644 \n",
      "var_168 Cum val: 0.90380  - train: 0.92673 \n",
      "var_169 Cum val: 0.90495  - train: 0.92767 \n",
      "var_170 Cum val: 0.90633  - train: 0.92881 \n",
      "var_171 Cum val: 0.90652  - train: 0.92904 \n",
      "var_172 Cum val: 0.90754  - train: 0.92993 \n",
      "var_173 Cum val: 0.90849  - train: 0.93075 \n",
      "var_174 Cum val: 0.91038  - train: 0.93223 \n",
      "var_175 Cum val: 0.91071  - train: 0.93260 \n",
      "var_176 Cum val: 0.91073  - train: 0.93271 \n",
      "var_177 Cum val: 0.91182  - train: 0.93364 \n",
      "var_178 Cum val: 0.91206  - train: 0.93394 \n",
      "var_179 Cum val: 0.91335  - train: 0.93493 \n",
      "var_180 Cum val: 0.91403  - train: 0.93550 \n",
      "var_181 Cum val: 0.91413  - train: 0.93568 \n",
      "var_182 Cum val: 0.91416  - train: 0.93578 \n",
      "var_183 Cum val: 0.91416  - train: 0.93578 \n",
      "var_184 Cum val: 0.91562  - train: 0.93697 \n",
      "var_185 Cum val: 0.91562  - train: 0.93697 \n",
      "var_186 Cum val: 0.91593  - train: 0.93728 \n",
      "var_187 Cum val: 0.91633  - train: 0.93775 \n",
      "var_188 Cum val: 0.91711  - train: 0.93844 \n",
      "var_189 Cum val: 0.91711  - train: 0.93854 \n",
      "var_190 Cum val: 0.91860  - train: 0.93974 \n",
      "var_191 Cum val: 0.91962  - train: 0.94063 \n",
      "var_192 Cum val: 0.92028  - train: 0.94122 \n",
      "var_193 Cum val: 0.92044  - train: 0.94146 \n",
      "var_194 Cum val: 0.92063  - train: 0.94169 \n",
      "var_195 Cum val: 0.92099  - train: 0.94205 \n",
      "var_196 Cum val: 0.92128  - train: 0.94231 \n",
      "var_197 Cum val: 0.92178  - train: 0.94285 \n",
      "var_198 Cum val: 0.92327  - train: 0.94396 \n",
      "var_199 Cum val: 0.92359  - train: 0.94436 \n"
     ]
    }
   ],
   "source": [
    "preds_oof_cum = np.zeros(preds_oof.shape[0])\n",
    "if use_experimental:\n",
    "    preds_test_cum = np.zeros(preds_test.shape[0])\n",
    "preds_train_cum = np.zeros(preds_train.shape[0])\n",
    "for i in range(len(features)):\n",
    "    preds_oof_cum += preds_oof[:,i]\n",
    "    preds_train_cum += preds_train[:,i]\n",
    "    print(\"var_{} Cum val: {:<8.5f}\".format(i,roc_auc_score(target_train, preds_oof_cum)), end=\"\")\n",
    "    if use_experimental:\n",
    "        preds_test_cum += preds_test[:,i]\n",
    "        print(\" - test : {:<8.5f}\".format(roc_auc_score(target_test, preds_test_cum)), end=\"\")\n",
    "    print(\" - train: {:<8.5f}\".format(roc_auc_score(target_train, preds_train_cum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(settings)\n",
    "print(settings_best_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA on predictors\n",
    "I plotted the predictions (sorted by feature), of the trees seaparately for the first 20 vars (x-axis corresponds to the z-score). The predictions are very noisy at the tails of the distributions, therefore I also tried using smoothed predictions (orange line) with no success however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "features_to_show = np.arange(20)\n",
    "plt.figure(figsize = (20,20))\n",
    "\n",
    "for i in features_to_show:\n",
    "    var = 'var_'+str(i)\n",
    "    signal = X_test[var].values\n",
    "    logits = preds_test[:,i]\n",
    "    func = interp1d(signal, logits)\n",
    "    space = np.linspace(signal.min(), signal.max(), 4000)\n",
    "    activations = func(space)\n",
    "    activations_smooth = gaussian_filter(activations, 10)\n",
    "    \n",
    "    func_smooth = interp1d(space, activations_smooth)\n",
    "    logits_smooth = func_smooth(signal)\n",
    "    plt.subplot(5,4,i+1)\n",
    "    plt.plot(space, activations)\n",
    "    plt.plot(space, activations_smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN \n",
    "The CNN model is the main reason for our high placement as we hit a wall using solely trees and couldn't improve upon 0.922 LB. At some point while playing around with the trees I noticed two things:\n",
    "1. Averaging the predictors might not be the best solution. At first I just averaged the predictors, then I observed that averaging the logits of predictors improved the final score massively, however i couldn't really explain why. Averaging logits here is equivalent with multiplying the probabilities p(t|x) which is not what happens in Naive Bayes, there you multiply p(x|t). I concluded that the boost comes from the concativity of the logarithm, which kind of translates to the predictors having lower confidence predicting t=1 and higher confidence predicting t=0, I further investigated concave functions and found the square root to work even better. \n",
    "\n",
    "2. The predictors are very noisy around the tails of the feature distributions and this pattern is likely related to the counts or the features themselves. Also the pattern seems to be similar across different features. \n",
    "\n",
    "Given these observations I tried using a CNN to learn these patterns. The original idea here was that it should learn when to trust the tree predictors given the feature and its counts. After reading some other top team solutions I think the CNN maybe picked up something else from the features.\n",
    "\n",
    "I choose the architecure in a way which would ensure feature independence up until the last dense layer. In order to minimize overfitting and utilize the similarity of patterns across different var_x I used convolutional layers. The convolutions are performed across different var_x and at any point the filters only have a single var and their respective features in their field of view. Batch normalization is a great regularizer here and very crucial for the success of the model. The model has a total of 2.8K trainable parameters which is sufficiently low to prevent overfitting. I verified this by splitting train data into train / test with `use_experimental = True` at the top of the kernel and using test AUC as a gauge. The final prediction is the average of the 7 CNNs trained on every fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1000, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 200, 32)           192       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 200, 32)           128       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 200, 24)           792       \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 200, 24)           96        \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 200, 16)           400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200, 16)           64        \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 200, 4)            68        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 800, 1)            0         \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 400, 1)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 401       \n",
      "=================================================================\n",
      "Total params: 3,741\n",
      "Trainable params: 2,797\n",
      "Non-trainable params: 944\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 171427 samples, validate on 28573 samples\n",
      "Epoch 1/60\n",
      " - 9s - loss: 0.2771 - acc: 0.8922 - val_loss: 0.2559 - val_acc: 0.9014\n",
      "Epoch 2/60\n",
      " - 3s - loss: 0.1922 - acc: 0.9289 - val_loss: 0.2911 - val_acc: 0.9033\n",
      "Epoch 3/60\n",
      " - 3s - loss: 0.1869 - acc: 0.9310 - val_loss: 0.3109 - val_acc: 0.9100\n",
      "Epoch 4/60\n",
      " - 3s - loss: 0.1851 - acc: 0.9314 - val_loss: 0.2384 - val_acc: 0.9265\n",
      "Epoch 5/60\n",
      " - 3s - loss: 0.1845 - acc: 0.9320 - val_loss: 0.2141 - val_acc: 0.9290\n",
      "Epoch 6/60\n",
      " - 3s - loss: 0.1843 - acc: 0.9316 - val_loss: 0.3621 - val_acc: 0.9074\n",
      "Epoch 7/60\n",
      " - 3s - loss: 0.1836 - acc: 0.9319 - val_loss: 0.3765 - val_acc: 0.9096\n",
      "Epoch 8/60\n",
      " - 3s - loss: 0.1827 - acc: 0.9322 - val_loss: 0.2357 - val_acc: 0.9256\n",
      "Epoch 9/60\n",
      " - 3s - loss: 0.1819 - acc: 0.9323 - val_loss: 0.2217 - val_acc: 0.9201\n",
      "Epoch 10/60\n",
      " - 3s - loss: 0.1815 - acc: 0.9328 - val_loss: 0.2155 - val_acc: 0.9286\n",
      "Epoch 11/60\n",
      " - 3s - loss: 0.1803 - acc: 0.9326 - val_loss: 0.1973 - val_acc: 0.9266\n",
      "Epoch 12/60\n",
      " - 3s - loss: 0.1815 - acc: 0.9328 - val_loss: 0.4875 - val_acc: 0.7886\n",
      "Epoch 13/60\n",
      " - 3s - loss: 0.1807 - acc: 0.9332 - val_loss: 0.2160 - val_acc: 0.9172\n",
      "Epoch 14/60\n",
      " - 3s - loss: 0.1795 - acc: 0.9338 - val_loss: 0.2402 - val_acc: 0.9045\n",
      "Epoch 15/60\n",
      " - 3s - loss: 0.1797 - acc: 0.9331 - val_loss: 0.2822 - val_acc: 0.8823\n",
      "Epoch 16/60\n",
      " - 3s - loss: 0.1798 - acc: 0.9336 - val_loss: 0.5674 - val_acc: 0.7561\n",
      "Epoch 17/60\n",
      " - 3s - loss: 0.1782 - acc: 0.9339 - val_loss: 0.3591 - val_acc: 0.8459\n",
      "Epoch 18/60\n",
      " - 3s - loss: 0.1787 - acc: 0.9337 - val_loss: 0.5828 - val_acc: 0.7581\n",
      "Epoch 19/60\n",
      " - 3s - loss: 0.1780 - acc: 0.9339 - val_loss: 0.1960 - val_acc: 0.9323\n",
      "Epoch 20/60\n",
      " - 3s - loss: 0.1784 - acc: 0.9337 - val_loss: 0.1856 - val_acc: 0.9336\n",
      "Epoch 21/60\n",
      " - 3s - loss: 0.1779 - acc: 0.9342 - val_loss: 0.3985 - val_acc: 0.8252\n",
      "Epoch 22/60\n",
      " - 3s - loss: 0.1779 - acc: 0.9338 - val_loss: 0.1849 - val_acc: 0.9342\n",
      "Epoch 23/60\n",
      " - 3s - loss: 0.1775 - acc: 0.9343 - val_loss: 0.7120 - val_acc: 0.9005\n",
      "Epoch 24/60\n",
      " - 3s - loss: 0.1772 - acc: 0.9342 - val_loss: 0.6387 - val_acc: 0.9007\n",
      "Epoch 25/60\n",
      " - 3s - loss: 0.1776 - acc: 0.9342 - val_loss: 1.3708 - val_acc: 0.4342\n",
      "Epoch 26/60\n",
      " - 3s - loss: 0.1768 - acc: 0.9345 - val_loss: 0.7301 - val_acc: 0.6660\n",
      "Epoch 27/60\n",
      " - 3s - loss: 0.1764 - acc: 0.9345 - val_loss: 0.3919 - val_acc: 0.9077\n",
      "Epoch 28/60\n",
      " - 3s - loss: 0.1771 - acc: 0.9343 - val_loss: 0.7925 - val_acc: 0.6477\n",
      "Epoch 29/60\n",
      " - 3s - loss: 0.1768 - acc: 0.9345 - val_loss: 0.2179 - val_acc: 0.9264\n",
      "Epoch 30/60\n",
      " - 3s - loss: 0.1763 - acc: 0.9348 - val_loss: 1.4290 - val_acc: 0.4238\n",
      "Epoch 31/60\n",
      " - 3s - loss: 0.1762 - acc: 0.9346 - val_loss: 0.2540 - val_acc: 0.9211\n",
      "Epoch 32/60\n",
      " - 3s - loss: 0.1766 - acc: 0.9344 - val_loss: 0.8293 - val_acc: 0.6274\n",
      "Epoch 33/60\n",
      " - 3s - loss: 0.1760 - acc: 0.9350 - val_loss: 0.3577 - val_acc: 0.8472\n",
      "Epoch 34/60\n",
      " - 3s - loss: 0.1771 - acc: 0.9345 - val_loss: 0.2236 - val_acc: 0.9274\n",
      "Epoch 35/60\n",
      " - 3s - loss: 0.1763 - acc: 0.9349 - val_loss: 0.4073 - val_acc: 0.9066\n",
      "Epoch 36/60\n",
      " - 3s - loss: 0.1770 - acc: 0.9347 - val_loss: 1.0677 - val_acc: 0.5187\n",
      "Epoch 37/60\n",
      " - 3s - loss: 0.1764 - acc: 0.9351 - val_loss: 0.2073 - val_acc: 0.9324\n",
      "Epoch 38/60\n",
      " - 3s - loss: 0.1764 - acc: 0.9343 - val_loss: 0.2315 - val_acc: 0.9083\n",
      "Epoch 39/60\n",
      " - 3s - loss: 0.1762 - acc: 0.9347 - val_loss: 0.1948 - val_acc: 0.9319\n",
      "Epoch 40/60\n",
      " - 3s - loss: 0.1766 - acc: 0.9346 - val_loss: 1.0904 - val_acc: 0.5469\n",
      "Epoch 41/60\n",
      " - 3s - loss: 0.1761 - acc: 0.9349 - val_loss: 3.0258 - val_acc: 0.1775\n",
      "Epoch 42/60\n",
      " - 3s - loss: 0.1768 - acc: 0.9346 - val_loss: 0.2208 - val_acc: 0.9131\n",
      "Epoch 43/60\n",
      " - 3s - loss: 0.1761 - acc: 0.9344 - val_loss: 0.2116 - val_acc: 0.9311\n",
      "Epoch 44/60\n",
      " - 3s - loss: 0.1759 - acc: 0.9345 - val_loss: 0.2134 - val_acc: 0.9313\n",
      "Epoch 45/60\n",
      " - 3s - loss: 0.1757 - acc: 0.9350 - val_loss: 0.2117 - val_acc: 0.9165\n",
      "Epoch 46/60\n",
      " - 3s - loss: 0.1750 - acc: 0.9351 - val_loss: 0.3984 - val_acc: 0.9076\n",
      "Epoch 47/60\n",
      " - 3s - loss: 0.1755 - acc: 0.9349 - val_loss: 0.2192 - val_acc: 0.9269\n",
      "Epoch 48/60\n",
      " - 3s - loss: 0.1760 - acc: 0.9348 - val_loss: 0.3980 - val_acc: 0.8263\n",
      "Epoch 49/60\n",
      " - 3s - loss: 0.1756 - acc: 0.9347 - val_loss: 0.6614 - val_acc: 0.9004\n",
      "Epoch 50/60\n",
      " - 3s - loss: 0.1728 - acc: 0.9362 - val_loss: 0.3747 - val_acc: 0.9073\n",
      "Epoch 51/60\n",
      " - 3s - loss: 0.1716 - acc: 0.9365 - val_loss: 0.2124 - val_acc: 0.9268\n",
      "Epoch 52/60\n",
      " - 3s - loss: 0.1715 - acc: 0.9368 - val_loss: 0.1857 - val_acc: 0.9335\n",
      "Epoch 53/60\n",
      " - 3s - loss: 0.1713 - acc: 0.9365 - val_loss: 0.1936 - val_acc: 0.9317\n",
      "Epoch 54/60\n",
      " - 3s - loss: 0.1713 - acc: 0.9364 - val_loss: 0.1792 - val_acc: 0.9349\n",
      "Epoch 55/60\n",
      " - 3s - loss: 0.1713 - acc: 0.9367 - val_loss: 0.1814 - val_acc: 0.9346\n",
      "Epoch 56/60\n",
      " - 3s - loss: 0.1711 - acc: 0.9368 - val_loss: 0.1851 - val_acc: 0.9339\n",
      "Epoch 57/60\n",
      " - 3s - loss: 0.1710 - acc: 0.9367 - val_loss: 0.1832 - val_acc: 0.9340\n",
      "Epoch 58/60\n",
      " - 3s - loss: 0.1712 - acc: 0.9365 - val_loss: 0.1992 - val_acc: 0.9304\n",
      "Epoch 59/60\n",
      " - 3s - loss: 0.1712 - acc: 0.9365 - val_loss: 0.1935 - val_acc: 0.9324\n",
      "Epoch 60/60\n",
      " - 3s - loss: 0.1712 - acc: 0.9365 - val_loss: 0.1790 - val_acc: 0.9348\n",
      "0.9284009617451867\n",
      "Train on 171428 samples, validate on 28572 samples\n",
      "Epoch 1/60\n",
      " - 5s - loss: 0.2727 - acc: 0.8924 - val_loss: 0.2726 - val_acc: 0.8955\n",
      "Epoch 2/60\n",
      " - 3s - loss: 0.1904 - acc: 0.9295 - val_loss: 0.3067 - val_acc: 0.9016\n",
      "Epoch 3/60\n",
      " - 3s - loss: 0.1822 - acc: 0.9334 - val_loss: 0.4239 - val_acc: 0.8202\n",
      "Epoch 4/60\n",
      " - 3s - loss: 0.1817 - acc: 0.9336 - val_loss: 2.6787 - val_acc: 0.1385\n",
      "Epoch 5/60\n",
      " - 3s - loss: 0.1808 - acc: 0.9338 - val_loss: 0.3827 - val_acc: 0.8351\n",
      "Epoch 6/60\n",
      " - 3s - loss: 0.1800 - acc: 0.9337 - val_loss: 2.2112 - val_acc: 0.1719\n",
      "Epoch 7/60\n",
      " - 3s - loss: 0.1797 - acc: 0.9337 - val_loss: 5.9583 - val_acc: 0.1006\n",
      "Epoch 8/60\n",
      " - 3s - loss: 0.1793 - acc: 0.9340 - val_loss: 4.5107 - val_acc: 0.1009\n",
      "Epoch 9/60\n",
      " - 3s - loss: 0.1781 - acc: 0.9343 - val_loss: 10.7691 - val_acc: 0.1005\n",
      "Epoch 10/60\n",
      " - 3s - loss: 0.1788 - acc: 0.9343 - val_loss: 14.3399 - val_acc: 0.1005\n",
      "Epoch 11/60\n",
      " - 3s - loss: 0.1799 - acc: 0.9340 - val_loss: 14.3404 - val_acc: 0.1005\n",
      "Epoch 12/60\n",
      " - 3s - loss: 0.1769 - acc: 0.9346 - val_loss: 8.9268 - val_acc: 0.1005\n",
      "Epoch 13/60\n",
      " - 3s - loss: 0.1769 - acc: 0.9346 - val_loss: 3.4918 - val_acc: 0.1085\n",
      "Epoch 14/60\n",
      " - 3s - loss: 0.1772 - acc: 0.9344 - val_loss: 2.1130 - val_acc: 0.2435\n",
      "Epoch 15/60\n",
      " - 3s - loss: 0.1769 - acc: 0.9346 - val_loss: 0.2444 - val_acc: 0.9244\n",
      "Epoch 16/60\n",
      " - 3s - loss: 0.1772 - acc: 0.9346 - val_loss: 0.9576 - val_acc: 0.8997\n",
      "Epoch 17/60\n",
      " - 3s - loss: 0.1758 - acc: 0.9349 - val_loss: 0.2183 - val_acc: 0.9280\n",
      "Epoch 18/60\n",
      " - 3s - loss: 0.1762 - acc: 0.9351 - val_loss: 1.9155 - val_acc: 0.2838\n",
      "Epoch 19/60\n",
      " - 3s - loss: 0.1774 - acc: 0.9345 - val_loss: 14.3404 - val_acc: 0.1005\n",
      "Epoch 20/60\n",
      " - 3s - loss: 0.1771 - acc: 0.9342 - val_loss: 14.3404 - val_acc: 0.1005\n",
      "Epoch 21/60\n",
      " - 3s - loss: 0.1774 - acc: 0.9344 - val_loss: 14.3404 - val_acc: 0.1005\n",
      "Epoch 22/60\n",
      " - 3s - loss: 0.1764 - acc: 0.9350 - val_loss: 11.3487 - val_acc: 0.1005\n",
      "Epoch 23/60\n",
      " - 3s - loss: 0.1760 - acc: 0.9352 - val_loss: 3.0358 - val_acc: 0.1428\n",
      "Epoch 24/60\n",
      " - 3s - loss: 0.1766 - acc: 0.9350 - val_loss: 0.4867 - val_acc: 0.9036\n",
      "Epoch 25/60\n",
      " - 3s - loss: 0.1766 - acc: 0.9347 - val_loss: 3.2247 - val_acc: 0.1385\n",
      "Epoch 26/60\n",
      " - 3s - loss: 0.1764 - acc: 0.9349 - val_loss: 0.3459 - val_acc: 0.9146\n",
      "Epoch 27/60\n",
      " - 3s - loss: 0.1756 - acc: 0.9352 - val_loss: 2.3740 - val_acc: 0.1781\n",
      "Epoch 28/60\n",
      " - 3s - loss: 0.1762 - acc: 0.9353 - val_loss: 0.7207 - val_acc: 0.6692\n",
      "Epoch 29/60\n",
      " - 3s - loss: 0.1761 - acc: 0.9350 - val_loss: 0.2963 - val_acc: 0.9147\n",
      "Epoch 30/60\n",
      " - 3s - loss: 0.1758 - acc: 0.9354 - val_loss: 0.2222 - val_acc: 0.9125\n",
      "Epoch 31/60\n",
      " - 3s - loss: 0.1757 - acc: 0.9351 - val_loss: 0.2391 - val_acc: 0.9217\n",
      "Epoch 32/60\n",
      " - 3s - loss: 0.1760 - acc: 0.9351 - val_loss: 0.3058 - val_acc: 0.8755\n",
      "Epoch 33/60\n",
      " - 3s - loss: 0.1755 - acc: 0.9351 - val_loss: 0.5771 - val_acc: 0.7363\n",
      "Epoch 34/60\n",
      " - 3s - loss: 0.1757 - acc: 0.9352 - val_loss: 0.9137 - val_acc: 0.5659\n",
      "Epoch 35/60\n",
      " - 3s - loss: 0.1757 - acc: 0.9351 - val_loss: 0.1868 - val_acc: 0.9286\n",
      "Epoch 36/60\n",
      " - 3s - loss: 0.1756 - acc: 0.9352 - val_loss: 0.2497 - val_acc: 0.9234\n",
      "Epoch 37/60\n",
      " - 3s - loss: 0.1751 - acc: 0.9353 - val_loss: 0.1882 - val_acc: 0.9290\n",
      "Epoch 38/60\n",
      " - 3s - loss: 0.1752 - acc: 0.9355 - val_loss: 0.2953 - val_acc: 0.8803\n",
      "Epoch 39/60\n",
      " - 3s - loss: 0.1758 - acc: 0.9350 - val_loss: 0.2488 - val_acc: 0.9030\n",
      "Epoch 40/60\n",
      " - 3s - loss: 0.1757 - acc: 0.9352 - val_loss: 0.2284 - val_acc: 0.9263\n",
      "Epoch 41/60\n",
      " - 3s - loss: 0.1757 - acc: 0.9351 - val_loss: 0.4021 - val_acc: 0.8255\n",
      "Epoch 42/60\n",
      " - 3s - loss: 0.1748 - acc: 0.9355 - val_loss: 0.3929 - val_acc: 0.8310\n",
      "Epoch 43/60\n",
      " - 3s - loss: 0.1752 - acc: 0.9356 - val_loss: 0.3803 - val_acc: 0.9106\n",
      "Epoch 44/60\n",
      " - 3s - loss: 0.1749 - acc: 0.9358 - val_loss: 0.2192 - val_acc: 0.9247\n",
      "Epoch 45/60\n",
      " - 3s - loss: 0.1749 - acc: 0.9361 - val_loss: 0.2142 - val_acc: 0.9277\n",
      "Epoch 46/60\n",
      " - 3s - loss: 0.1760 - acc: 0.9352 - val_loss: 1.2505 - val_acc: 0.4420\n",
      "Epoch 47/60\n",
      " - 3s - loss: 0.1753 - acc: 0.9352 - val_loss: 0.5645 - val_acc: 0.7409\n",
      "Epoch 48/60\n",
      " - 3s - loss: 0.1748 - acc: 0.9357 - val_loss: 0.3371 - val_acc: 0.8597\n",
      "Epoch 49/60\n",
      " - 3s - loss: 0.1747 - acc: 0.9357 - val_loss: 0.2511 - val_acc: 0.9194\n",
      "Epoch 50/60\n",
      " - 3s - loss: 0.1727 - acc: 0.9362 - val_loss: 0.1785 - val_acc: 0.9332\n",
      "Epoch 51/60\n",
      " - 3s - loss: 0.1712 - acc: 0.9366 - val_loss: 0.2111 - val_acc: 0.9190\n",
      "Epoch 52/60\n",
      " - 3s - loss: 0.1709 - acc: 0.9367 - val_loss: 0.2264 - val_acc: 0.9123\n",
      "Epoch 53/60\n",
      " - 3s - loss: 0.1709 - acc: 0.9369 - val_loss: 0.1974 - val_acc: 0.9253\n",
      "Epoch 54/60\n",
      " - 3s - loss: 0.1710 - acc: 0.9370 - val_loss: 0.1980 - val_acc: 0.9250\n",
      "Epoch 55/60\n",
      " - 3s - loss: 0.1708 - acc: 0.9369 - val_loss: 0.3233 - val_acc: 0.8665\n",
      "Epoch 56/60\n",
      " - 3s - loss: 0.1709 - acc: 0.9369 - val_loss: 0.2361 - val_acc: 0.9083\n",
      "Epoch 57/60\n",
      " - 3s - loss: 0.1709 - acc: 0.9369 - val_loss: 0.2350 - val_acc: 0.9090\n",
      "Epoch 58/60\n",
      " - 3s - loss: 0.1708 - acc: 0.9370 - val_loss: 0.2050 - val_acc: 0.9221\n",
      "Epoch 59/60\n",
      " - 3s - loss: 0.1709 - acc: 0.9372 - val_loss: 0.2082 - val_acc: 0.9208\n",
      "Epoch 60/60\n",
      " - 3s - loss: 0.1708 - acc: 0.9371 - val_loss: 0.2666 - val_acc: 0.8934\n",
      "0.928936408367241\n",
      "Train on 171429 samples, validate on 28571 samples\n",
      "Epoch 1/60\n",
      " - 6s - loss: 0.2684 - acc: 0.8963 - val_loss: 0.4516 - val_acc: 0.8995\n",
      "Epoch 2/60\n",
      " - 3s - loss: 0.1898 - acc: 0.9300 - val_loss: 0.2369 - val_acc: 0.9200\n",
      "Epoch 3/60\n",
      " - 3s - loss: 0.1875 - acc: 0.9306 - val_loss: 0.2088 - val_acc: 0.9263\n",
      "Epoch 4/60\n",
      " - 3s - loss: 0.1853 - acc: 0.9315 - val_loss: 0.2258 - val_acc: 0.9258\n",
      "Epoch 5/60\n",
      " - 3s - loss: 0.1840 - acc: 0.9324 - val_loss: 0.3056 - val_acc: 0.8754\n",
      "Epoch 6/60\n",
      " - 3s - loss: 0.1834 - acc: 0.9324 - val_loss: 0.1988 - val_acc: 0.9301\n",
      "Epoch 7/60\n",
      " - 3s - loss: 0.1835 - acc: 0.9320 - val_loss: 0.1969 - val_acc: 0.9290\n",
      "Epoch 8/60\n",
      " - 3s - loss: 0.1833 - acc: 0.9323 - val_loss: 0.2550 - val_acc: 0.8994\n",
      "Epoch 9/60\n",
      " - 3s - loss: 0.1817 - acc: 0.9331 - val_loss: 0.2470 - val_acc: 0.9036\n",
      "Epoch 10/60\n",
      " - 3s - loss: 0.1815 - acc: 0.9333 - val_loss: 0.2020 - val_acc: 0.9303\n",
      "Epoch 11/60\n",
      " - 3s - loss: 0.1812 - acc: 0.9331 - val_loss: 0.1994 - val_acc: 0.9273\n",
      "Epoch 12/60\n",
      " - 3s - loss: 0.1800 - acc: 0.9335 - val_loss: 0.1998 - val_acc: 0.9299\n",
      "Epoch 13/60\n",
      " - 3s - loss: 0.1818 - acc: 0.9328 - val_loss: 0.2004 - val_acc: 0.9243\n",
      "Epoch 14/60\n",
      " - 3s - loss: 0.1805 - acc: 0.9332 - val_loss: 0.1924 - val_acc: 0.9292\n",
      "Epoch 15/60\n",
      " - 3s - loss: 0.1803 - acc: 0.9335 - val_loss: 0.2345 - val_acc: 0.9082\n",
      "Epoch 16/60\n",
      " - 3s - loss: 0.1796 - acc: 0.9337 - val_loss: 0.1929 - val_acc: 0.9288\n",
      "Epoch 17/60\n",
      " - 3s - loss: 0.1799 - acc: 0.9338 - val_loss: 0.1906 - val_acc: 0.9291\n",
      "Epoch 18/60\n",
      " - 3s - loss: 0.1801 - acc: 0.9334 - val_loss: 0.2487 - val_acc: 0.9015\n",
      "Epoch 19/60\n",
      " - 3s - loss: 0.1797 - acc: 0.9337 - val_loss: 0.1972 - val_acc: 0.9284\n",
      "Epoch 20/60\n",
      " - 3s - loss: 0.1796 - acc: 0.9335 - val_loss: 0.1963 - val_acc: 0.9270\n",
      "Epoch 21/60\n",
      " - 3s - loss: 0.1799 - acc: 0.9337 - val_loss: 0.6185 - val_acc: 0.7377\n",
      "Epoch 22/60\n",
      " - 3s - loss: 0.1794 - acc: 0.9339 - val_loss: 0.3467 - val_acc: 0.8535\n",
      "Epoch 23/60\n",
      " - 3s - loss: 0.1787 - acc: 0.9337 - val_loss: 0.4094 - val_acc: 0.8368\n",
      "Epoch 24/60\n",
      " - 3s - loss: 0.1787 - acc: 0.9337 - val_loss: 0.2140 - val_acc: 0.9241\n",
      "Epoch 25/60\n",
      " - 3s - loss: 0.1777 - acc: 0.9341 - val_loss: 0.2998 - val_acc: 0.9193\n",
      "Epoch 26/60\n",
      " - 3s - loss: 0.1782 - acc: 0.9339 - val_loss: 0.2335 - val_acc: 0.9234\n",
      "Epoch 27/60\n",
      " - 3s - loss: 0.1782 - acc: 0.9341 - val_loss: 0.6179 - val_acc: 0.9008\n",
      "Epoch 28/60\n",
      " - 3s - loss: 0.1780 - acc: 0.9344 - val_loss: 0.2700 - val_acc: 0.9233\n",
      "Epoch 29/60\n",
      " - 3s - loss: 0.1765 - acc: 0.9342 - val_loss: 0.2195 - val_acc: 0.9283\n",
      "Epoch 30/60\n",
      " - 3s - loss: 0.1770 - acc: 0.9345 - val_loss: 0.1881 - val_acc: 0.9318\n",
      "Epoch 31/60\n",
      " - 3s - loss: 0.1775 - acc: 0.9339 - val_loss: 0.2269 - val_acc: 0.9243\n",
      "Epoch 32/60\n",
      " - 3s - loss: 0.1775 - acc: 0.9342 - val_loss: 0.1851 - val_acc: 0.9319\n",
      "Epoch 33/60\n",
      " - 3s - loss: 0.1780 - acc: 0.9338 - val_loss: 0.2061 - val_acc: 0.9299\n",
      "Epoch 34/60\n",
      " - 3s - loss: 0.1773 - acc: 0.9343 - val_loss: 0.1845 - val_acc: 0.9326\n",
      "Epoch 35/60\n",
      " - 3s - loss: 0.1772 - acc: 0.9347 - val_loss: 0.5145 - val_acc: 0.9032\n",
      "Epoch 36/60\n",
      " - 3s - loss: 0.1764 - acc: 0.9347 - val_loss: 0.3816 - val_acc: 0.9095\n",
      "Epoch 37/60\n",
      " - 3s - loss: 0.1767 - acc: 0.9347 - val_loss: 0.4850 - val_acc: 0.7815\n",
      "Epoch 38/60\n",
      " - 3s - loss: 0.1774 - acc: 0.9340 - val_loss: 0.2542 - val_acc: 0.9189\n",
      "Epoch 39/60\n",
      " - 3s - loss: 0.1772 - acc: 0.9346 - val_loss: 0.2999 - val_acc: 0.8762\n",
      "Epoch 40/60\n",
      " - 3s - loss: 0.1767 - acc: 0.9343 - val_loss: 0.3063 - val_acc: 0.9106\n",
      "Epoch 41/60\n",
      " - 3s - loss: 0.1758 - acc: 0.9346 - val_loss: 0.2044 - val_acc: 0.9278\n",
      "Epoch 42/60\n",
      " - 3s - loss: 0.1763 - acc: 0.9348 - val_loss: 0.4873 - val_acc: 0.9036\n",
      "Epoch 43/60\n",
      " - 3s - loss: 0.1759 - acc: 0.9350 - val_loss: 0.5307 - val_acc: 0.9022\n",
      "Epoch 44/60\n",
      " - 3s - loss: 0.1765 - acc: 0.9346 - val_loss: 0.2839 - val_acc: 0.9210\n",
      "Epoch 45/60\n",
      " - 3s - loss: 0.1768 - acc: 0.9346 - val_loss: 0.5032 - val_acc: 0.9041\n",
      "Epoch 46/60\n",
      " - 3s - loss: 0.1766 - acc: 0.9343 - val_loss: 0.2803 - val_acc: 0.9168\n",
      "Epoch 47/60\n",
      " - 3s - loss: 0.1759 - acc: 0.9350 - val_loss: 0.2486 - val_acc: 0.9190\n",
      "Epoch 48/60\n",
      " - 3s - loss: 0.1762 - acc: 0.9347 - val_loss: 0.5803 - val_acc: 0.9016\n",
      "Epoch 49/60\n",
      " - 3s - loss: 0.1757 - acc: 0.9353 - val_loss: 0.5004 - val_acc: 0.9029\n",
      "Epoch 50/60\n",
      " - 3s - loss: 0.1732 - acc: 0.9358 - val_loss: 0.2590 - val_acc: 0.9181\n",
      "Epoch 51/60\n",
      " - 3s - loss: 0.1718 - acc: 0.9365 - val_loss: 0.2393 - val_acc: 0.9197\n",
      "Epoch 52/60\n",
      " - 3s - loss: 0.1717 - acc: 0.9363 - val_loss: 0.1806 - val_acc: 0.9339\n",
      "Epoch 53/60\n",
      " - 3s - loss: 0.1716 - acc: 0.9364 - val_loss: 0.1805 - val_acc: 0.9338\n",
      "Epoch 54/60\n",
      " - 3s - loss: 0.1717 - acc: 0.9363 - val_loss: 0.1762 - val_acc: 0.9357\n",
      "Epoch 55/60\n",
      " - 3s - loss: 0.1715 - acc: 0.9365 - val_loss: 0.1761 - val_acc: 0.9358\n",
      "Epoch 56/60\n",
      " - 3s - loss: 0.1714 - acc: 0.9366 - val_loss: 0.1788 - val_acc: 0.9350\n",
      "Epoch 57/60\n",
      " - 3s - loss: 0.1715 - acc: 0.9364 - val_loss: 0.1784 - val_acc: 0.9351\n",
      "Epoch 58/60\n",
      " - 3s - loss: 0.1714 - acc: 0.9363 - val_loss: 0.1852 - val_acc: 0.9327\n",
      "Epoch 59/60\n",
      " - 3s - loss: 0.1715 - acc: 0.9365 - val_loss: 0.2245 - val_acc: 0.9139\n",
      "Epoch 60/60\n",
      " - 3s - loss: 0.1715 - acc: 0.9365 - val_loss: 0.1896 - val_acc: 0.9297\n",
      "0.9291399256736896\n",
      "Train on 171429 samples, validate on 28571 samples\n",
      "Epoch 1/60\n",
      " - 5s - loss: 0.2747 - acc: 0.8930 - val_loss: 1.3318 - val_acc: 0.3797\n",
      "Epoch 2/60\n",
      " - 3s - loss: 0.1941 - acc: 0.9286 - val_loss: 7.4327 - val_acc: 0.1009\n",
      "Epoch 3/60\n",
      " - 3s - loss: 0.1846 - acc: 0.9321 - val_loss: 12.2459 - val_acc: 0.1005\n",
      "Epoch 4/60\n",
      " - 3s - loss: 0.1826 - acc: 0.9327 - val_loss: 14.2694 - val_acc: 0.1005\n",
      "Epoch 5/60\n",
      " - 3s - loss: 0.1803 - acc: 0.9336 - val_loss: 14.3404 - val_acc: 0.1005\n",
      "Epoch 6/60\n",
      " - 3s - loss: 0.1798 - acc: 0.9336 - val_loss: 14.3404 - val_acc: 0.1005\n",
      "Epoch 7/60\n",
      " - 3s - loss: 0.1799 - acc: 0.9337 - val_loss: 14.3404 - val_acc: 0.1005\n",
      "Epoch 8/60\n",
      " - 3s - loss: 0.1782 - acc: 0.9339 - val_loss: 14.3404 - val_acc: 0.1005\n",
      "Epoch 9/60\n",
      " - 3s - loss: 0.1775 - acc: 0.9344 - val_loss: 13.6268 - val_acc: 0.1005\n",
      "Epoch 10/60\n",
      " - 3s - loss: 0.1795 - acc: 0.9335 - val_loss: 10.4073 - val_acc: 0.1005\n",
      "Epoch 11/60\n",
      " - 3s - loss: 0.1778 - acc: 0.9346 - val_loss: 10.9398 - val_acc: 0.1005\n",
      "Epoch 12/60\n",
      " - 3s - loss: 0.1779 - acc: 0.9341 - val_loss: 3.4934 - val_acc: 0.1329\n",
      "Epoch 13/60\n",
      " - 3s - loss: 0.1778 - acc: 0.9343 - val_loss: 12.2721 - val_acc: 0.1005\n",
      "Epoch 14/60\n",
      " - 3s - loss: 0.1782 - acc: 0.9340 - val_loss: 12.3735 - val_acc: 0.1005\n",
      "Epoch 15/60\n",
      " - 3s - loss: 0.1776 - acc: 0.9344 - val_loss: 0.2165 - val_acc: 0.9170\n",
      "Epoch 16/60\n",
      " - 3s - loss: 0.1766 - acc: 0.9349 - val_loss: 10.9198 - val_acc: 0.1005\n",
      "Epoch 17/60\n",
      " - 3s - loss: 0.1768 - acc: 0.9347 - val_loss: 2.8819 - val_acc: 0.1478\n",
      "Epoch 18/60\n",
      " - 3s - loss: 0.1769 - acc: 0.9344 - val_loss: 0.7932 - val_acc: 0.6444\n",
      "Epoch 19/60\n",
      " - 3s - loss: 0.1775 - acc: 0.9339 - val_loss: 0.1787 - val_acc: 0.9342\n",
      "Epoch 20/60\n",
      " - 3s - loss: 0.1771 - acc: 0.9345 - val_loss: 5.0615 - val_acc: 0.1036\n",
      "Epoch 21/60\n",
      " - 3s - loss: 0.1762 - acc: 0.9350 - val_loss: 0.3939 - val_acc: 0.8334\n",
      "Epoch 22/60\n",
      " - 3s - loss: 0.1761 - acc: 0.9348 - val_loss: 3.6649 - val_acc: 0.1068\n",
      "Epoch 23/60\n",
      " - 3s - loss: 0.1768 - acc: 0.9348 - val_loss: 13.0265 - val_acc: 0.1005\n",
      "Epoch 24/60\n",
      " - 3s - loss: 0.1767 - acc: 0.9346 - val_loss: 4.0758 - val_acc: 0.1034\n",
      "Epoch 25/60\n",
      " - 3s - loss: 0.1763 - acc: 0.9351 - val_loss: 3.0835 - val_acc: 0.1228\n",
      "Epoch 26/60\n",
      " - 3s - loss: 0.1763 - acc: 0.9348 - val_loss: 7.8479 - val_acc: 0.1005\n",
      "Epoch 27/60\n",
      " - 3s - loss: 0.1763 - acc: 0.9347 - val_loss: 0.3562 - val_acc: 0.9102\n",
      "Epoch 28/60\n",
      " - 3s - loss: 0.1760 - acc: 0.9348 - val_loss: 5.5281 - val_acc: 0.1007\n",
      "Epoch 29/60\n",
      " - 3s - loss: 0.1766 - acc: 0.9348 - val_loss: 0.2464 - val_acc: 0.9021\n",
      "Epoch 30/60\n",
      " - 3s - loss: 0.1774 - acc: 0.9341 - val_loss: 3.6540 - val_acc: 0.1331\n",
      "Epoch 31/60\n",
      " - 3s - loss: 0.1759 - acc: 0.9346 - val_loss: 8.2315 - val_acc: 0.1005\n",
      "Epoch 32/60\n",
      " - 3s - loss: 0.1764 - acc: 0.9348 - val_loss: 0.6344 - val_acc: 0.9006\n",
      "Epoch 33/60\n",
      " - 3s - loss: 0.1765 - acc: 0.9347 - val_loss: 0.5239 - val_acc: 0.7642\n",
      "Epoch 34/60\n",
      " - 3s - loss: 0.1760 - acc: 0.9350 - val_loss: 0.1787 - val_acc: 0.9337\n",
      "Epoch 35/60\n",
      " - 3s - loss: 0.1757 - acc: 0.9352 - val_loss: 0.2554 - val_acc: 0.8985\n",
      "Epoch 36/60\n",
      " - 3s - loss: 0.1762 - acc: 0.9350 - val_loss: 0.4526 - val_acc: 0.8008\n",
      "Epoch 37/60\n",
      " - 3s - loss: 0.1762 - acc: 0.9349 - val_loss: 0.3116 - val_acc: 0.8701\n",
      "Epoch 38/60\n",
      " - 3s - loss: 0.1759 - acc: 0.9353 - val_loss: 1.0203 - val_acc: 0.5527\n",
      "Epoch 39/60\n",
      " - 3s - loss: 0.1761 - acc: 0.9353 - val_loss: 2.6181 - val_acc: 0.1777\n",
      "Epoch 40/60\n",
      " - 3s - loss: 0.1759 - acc: 0.9350 - val_loss: 0.4894 - val_acc: 0.9039\n",
      "Epoch 41/60\n",
      " - 3s - loss: 0.1758 - acc: 0.9347 - val_loss: 2.8897 - val_acc: 0.1334\n",
      "Epoch 42/60\n",
      " - 3s - loss: 0.1756 - acc: 0.9355 - val_loss: 0.4735 - val_acc: 0.9054\n",
      "Epoch 43/60\n",
      " - 3s - loss: 0.1757 - acc: 0.9354 - val_loss: 0.1770 - val_acc: 0.9347\n",
      "Epoch 44/60\n",
      " - 3s - loss: 0.1774 - acc: 0.9344 - val_loss: 14.3404 - val_acc: 0.1005\n",
      "Epoch 45/60\n",
      " - 3s - loss: 0.1778 - acc: 0.9343 - val_loss: 14.3404 - val_acc: 0.1005\n",
      "Epoch 46/60\n",
      " - 3s - loss: 0.1776 - acc: 0.9343 - val_loss: 14.3404 - val_acc: 0.1005\n",
      "Epoch 47/60\n",
      " - 3s - loss: 0.1769 - acc: 0.9348 - val_loss: 11.6471 - val_acc: 0.1005\n",
      "Epoch 48/60\n",
      " - 3s - loss: 0.1768 - acc: 0.9347 - val_loss: 9.2428 - val_acc: 0.1005\n",
      "Epoch 49/60\n",
      " - 3s - loss: 0.1764 - acc: 0.9345 - val_loss: 6.4584 - val_acc: 0.1005\n",
      "Epoch 50/60\n",
      " - 3s - loss: 0.1740 - acc: 0.9356 - val_loss: 1.9488 - val_acc: 0.2596\n",
      "Epoch 51/60\n",
      " - 3s - loss: 0.1725 - acc: 0.9362 - val_loss: 1.3675 - val_acc: 0.3920\n",
      "Epoch 52/60\n",
      " - 3s - loss: 0.1723 - acc: 0.9365 - val_loss: 0.9747 - val_acc: 0.5357\n",
      "Epoch 53/60\n",
      " - 3s - loss: 0.1721 - acc: 0.9362 - val_loss: 0.3577 - val_acc: 0.8469\n",
      "Epoch 54/60\n",
      " - 3s - loss: 0.1722 - acc: 0.9365 - val_loss: 0.5057 - val_acc: 0.7660\n",
      "Epoch 55/60\n",
      " - 3s - loss: 0.1721 - acc: 0.9364 - val_loss: 0.3061 - val_acc: 0.8742\n",
      "Epoch 56/60\n",
      " - 3s - loss: 0.1720 - acc: 0.9365 - val_loss: 0.5021 - val_acc: 0.7691\n",
      "Epoch 57/60\n",
      " - 3s - loss: 0.1721 - acc: 0.9365 - val_loss: 0.1924 - val_acc: 0.9296\n",
      "Epoch 58/60\n",
      " - 3s - loss: 0.1720 - acc: 0.9363 - val_loss: 0.2270 - val_acc: 0.9140\n",
      "Epoch 59/60\n",
      " - 3s - loss: 0.1720 - acc: 0.9365 - val_loss: 0.1884 - val_acc: 0.9303\n",
      "Epoch 60/60\n",
      " - 3s - loss: 0.1719 - acc: 0.9364 - val_loss: 0.2043 - val_acc: 0.9244\n",
      "0.9292416843960574\n",
      "Train on 171429 samples, validate on 28571 samples\n",
      "Epoch 1/60\n",
      " - 6s - loss: 0.2725 - acc: 0.8929 - val_loss: 2.6599 - val_acc: 0.1105\n",
      "Epoch 2/60\n",
      " - 3s - loss: 0.1911 - acc: 0.9302 - val_loss: 1.5467 - val_acc: 0.1778\n",
      "Epoch 3/60\n",
      " - 3s - loss: 0.1844 - acc: 0.9319 - val_loss: 0.1969 - val_acc: 0.9273\n",
      "Epoch 4/60\n",
      " - 3s - loss: 0.1834 - acc: 0.9325 - val_loss: 1.0338 - val_acc: 0.4966\n",
      "Epoch 5/60\n",
      " - 3s - loss: 0.1831 - acc: 0.9327 - val_loss: 1.5885 - val_acc: 0.3835\n",
      "Epoch 6/60\n",
      " - 3s - loss: 0.1826 - acc: 0.9325 - val_loss: 3.0478 - val_acc: 0.1399\n",
      "Epoch 7/60\n",
      " - 3s - loss: 0.1799 - acc: 0.9340 - val_loss: 7.9364 - val_acc: 0.1005\n",
      "Epoch 8/60\n",
      " - 3s - loss: 0.1783 - acc: 0.9340 - val_loss: 0.2065 - val_acc: 0.9238\n",
      "Epoch 9/60\n",
      " - 3s - loss: 0.1788 - acc: 0.9335 - val_loss: 0.2157 - val_acc: 0.9205\n",
      "Epoch 10/60\n",
      " - 3s - loss: 0.1780 - acc: 0.9344 - val_loss: 0.2375 - val_acc: 0.9226\n",
      "Epoch 11/60\n",
      " - 3s - loss: 0.1776 - acc: 0.9341 - val_loss: 3.4248 - val_acc: 0.1094\n",
      "Epoch 12/60\n",
      " - 3s - loss: 0.1763 - acc: 0.9348 - val_loss: 4.8508 - val_acc: 0.1009\n",
      "Epoch 13/60\n",
      " - 3s - loss: 0.1775 - acc: 0.9341 - val_loss: 0.2248 - val_acc: 0.9271\n",
      "Epoch 14/60\n",
      " - 3s - loss: 0.1767 - acc: 0.9347 - val_loss: 0.1931 - val_acc: 0.9309\n",
      "Epoch 15/60\n",
      " - 3s - loss: 0.1768 - acc: 0.9347 - val_loss: 0.1791 - val_acc: 0.9337\n",
      "Epoch 16/60\n",
      " - 3s - loss: 0.1771 - acc: 0.9344 - val_loss: 0.9282 - val_acc: 0.5885\n",
      "Epoch 17/60\n",
      " - 3s - loss: 0.1769 - acc: 0.9347 - val_loss: 5.9128 - val_acc: 0.1006\n",
      "Epoch 18/60\n",
      " - 3s - loss: 0.1765 - acc: 0.9350 - val_loss: 4.4406 - val_acc: 0.1029\n",
      "Epoch 19/60\n",
      " - 3s - loss: 0.1768 - acc: 0.9348 - val_loss: 0.5035 - val_acc: 0.7873\n",
      "Epoch 20/60\n",
      " - 3s - loss: 0.1771 - acc: 0.9350 - val_loss: 2.3113 - val_acc: 0.2008\n",
      "Epoch 21/60\n",
      " - 3s - loss: 0.1763 - acc: 0.9351 - val_loss: 12.0709 - val_acc: 0.1005\n",
      "Epoch 22/60\n",
      " - 3s - loss: 0.1765 - acc: 0.9346 - val_loss: 4.5430 - val_acc: 0.1010\n",
      "Epoch 23/60\n",
      " - 3s - loss: 0.1762 - acc: 0.9352 - val_loss: 0.6393 - val_acc: 0.9007\n",
      "Epoch 24/60\n",
      " - 3s - loss: 0.1759 - acc: 0.9352 - val_loss: 10.8370 - val_acc: 0.1005\n",
      "Epoch 25/60\n",
      " - 3s - loss: 0.1766 - acc: 0.9347 - val_loss: 5.9826 - val_acc: 0.1006\n",
      "Epoch 26/60\n",
      " - 3s - loss: 0.1760 - acc: 0.9348 - val_loss: 4.5726 - val_acc: 0.1026\n",
      "Epoch 27/60\n",
      " - 3s - loss: 0.1754 - acc: 0.9351 - val_loss: 1.2691 - val_acc: 0.4280\n",
      "Epoch 28/60\n",
      " - 3s - loss: 0.1761 - acc: 0.9349 - val_loss: 0.3360 - val_acc: 0.9131\n",
      "Epoch 29/60\n",
      " - 3s - loss: 0.1754 - acc: 0.9352 - val_loss: 3.6109 - val_acc: 0.1108\n",
      "Epoch 30/60\n",
      " - 3s - loss: 0.1761 - acc: 0.9354 - val_loss: 4.5323 - val_acc: 0.1029\n",
      "Epoch 31/60\n",
      " - 3s - loss: 0.1754 - acc: 0.9350 - val_loss: 0.3189 - val_acc: 0.8666\n",
      "Epoch 32/60\n",
      " - 3s - loss: 0.1765 - acc: 0.9346 - val_loss: 0.4809 - val_acc: 0.7884\n",
      "Epoch 33/60\n",
      " - 3s - loss: 0.1764 - acc: 0.9349 - val_loss: 3.1140 - val_acc: 0.1451\n",
      "Epoch 34/60\n",
      " - 3s - loss: 0.1761 - acc: 0.9350 - val_loss: 0.3715 - val_acc: 0.8406\n",
      "Epoch 35/60\n",
      " - 3s - loss: 0.1751 - acc: 0.9352 - val_loss: 3.2150 - val_acc: 0.1220\n",
      "Epoch 36/60\n",
      " - 3s - loss: 0.1753 - acc: 0.9354 - val_loss: 6.3218 - val_acc: 0.1005\n",
      "Epoch 37/60\n",
      " - 3s - loss: 0.1752 - acc: 0.9350 - val_loss: 0.1812 - val_acc: 0.9337\n",
      "Epoch 38/60\n",
      " - 3s - loss: 0.1759 - acc: 0.9351 - val_loss: 0.2136 - val_acc: 0.9166\n",
      "Epoch 39/60\n",
      " - 3s - loss: 0.1761 - acc: 0.9349 - val_loss: 0.2558 - val_acc: 0.9218\n",
      "Epoch 40/60\n",
      " - 3s - loss: 0.1756 - acc: 0.9353 - val_loss: 0.6688 - val_acc: 0.6872\n",
      "Epoch 41/60\n",
      " - 3s - loss: 0.1756 - acc: 0.9353 - val_loss: 0.2018 - val_acc: 0.9232\n",
      "Epoch 42/60\n",
      " - 3s - loss: 0.1758 - acc: 0.9349 - val_loss: 0.4066 - val_acc: 0.8236\n",
      "Epoch 43/60\n",
      " - 3s - loss: 0.1760 - acc: 0.9352 - val_loss: 3.2037 - val_acc: 0.1309\n",
      "Epoch 44/60\n",
      " - 3s - loss: 0.1754 - acc: 0.9352 - val_loss: 0.4352 - val_acc: 0.8125\n",
      "Epoch 45/60\n",
      " - 3s - loss: 0.1755 - acc: 0.9350 - val_loss: 1.0674 - val_acc: 0.5550\n",
      "Epoch 46/60\n",
      " - 3s - loss: 0.1756 - acc: 0.9353 - val_loss: 0.7022 - val_acc: 0.6810\n",
      "Epoch 47/60\n",
      " - 3s - loss: 0.1756 - acc: 0.9351 - val_loss: 0.8357 - val_acc: 0.6203\n",
      "Epoch 48/60\n",
      " - 3s - loss: 0.1767 - acc: 0.9349 - val_loss: 0.3950 - val_acc: 0.8274\n",
      "Epoch 49/60\n",
      " - 3s - loss: 0.1754 - acc: 0.9353 - val_loss: 0.4426 - val_acc: 0.9058\n",
      "Epoch 50/60\n",
      " - 3s - loss: 0.1727 - acc: 0.9362 - val_loss: 0.2320 - val_acc: 0.9104\n",
      "Epoch 51/60\n",
      " - 3s - loss: 0.1716 - acc: 0.9368 - val_loss: 0.2915 - val_acc: 0.8806\n",
      "Epoch 52/60\n",
      " - 3s - loss: 0.1713 - acc: 0.9367 - val_loss: 0.2849 - val_acc: 0.8846\n",
      "Epoch 53/60\n",
      " - 3s - loss: 0.1712 - acc: 0.9367 - val_loss: 0.2087 - val_acc: 0.9227\n",
      "Epoch 54/60\n",
      " - 3s - loss: 0.1712 - acc: 0.9371 - val_loss: 0.2126 - val_acc: 0.9216\n",
      "Epoch 55/60\n",
      " - 3s - loss: 0.1713 - acc: 0.9369 - val_loss: 0.1732 - val_acc: 0.9363\n",
      "Epoch 56/60\n",
      " - 3s - loss: 0.1712 - acc: 0.9368 - val_loss: 0.1743 - val_acc: 0.9358\n",
      "Epoch 57/60\n",
      " - 3s - loss: 0.1712 - acc: 0.9370 - val_loss: 0.1802 - val_acc: 0.9328\n",
      "Epoch 58/60\n",
      " - 3s - loss: 0.1712 - acc: 0.9370 - val_loss: 0.1809 - val_acc: 0.9335\n",
      "Epoch 59/60\n",
      " - 3s - loss: 0.1712 - acc: 0.9370 - val_loss: 0.1818 - val_acc: 0.9333\n",
      "Epoch 60/60\n",
      " - 3s - loss: 0.1712 - acc: 0.9371 - val_loss: 0.1735 - val_acc: 0.9365\n",
      "0.9293345440771753\n",
      "Train on 171429 samples, validate on 28571 samples\n",
      "Epoch 1/60\n",
      " - 7s - loss: 0.2849 - acc: 0.8883 - val_loss: 0.2767 - val_acc: 0.9152\n",
      "Epoch 2/60\n",
      " - 3s - loss: 0.1931 - acc: 0.9286 - val_loss: 0.2654 - val_acc: 0.9130\n",
      "Epoch 3/60\n",
      " - 3s - loss: 0.1840 - acc: 0.9322 - val_loss: 0.1952 - val_acc: 0.9290\n",
      "Epoch 4/60\n",
      " - 3s - loss: 0.1832 - acc: 0.9325 - val_loss: 0.8951 - val_acc: 0.6114\n",
      "Epoch 5/60\n",
      " - 3s - loss: 0.1820 - acc: 0.9326 - val_loss: 4.9259 - val_acc: 0.1173\n",
      "Epoch 6/60\n",
      " - 3s - loss: 0.1818 - acc: 0.9328 - val_loss: 10.7639 - val_acc: 0.1005\n",
      "Epoch 7/60\n",
      " - 3s - loss: 0.1806 - acc: 0.9329 - val_loss: 13.4260 - val_acc: 0.1005\n",
      "Epoch 8/60\n",
      " - 3s - loss: 0.1794 - acc: 0.9338 - val_loss: 14.0872 - val_acc: 0.1005\n",
      "Epoch 9/60\n",
      " - 3s - loss: 0.1796 - acc: 0.9337 - val_loss: 12.0774 - val_acc: 0.1005\n",
      "Epoch 10/60\n",
      " - 3s - loss: 0.1786 - acc: 0.9338 - val_loss: 8.3755 - val_acc: 0.1005\n",
      "Epoch 11/60\n",
      " - 3s - loss: 0.1784 - acc: 0.9342 - val_loss: 11.7083 - val_acc: 0.1005\n",
      "Epoch 12/60\n",
      " - 3s - loss: 0.1789 - acc: 0.9336 - val_loss: 13.6879 - val_acc: 0.1005\n",
      "Epoch 13/60\n",
      " - 3s - loss: 0.1780 - acc: 0.9341 - val_loss: 3.1064 - val_acc: 0.1488\n",
      "Epoch 14/60\n",
      " - 3s - loss: 0.1776 - acc: 0.9343 - val_loss: 2.7281 - val_acc: 0.1666\n",
      "Epoch 15/60\n",
      " - 3s - loss: 0.1772 - acc: 0.9345 - val_loss: 0.1956 - val_acc: 0.9263\n",
      "Epoch 16/60\n",
      " - 3s - loss: 0.1770 - acc: 0.9347 - val_loss: 2.2676 - val_acc: 0.1978\n",
      "Epoch 17/60\n",
      " - 3s - loss: 0.1774 - acc: 0.9345 - val_loss: 1.2399 - val_acc: 0.4611\n",
      "Epoch 18/60\n",
      " - 3s - loss: 0.1778 - acc: 0.9344 - val_loss: 4.8260 - val_acc: 0.1085\n",
      "Epoch 19/60\n",
      " - 3s - loss: 0.1775 - acc: 0.9344 - val_loss: 4.8902 - val_acc: 0.1023\n",
      "Epoch 20/60\n",
      " - 3s - loss: 0.1778 - acc: 0.9340 - val_loss: 0.4550 - val_acc: 0.7927\n",
      "Epoch 21/60\n",
      " - 3s - loss: 0.1774 - acc: 0.9346 - val_loss: 0.2116 - val_acc: 0.9196\n",
      "Epoch 22/60\n",
      " - 3s - loss: 0.1763 - acc: 0.9349 - val_loss: 2.9193 - val_acc: 0.1351\n",
      "Epoch 23/60\n",
      " - 3s - loss: 0.1772 - acc: 0.9346 - val_loss: 2.4558 - val_acc: 0.1629\n",
      "Epoch 24/60\n",
      " - 3s - loss: 0.1766 - acc: 0.9347 - val_loss: 3.5838 - val_acc: 0.1169\n",
      "Epoch 25/60\n",
      " - 3s - loss: 0.1759 - acc: 0.9350 - val_loss: 6.8348 - val_acc: 0.1005\n",
      "Epoch 26/60\n",
      " - 3s - loss: 0.1771 - acc: 0.9342 - val_loss: 0.2833 - val_acc: 0.8824\n",
      "Epoch 27/60\n",
      " - 3s - loss: 0.1759 - acc: 0.9352 - val_loss: 1.8186 - val_acc: 0.2974\n",
      "Epoch 28/60\n",
      " - 3s - loss: 0.1761 - acc: 0.9348 - val_loss: 1.1299 - val_acc: 0.5117\n",
      "Epoch 29/60\n",
      " - 3s - loss: 0.1762 - acc: 0.9348 - val_loss: 0.1804 - val_acc: 0.9334\n",
      "Epoch 30/60\n",
      " - 3s - loss: 0.1759 - acc: 0.9353 - val_loss: 0.2456 - val_acc: 0.9228\n",
      "Epoch 31/60\n",
      " - 3s - loss: 0.1762 - acc: 0.9345 - val_loss: 2.2833 - val_acc: 0.1869\n",
      "Epoch 32/60\n",
      " - 3s - loss: 0.1755 - acc: 0.9354 - val_loss: 1.0424 - val_acc: 0.5363\n",
      "Epoch 33/60\n",
      " - 3s - loss: 0.1757 - acc: 0.9349 - val_loss: 0.5637 - val_acc: 0.9013\n",
      "Epoch 34/60\n",
      " - 3s - loss: 0.1756 - acc: 0.9351 - val_loss: 0.1835 - val_acc: 0.9340\n",
      "Epoch 35/60\n",
      " - 3s - loss: 0.1760 - acc: 0.9351 - val_loss: 0.2147 - val_acc: 0.9289\n",
      "Epoch 36/60\n",
      " - 3s - loss: 0.1763 - acc: 0.9348 - val_loss: 0.4000 - val_acc: 0.9072\n",
      "Epoch 37/60\n",
      " - 3s - loss: 0.1769 - acc: 0.9346 - val_loss: 0.4972 - val_acc: 0.9042\n",
      "Epoch 38/60\n",
      " - 3s - loss: 0.1752 - acc: 0.9355 - val_loss: 0.4772 - val_acc: 0.9050\n",
      "Epoch 39/60\n",
      " - 3s - loss: 0.1763 - acc: 0.9347 - val_loss: 0.1817 - val_acc: 0.9337\n",
      "Epoch 40/60\n",
      " - 3s - loss: 0.1761 - acc: 0.9353 - val_loss: 0.3573 - val_acc: 0.9101\n",
      "Epoch 41/60\n",
      " - 3s - loss: 0.1762 - acc: 0.9349 - val_loss: 0.2142 - val_acc: 0.9309\n",
      "Epoch 42/60\n",
      " - 3s - loss: 0.1760 - acc: 0.9350 - val_loss: 0.2222 - val_acc: 0.9276\n",
      "Epoch 43/60\n",
      " - 3s - loss: 0.1755 - acc: 0.9355 - val_loss: 0.6912 - val_acc: 0.6903\n",
      "Epoch 44/60\n",
      " - 3s - loss: 0.1759 - acc: 0.9352 - val_loss: 0.6753 - val_acc: 0.6986\n",
      "Epoch 45/60\n",
      " - 3s - loss: 0.1759 - acc: 0.9350 - val_loss: 3.2389 - val_acc: 0.1270\n",
      "Epoch 46/60\n",
      " - 3s - loss: 0.1762 - acc: 0.9348 - val_loss: 1.0482 - val_acc: 0.5071\n",
      "Epoch 47/60\n",
      " - 3s - loss: 0.1759 - acc: 0.9354 - val_loss: 1.0873 - val_acc: 0.4699\n",
      "Epoch 48/60\n",
      " - 3s - loss: 0.1771 - acc: 0.9348 - val_loss: 1.0003 - val_acc: 0.5299\n",
      "Epoch 49/60\n",
      " - 3s - loss: 0.1758 - acc: 0.9350 - val_loss: 0.6000 - val_acc: 0.7246\n",
      "Epoch 50/60\n",
      " - 3s - loss: 0.1730 - acc: 0.9358 - val_loss: 0.3291 - val_acc: 0.8615\n",
      "Epoch 51/60\n",
      " - 3s - loss: 0.1717 - acc: 0.9364 - val_loss: 0.5459 - val_acc: 0.7439\n",
      "Epoch 52/60\n",
      " - 3s - loss: 0.1716 - acc: 0.9366 - val_loss: 0.2633 - val_acc: 0.8951\n",
      "Epoch 53/60\n",
      " - 3s - loss: 0.1715 - acc: 0.9368 - val_loss: 0.1894 - val_acc: 0.9307\n",
      "Epoch 54/60\n",
      " - 3s - loss: 0.1714 - acc: 0.9367 - val_loss: 0.2457 - val_acc: 0.9045\n",
      "Epoch 55/60\n",
      " - 3s - loss: 0.1714 - acc: 0.9368 - val_loss: 0.2346 - val_acc: 0.9106\n",
      "Epoch 56/60\n",
      " - 3s - loss: 0.1713 - acc: 0.9369 - val_loss: 0.1768 - val_acc: 0.9349\n",
      "Epoch 57/60\n",
      " - 3s - loss: 0.1712 - acc: 0.9367 - val_loss: 0.1821 - val_acc: 0.9325\n",
      "Epoch 58/60\n",
      " - 3s - loss: 0.1712 - acc: 0.9366 - val_loss: 0.1965 - val_acc: 0.9289\n",
      "Epoch 59/60\n",
      " - 3s - loss: 0.1714 - acc: 0.9364 - val_loss: 0.1737 - val_acc: 0.9351\n",
      "Epoch 60/60\n",
      " - 3s - loss: 0.1712 - acc: 0.9371 - val_loss: 0.1958 - val_acc: 0.9294\n",
      "0.929380332957761\n",
      "Train on 171429 samples, validate on 28571 samples\n",
      "Epoch 1/60\n",
      " - 7s - loss: 0.2751 - acc: 0.8915 - val_loss: 0.8945 - val_acc: 0.8995\n",
      "Epoch 2/60\n",
      " - 3s - loss: 0.1964 - acc: 0.9276 - val_loss: 1.0680 - val_acc: 0.8995\n",
      "Epoch 3/60\n",
      " - 3s - loss: 0.1869 - acc: 0.9312 - val_loss: 0.2088 - val_acc: 0.9252\n",
      "Epoch 4/60\n",
      " - 3s - loss: 0.1845 - acc: 0.9323 - val_loss: 0.1855 - val_acc: 0.9308\n",
      "Epoch 5/60\n",
      " - 3s - loss: 0.1834 - acc: 0.9322 - val_loss: 0.4773 - val_acc: 0.7900\n",
      "Epoch 6/60\n",
      " - 3s - loss: 0.1821 - acc: 0.9325 - val_loss: 3.9533 - val_acc: 0.1086\n",
      "Epoch 7/60\n",
      " - 3s - loss: 0.1815 - acc: 0.9327 - val_loss: 8.4600 - val_acc: 0.1005\n",
      "Epoch 8/60\n",
      " - 3s - loss: 0.1797 - acc: 0.9336 - val_loss: 4.6717 - val_acc: 0.1028\n",
      "Epoch 9/60\n",
      " - 3s - loss: 0.1790 - acc: 0.9340 - val_loss: 4.3699 - val_acc: 0.1044\n",
      "Epoch 10/60\n",
      " - 3s - loss: 0.1787 - acc: 0.9337 - val_loss: 4.5850 - val_acc: 0.1066\n",
      "Epoch 11/60\n",
      " - 3s - loss: 0.1788 - acc: 0.9341 - val_loss: 0.2195 - val_acc: 0.9158\n",
      "Epoch 12/60\n",
      " - 3s - loss: 0.1784 - acc: 0.9339 - val_loss: 2.7153 - val_acc: 0.1740\n",
      "Epoch 13/60\n",
      " - 3s - loss: 0.1784 - acc: 0.9342 - val_loss: 3.9853 - val_acc: 0.1244\n",
      "Epoch 14/60\n",
      " - 3s - loss: 0.1777 - acc: 0.9343 - val_loss: 1.7640 - val_acc: 0.3471\n",
      "Epoch 15/60\n",
      " - 3s - loss: 0.1774 - acc: 0.9347 - val_loss: 0.9389 - val_acc: 0.5657\n",
      "Epoch 16/60\n",
      " - 3s - loss: 0.1776 - acc: 0.9347 - val_loss: 0.2682 - val_acc: 0.8904\n",
      "Epoch 17/60\n",
      " - 3s - loss: 0.1781 - acc: 0.9343 - val_loss: 1.9182 - val_acc: 0.2993\n",
      "Epoch 18/60\n",
      " - 3s - loss: 0.1781 - acc: 0.9340 - val_loss: 0.7109 - val_acc: 0.6713\n",
      "Epoch 19/60\n",
      " - 3s - loss: 0.1771 - acc: 0.9345 - val_loss: 0.5751 - val_acc: 0.7444\n",
      "Epoch 20/60\n",
      " - 3s - loss: 0.1772 - acc: 0.9345 - val_loss: 0.3884 - val_acc: 0.8308\n",
      "Epoch 21/60\n",
      " - 3s - loss: 0.1773 - acc: 0.9346 - val_loss: 0.1749 - val_acc: 0.9351\n",
      "Epoch 22/60\n",
      " - 3s - loss: 0.1777 - acc: 0.9341 - val_loss: 7.1589 - val_acc: 0.1005\n",
      "Epoch 23/60\n",
      " - 3s - loss: 0.1773 - acc: 0.9345 - val_loss: 0.2460 - val_acc: 0.8997\n",
      "Epoch 24/60\n",
      " - 3s - loss: 0.1773 - acc: 0.9345 - val_loss: 0.8477 - val_acc: 0.8999\n",
      "Epoch 25/60\n",
      " - 3s - loss: 0.1771 - acc: 0.9346 - val_loss: 0.3343 - val_acc: 0.9131\n",
      "Epoch 26/60\n",
      " - 3s - loss: 0.1767 - acc: 0.9346 - val_loss: 0.2328 - val_acc: 0.9255\n",
      "Epoch 27/60\n",
      " - 3s - loss: 0.1769 - acc: 0.9344 - val_loss: 0.9943 - val_acc: 0.5523\n",
      "Epoch 28/60\n",
      " - 3s - loss: 0.1776 - acc: 0.9341 - val_loss: 1.7686 - val_acc: 0.2880\n",
      "Epoch 29/60\n",
      " - 3s - loss: 0.1768 - acc: 0.9345 - val_loss: 0.5404 - val_acc: 0.9028\n",
      "Epoch 30/60\n",
      " - 3s - loss: 0.1772 - acc: 0.9344 - val_loss: 0.2482 - val_acc: 0.9229\n",
      "Epoch 31/60\n",
      " - 3s - loss: 0.1771 - acc: 0.9347 - val_loss: 0.2998 - val_acc: 0.8734\n",
      "Epoch 32/60\n",
      " - 3s - loss: 0.1763 - acc: 0.9349 - val_loss: 0.1953 - val_acc: 0.9326\n",
      "Epoch 33/60\n",
      " - 3s - loss: 0.1769 - acc: 0.9348 - val_loss: 1.7498 - val_acc: 0.3084\n",
      "Epoch 34/60\n",
      " - 3s - loss: 0.1768 - acc: 0.9344 - val_loss: 0.1929 - val_acc: 0.9276\n",
      "Epoch 35/60\n",
      " - 3s - loss: 0.1774 - acc: 0.9342 - val_loss: 0.5628 - val_acc: 0.7475\n",
      "Epoch 36/60\n",
      " - 3s - loss: 0.1768 - acc: 0.9346 - val_loss: 0.4975 - val_acc: 0.9042\n",
      "Epoch 37/60\n",
      " - 3s - loss: 0.1764 - acc: 0.9346 - val_loss: 0.4386 - val_acc: 0.9039\n",
      "Epoch 38/60\n",
      " - 3s - loss: 0.1761 - acc: 0.9349 - val_loss: 0.2310 - val_acc: 0.9235\n",
      "Epoch 39/60\n",
      " - 3s - loss: 0.1763 - acc: 0.9347 - val_loss: 0.2026 - val_acc: 0.9231\n",
      "Epoch 40/60\n",
      " - 3s - loss: 0.1759 - acc: 0.9349 - val_loss: 0.1821 - val_acc: 0.9337\n",
      "Epoch 41/60\n",
      " - 3s - loss: 0.1757 - acc: 0.9350 - val_loss: 0.2525 - val_acc: 0.8969\n",
      "Epoch 42/60\n",
      " - 3s - loss: 0.1764 - acc: 0.9351 - val_loss: 0.4149 - val_acc: 0.9081\n",
      "Epoch 43/60\n",
      " - 3s - loss: 0.1766 - acc: 0.9350 - val_loss: 0.8071 - val_acc: 0.6265\n",
      "Epoch 44/60\n",
      " - 3s - loss: 0.1772 - acc: 0.9343 - val_loss: 0.3329 - val_acc: 0.9118\n",
      "Epoch 45/60\n",
      " - 3s - loss: 0.1764 - acc: 0.9347 - val_loss: 0.4831 - val_acc: 0.9036\n",
      "Epoch 46/60\n",
      " - 3s - loss: 0.1759 - acc: 0.9352 - val_loss: 0.3578 - val_acc: 0.8469\n",
      "Epoch 47/60\n",
      " - 3s - loss: 0.1763 - acc: 0.9347 - val_loss: 0.3567 - val_acc: 0.9115\n",
      "Epoch 48/60\n",
      " - 3s - loss: 0.1757 - acc: 0.9350 - val_loss: 0.2894 - val_acc: 0.8793\n",
      "Epoch 49/60\n",
      " - 3s - loss: 0.1758 - acc: 0.9349 - val_loss: 0.1761 - val_acc: 0.9344\n",
      "Epoch 50/60\n",
      " - 3s - loss: 0.1733 - acc: 0.9357 - val_loss: 0.2597 - val_acc: 0.9194\n",
      "Epoch 51/60\n",
      " - 3s - loss: 0.1721 - acc: 0.9364 - val_loss: 0.2030 - val_acc: 0.9281\n",
      "Epoch 52/60\n",
      " - 3s - loss: 0.1718 - acc: 0.9370 - val_loss: 0.1777 - val_acc: 0.9346\n",
      "Epoch 53/60\n",
      " - 3s - loss: 0.1718 - acc: 0.9365 - val_loss: 0.1726 - val_acc: 0.9372\n",
      "Epoch 54/60\n",
      " - 3s - loss: 0.1717 - acc: 0.9366 - val_loss: 0.1865 - val_acc: 0.9313\n",
      "Epoch 55/60\n",
      " - 3s - loss: 0.1716 - acc: 0.9366 - val_loss: 0.2189 - val_acc: 0.9162\n",
      "Epoch 56/60\n",
      " - 3s - loss: 0.1716 - acc: 0.9368 - val_loss: 0.2008 - val_acc: 0.9257\n",
      "Epoch 57/60\n",
      " - 3s - loss: 0.1716 - acc: 0.9368 - val_loss: 0.1803 - val_acc: 0.9340\n",
      "Epoch 58/60\n",
      " - 3s - loss: 0.1715 - acc: 0.9368 - val_loss: 0.1896 - val_acc: 0.9295\n",
      "Epoch 59/60\n",
      " - 3s - loss: 0.1715 - acc: 0.9368 - val_loss: 0.1698 - val_acc: 0.9371\n",
      "Epoch 60/60\n",
      " - 3s - loss: 0.1715 - acc: 0.9368 - val_loss: 0.1754 - val_acc: 0.9355\n",
      "0.9294678879518088\n",
      "0.9294678879518088\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "n_splits = 7\n",
    "num_preds = 5\n",
    "epochs = 60\n",
    "learning_rate_init = 0.02\n",
    "batch_size = 4000\n",
    "\n",
    "num_features = len(features)\n",
    "\n",
    "def get_features(preds, df):\n",
    "    list_features = [preds, df[features].values, df[features_count].values, df[features_deviation], df[features_density]]\n",
    "    list_indices = []\n",
    "    for i in range(num_features):\n",
    "        indices = np.arange(num_preds)*num_features + i\n",
    "        list_indices.append(indices)\n",
    "    indices = np.concatenate(list_indices)\n",
    "    feats = np.concatenate(list_features, axis=1)[:,indices]\n",
    "    return feats \n",
    "\n",
    "def get_model_3():\n",
    "    inp = keras.layers.Input((num_features*num_preds,))\n",
    "    x = keras.layers.Reshape((num_features*num_preds,1))(inp)\n",
    "    x = keras.layers.Conv1D(32,num_preds,strides=num_preds, activation='elu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv1D(24,1, activation='elu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv1D(16,1, activation='elu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Conv1D(4,1, activation='elu')(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Reshape((num_features*4,1))(x)\n",
    "    x = keras.layers.AveragePooling1D(2)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    out = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    return keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "\n",
    "def lr_scheduler(epoch):\n",
    "    if epoch <= epochs*0.8:\n",
    "        return learning_rate_init\n",
    "    else:\n",
    "        return learning_rate_init * 0.1\n",
    "\n",
    "def train_NN(features_oof, features_test, features_train, features_fake):\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "    preds_nn_oof = np.zeros(features_oof.shape[0])\n",
    "    preds_nn_test = np.zeros(features_test.shape[0])\n",
    "    preds_nn_fake = np.zeros(features_fake.shape[0])\n",
    "\n",
    "    for trn_idx, val_idx in folds.split(features_oof, target_train):\n",
    "        features_oof_tr = features_oof[trn_idx, :]\n",
    "        target_oof_tr = target_train.values[trn_idx]\n",
    "        features_oof_val = features_oof[val_idx, :]\n",
    "        target_oof_val = target_train.values[val_idx]\n",
    "\n",
    "        optimizer = keras.optimizers.Adam(lr = learning_rate_init, decay = 0.00001)\n",
    "        model = get_model_3()\n",
    "        callbacks = []\n",
    "        callbacks.append(keras.callbacks.LearningRateScheduler(lr_scheduler))\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.fit(features_oof_tr, target_oof_tr, validation_data=(features_oof_val, target_oof_val), epochs=epochs, verbose=2, batch_size=batch_size, callbacks=callbacks)\n",
    "\n",
    "        preds_nn_oof += model.predict(features_oof, batch_size=2000)[:,0]\n",
    "        preds_nn_test += model.predict(features_test, batch_size=2000)[:,0]\n",
    "        preds_nn_fake += model.predict(features_fake, batch_size=2000)[:,0]\n",
    "\n",
    "        print(roc_auc_score(target_train, preds_nn_oof))\n",
    "        if use_experimental:\n",
    "            print(roc_auc_score(target_test, preds_nn_test))\n",
    "            print(roc_auc_score(target_test, preds_test.mean(axis=1)))\n",
    "\n",
    "    preds_nn_oof /= n_splits\n",
    "    preds_nn_test /= n_splits\n",
    "    preds_nn_fake /= n_splits\n",
    "    return preds_nn_oof, preds_nn_test, preds_nn_fake\n",
    "\n",
    "\n",
    "features_oof = get_features(preds_oof, X_train)\n",
    "features_test = get_features(preds_test, X_test)\n",
    "if not use_experimental:\n",
    "    del X_test\n",
    "features_train = get_features(preds_train, X_train)\n",
    "if not use_experimental:\n",
    "    del X_train\n",
    "features_fake = get_features(preds_fake, X_fake)\n",
    "if not use_experimental:\n",
    "    del X_fake\n",
    "    del preds_oof\n",
    "    del preds_fake\n",
    "    del preds_train\n",
    "    del preds_test\n",
    "\n",
    "print(get_model_3().summary())\n",
    "    \n",
    "preds_nn_oof, preds_nn_test, preds_nn_fake = train_NN(features_oof, features_test, features_train, features_fake)\n",
    "\n",
    "print(roc_auc_score(target_train, preds_nn_oof))\n",
    "if use_experimental:\n",
    "    print('test AUC: ', roc_auc_score(target_test, preds_nn_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oof  :  0.9294678879518088\n",
      "    ID_code    target\n",
      "0    test_0  0.166367\n",
      "1    test_1  0.328820\n",
      "2    test_2  0.383148\n",
      "3    test_3  0.250447\n",
      "4    test_4  0.152536\n",
      "5    test_5  0.003037\n",
      "6    test_6  0.005144\n",
      "7    test_7  0.092654\n",
      "8    test_8  0.004632\n",
      "9    test_9  0.006795\n",
      "10  test_10  0.396071\n",
      "11  test_11  0.112003\n",
      "12  test_12  0.065132\n",
      "13  test_13  0.064719\n",
      "14  test_14  0.003989\n",
      "15  test_15  0.043415\n",
      "16  test_16  0.695849\n",
      "17  test_17  0.025956\n",
      "18  test_18  0.083287\n",
      "19  test_19  0.008318\n"
     ]
    }
   ],
   "source": [
    "preds_oof_final = preds_nn_oof\n",
    "preds_test_final = preds_nn_test\n",
    "preds_fake_final = preds_nn_fake\n",
    "\n",
    "print('oof  : ', roc_auc_score(target_train, preds_oof_final))\n",
    "if use_experimental:\n",
    "    print('test : ', roc_auc_score(target_test, preds_test_final))\n",
    "    print('train: ', roc_auc_score(target_fake, preds_fake_final))\n",
    "\n",
    "if not use_experimental:\n",
    "    sub = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\n",
    "    predictions_all = np.zeros(test_df.shape[0])\n",
    "    predictions_all[indices_real] = preds_test_final\n",
    "    predictions_all[indices_fake] = preds_fake_final\n",
    "    sub[\"target\"] = predictions_all\n",
    "    sub.to_csv(\"submission.csv\", index=False)\n",
    "    print(sub.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
